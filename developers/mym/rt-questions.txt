Q9:

1 local_lock/unlock与preempt_disable/enable, OR  local_irq_disable/enable如何相互替换？ 


from PATCH idr: Use local lock instead of preempt enable/disable
=====================================
https://www.kernel.org/pub/linux/kernel/projects/rt/4.9/older/patches-4.9-rt1.tar.gz

* [  3.10 -    4.9] idr: Use local lock instead of preempt enable/disable{C::performance::mutex::idr: Use local lock instead of preempt enable/disable}
  + [[file:3.10/idr-use-local-lock-for-protection.patch][3.10]]
  

local_lock/unlock与preempt_disable/enable 的关系。


```
rt_spin_lock 即没有migration_disable, 并有rt_mutex
------------------------------------
#ifdef CONFIG_PREEMPT_RT_FULL
# define spin_lock_local(lock)			rt_spin_lock(lock)

------------------------------------
static inline void __local_lock(struct local_irq_lock *lv)
{
	if (lv->owner != current) {
		spin_lock_local(&lv->lock);
		LL_WARN(lv->owner);
		LL_WARN(lv->nestcnt);
		lv->owner = current;
	}
	lv->nestcnt++;
}
------------------------------------
#ifdef CONFIG_PREEMPT_RT_FULL

#define get_local_var(var) (*({		\
	       migrate_disable();	\
	       this_cpu_ptr(&var);	}))


------------------------------------
#define local_lock(lvar)					\
	do { __local_lock(&get_local_var(lvar)); } while (0)

```

local_lock主要是disable migrate，并且使用rt_mutex类型的lock,并且可抢占

preempt_disable主要是关闭本地抢占，但其实没有实现smp互斥

local_irq_disable 是关闭本地中断,，但其实没有实现smp互斥

三者是有区别的

从这两个api的diff来看，只需要锁就行，可以抢占、可以迁移的。
We need to protect the per cpu variable and prevent migration.
但是和其注释信息是相矛盾的。

这里原来为啥需要关闭抢占？
A: 保证不能有单cpu上由于进程抢占，导致另外进程访问带来的潜在single cpu下的互斥



from PATCH mm/memcontrol: Replace local_irq_disable with local locks
================================
* [  3.18 -   4.11] mm/memcontrol: Replace local_irq_disable with local locks {C::performance::mutex::Replace local_irq_disable with local locks}
  + [[file:3.18/mm-memcontrol-do_not_disable_irq.patch][3.18]]

 这里local_irq_disable被替换为local_lock_irq
 其语义就是rt_spin_lock 即睡眠锁。
 也就是把改关闭中的地方换成了睡眠锁。因此可以推断？是中断线程化导致的可以这样做？
 


 
 
 1 local_bh_disable/enable与_local_bh_doisable/enable如何相互替换？

 from PATCH genirq: Allow disabling of softirq processing in irq thread context

 
 * [   3.4 -   4.11] genirq: Allow disabling of softirq processing in irq thread context{C::performance::irq::Allow disabling of softirq processing in irq thread context}
  + [[file:3.4/irq-allow-disabling-of-softirq-processing-in-irq-thread-context.patch][3.4]]

将local_bh_enable替换为_local_bh_enable，发现是减少了检查、并且执行
do_current_softirqs的工作。
（这里都是比较的RT版本的API。没有和VNINLLA版本做对比。感觉应该是对的。）


+void _local_bh_enable(void)
+{
+	if (WARN_ON(current->softirq_nestcnt == 0))
+		return;
+	if (--current->softirq_nestcnt == 0)
+		migrate_enable();
+}
+EXPORT_SYMBOL(_local_bh_enable);


void __local_bh_enable(void)
{
        if (WARN_ON(current->softirq_nestcnt == 0))
                return;

        local_irq_disable();
        if (current->softirq_nestcnt == 1 && current->softirqs_raised)
                do_current_softirqs();
        local_irq_enable();

        if (--current->softirq_nestcnt == 0)
                migrate_enable();
}
EXPORT_SYMBOL(__local_bh_enable);

原因：说是在RT情况下面少做一些事情，来提供性能。

loca_bh_disable在non-rt和rt下语义发生了比较大的变化

在non-rt下，loca_bh_disable会add  __preempt_count
DEFINE_PER_CPU(int, __preempt_count) = INIT_PREEMPT_COUNT;
使得调度器不执行此cpu中的softirq 代码

在rt下，softirq都是kernel thread了. loca_bh_disable的语义为禁止迁移 migrate_disable

1  何时需要把disable_irq替换为disable_irq_nosync  这里面有规律吗？
chyyuu: 我看到disable_irq 和 disable_irq_nosync 在non-rt和rt下的实现是一样的。所以这里的问题不应该是rt独有的问题。
chyyuu:在vanilla中，也有disable_irq和disable_irq_nosync的大量使用。

./drivers-net-8139-disable-irq-nosync.patch:3:Subject: drivers/net: Use disable_irq_nosync() in 8139too
./drivers-net-8139-disable-irq-nosync.patch:5:Use disable_irq_nosync() instead of disable_irq() as this might be
./drivers-net-8139-disable-irq-nosync.patch:21:-        disable_irq(irq);
./drivers-net-8139-disable-irq-nosync.patch:22:+        disable_irq_nosync(irq);

两者的区别是disable_irq会在disable_irq_nosync返回值为0的基础上，调用synchronize_irq
处理pending IRQ handlers。但是会引起死锁。


#ifdef CONFIG_NET_POLL_CONTROLLER
/*
 * Polling receive - used by netconsole and other diagnostic tools
 * to allow network i/o with interrupts disabled.
 */
static void rtl8139_poll_controller(struct net_device *dev)
{
        struct rtl8139_private *tp = netdev_priv(dev);
        const int irq = tp->pci_dev->irq;

        disable_irq_nosync(irq);
        rtl8139_interrupt(irq, dev);
        enable_irq(irq);
}
#endif


/**
 *      disable_irq - disable an irq and wait for completion
 *      @irq: Interrupt to disable
 *
 *      Disable the selected interrupt line.  Enables and Disables are
 *      nested.
 *      This function waits for any pending IRQ handlers for this interrupt
 *      to complete before returning. If you use this function while
 *      holding a resource the IRQ handler may need you will deadlock.
 *
 *      This function may be called - with care - from IRQ context.
 */
void disable_irq(unsigned int irq)
{
        if (!__disable_irq_nosync(irq))
                synchronize_irq(irq);
}
EXPORT_SYMBOL(disable_irq);

/**
 *      disable_irq_nosync - disable an irq without waiting
 *      @irq: Interrupt to disable
 *
 *      Disable the selected interrupt line.  Disables and Enables are
 *      nested.
 *      Unlike disable_irq(), this function does not ensure existing
 *      instances of the IRQ handler have completed before returning.
 *
 *      This function may be called from IRQ context.
 */
void disable_irq_nosync(unsigned int irq)
{
        __disable_irq_nosync(irq);
}
EXPORT_SYMBOL(disable_irq_nosync);

static int __disable_irq_nosync(unsigned int irq)
{
 unsigned long flags;
 struct irq_desc *desc = irq_get_desc_buslock(irq, &flags, ((1 << 0)));

 if (!desc)
  return -22;
 __disable_irq(desc);
 irq_put_desc_busunlock(desc, flags);
 return 0;
}




/**
 *      synchronize_irq - wait for pending IRQ handlers (on other CPUs)
 *      @irq: interrupt number to wait for
 *
 *      This function waits for any pending IRQ handlers for this interrupt
 *      to complete before returning. If you use this function while
 *      holding a resource the IRQ handler may need you will deadlock.
 *
 *      This function may be called - with care - from IRQ context.
 */
void synchronize_irq(unsigned int irq)
{
        struct irq_desc *desc = irq_to_desc(irq);

        if (desc) {
                __synchronize_hardirq(desc);
                /*
                 * We made sure that no hardirq handler is
                 * running. Now verify that no threaded handlers are
                 * active.
                 */
                wait_event(desc->wait_for_threads,
                           !atomic_read(&desc->threads_active));
        }
}
EXPORT_SYMBOL(synchronize_irq);


void synchronize_irq(unsigned int irq)
{
 struct irq_desc *desc = irq_to_desc(irq);

 if (desc) {
  __synchronize_hardirq(desc);

  do { do { do { } while (0); } while (0); if (!atomic_read(&desc->threads_active)) break; (void)({ __label__ __out; wait_queue_t __wait; long __ret = 0; init_wait_entry(&__wait, 0 ? 0x01 : 0); for (;;) { long __int = prepare_to_wait_event(&desc->wait_for_threads, &__wait, 2); if (!atomic_read(&desc->threads_active)) break; if ((!__builtin_constant_p(2) || 2 == 1 || 2 == (128 | 2)) && __int) { __ret = __int; goto __out; } schedule(); } finish_wait(&desc->wait_for_threads, &__wait); __out: __ret; }); } while (0)
                                          ;
 }
}



1  何时需要把preempt_conditional_sti替换为conditional_sti_ist  这里面有规律吗？

./4.1/fix-rt-int3-x86_32-3.2-rt.patch:14:The name of the function is changed from preempt_conditional_sti/cli()
./4.1/fix-rt-int3-x86_32-3.2-rt.patch:32:-static inline void preempt_conditional_sti(struct pt_regs *regs)
./4.1/fix-rt-int3-x86_32-3.2-rt.patch:33:+static inline void conditional_sti_ist(struct pt_regs *regs)
./4.1/fix-rt-int3-x86_32-3.2-rt.patch:70:-      preempt_conditional_sti(regs);
./4.1/fix-rt-int3-x86_32-3.2-rt.patch:71:+      conditional_sti_ist(regs);
./4.1/fix-rt-int3-x86_32-3.2-rt.patch:82:-      preempt_conditional_sti(regs);
./4.1/fix-rt-int3-x86_32-3.2-rt.patch:83:+      conditional_sti_ist(regs);


preempt_count_inc 和preempt_count_dec 加了编译选项，只有CONFIG_X86_64
时才会执行这两个函数。
原因注释里面说的很清楚了
+	/*
+	 * X86_64 uses a per CPU stack on the IST for certain traps
+	 * like int3. The task can not be preempted when using one
+	 * of these stacks, thus preemption must be disabled, otherwise
+	 * the stack can be corrupted if the task is scheduled out,
+	 * and another task comes in and uses this stack.
+	 *
+	 * On x86_32 the task keeps its own stack and it is OK if the
+	 * task schedules out.
+	 */



-static inline void preempt_conditional_sti(struct pt_regs *regs)
+static inline void conditional_sti_ist(struct pt_regs *regs)
 {
+#ifdef CONFIG_X86_64
+	/*
+	 * X86_64 uses a per CPU stack on the IST for certain traps
+	 * like int3. The task can not be preempted when using one
+	 * of these stacks, thus preemption must be disabled, otherwise
+	 * the stack can be corrupted if the task is scheduled out,
+	 * and another task comes in and uses this stack.
+	 *
+	 * On x86_32 the task keeps its own stack and it is OK if the
+	 * task schedules out.
+	 */
 	preempt_count_inc();
+#endif
 	if (regs->flags & X86_EFLAGS_IF)
 		local_irq_enable();
 }
@@ -101,11 +113,13 @@ static inline void conditional_cli(struc
 		local_irq_disable();
 }
 
-static inline void preempt_conditional_cli(struct pt_regs *regs)
+static inline void conditional_cli_ist(struct pt_regs *regs)
 {
 	if (regs->flags & X86_EFLAGS_IF)
 		local_irq_disable();
+#ifdef CONFIG_X86_64
 	preempt_count_dec();
+#endif
 }
 
 

1 何时需要添加io_schedule()？

/4.9/fs-jbd2-pull-your-plug-when-waiting-for-space.patch:28:+                  io_schedule();

+		if (current->plug)
+			io_schedule();


io_schedule会引发进程UNINTERRUPTERBALE睡眠等待，并且是需要唤醒的，这里是为了提高实时性能。

blk_plug构建了一个缓存碎片IO的请求队列。用于将顺序请求合并成一个大的请求。
合并后请求批量从per-task链表移动到设备请求队列，减少了设备请求队列锁竞争，
从而提高了效率。
详细参考资料：
http://blog.csdn.net/liumangxiong/article/details/10279089



task_struct成员
        /* stack plugging */  
        struct blk_plug *plug; 
 
 
 


1 何时需要把yield();替换为msleep(1);?


* [  3.14 -   4.11] net: sched: Use msleep() instead of yield(){C::bug::deadlock::deadlock::semantics::sched: Use msleep() instead of yield()}
  + [[file:3.14/net-sched-dev_deactivate_many-use-msleep-1-instead-o.patch][3.14]]

	
On PREEMPT_RT enabled systems the interrupt handler run as threads at prio 50
(by default). If a high priority userspace process tries to shut down a busy
network interface it might spin in a yield loop waiting for the device to
become idle. With the interrupt thread having a lower priority than the
looping process it might never be scheduled and so result in a deadlock on UP
systems.

yield会引发schedule，发生进程切换
msleep是休眠函数，也会发生进程切换，但是区别是：

	
应该是优先级高低导致的优先级翻转的问题
	
sleep()方法会给其他线程运行的机会,而不管其他线程的优先级,因此会给较低优先级的线程运行的机会;yeild(
)方法只会给优先
  级相同的或者比自己高的线程运行的机会.
sleep()方法使线程进入阻塞状态,而yeild()方法使线程进入就绪状态.
sleep方法允许较低优先级的线程获得运行机会，但yield（）方法执行时，当前线程仍处在可运行状态，所以不
可能让出较低优先级的线程此时获取CPU占有权。在一个运行系统中，如果较高优先级的线程没有调用sleep方法
，也没有受到I/O阻塞，那么较低优先级线程只能等待所有较高优先级的线程运行结束，方可有机会运行。

  
	
./2.6.24/tasklet-busy-loop-hack.patch:48:+              msleep(1);
./2.6.24/preempt-realtime-net.patch:554:-                       yield();
./2.6.24/preempt-realtime-net.patch:555:+                       msleep(1);
./2.6.24/preempt-realtime-core.patch:717:-                      yield();
./2.6.24/preempt-realtime-core.patch:718:+                      msleep(1);
./2.6.24/preempt-realtime-core.patch:751:-                      yield();
./2.6.24/preempt-realtime-core.patch:760:-              yield();


./4.11/tasklet-rt-prevent-tasklets-from-going-into-infinite-spin-in-rt.patch:363:-                      yield();
./4.11/tasklet-rt-prevent-tasklets-from-going-into-infinite-spin-in-rt.patch:364:+                      msleep(1);
./4.11/tasklet-rt-prevent-tasklets-from-going-into-infinite-spin-in-rt.patch:380:+              msleep(1);

./2.6.23/tasklet-busy-loop-hack.patch:48:+              msleep(1);
./2.6.23/preempt-realtime-net.patch:633:-               yield();
./2.6.23/preempt-realtime-net.patch:634:+               msleep(1);
./2.6.23/preempt-realtime-core.patch:738:-                      yield();
./2.6.23/preempt-realtime-core.patch:739:+                      msleep(1);
./2.6.23/preempt-realtime-core.patch:772:-                      yield();
./2.6.23/preempt-realtime-core.patch:781:-              yield();



/**
 * yield - yield the current processor to other threads.
 *
 * Do not ever use this function, there's a 99% chance you're doing it wrong.
 *
 * The scheduler is at all times free to pick the calling task as the most
 * eligible task to run, if removing the yield() call from your code breaks
 * it, its already broken.
 *
 * Typical broken usage is:
 *
 * while (!event)
 *      yield();
 *
 * where one assumes that yield() will let 'the other' process run that will
 * make event true. If the current task is a SCHED_FIFO task that will never
 * happen. Never use yield() as a progress guarantee!!
 *
 * If you want to use yield() to wait for something, use wait_event().
 * If you want to use yield() to be 'nice' for others, use cond_resched().
 * If you still want to use yield(), do not!
 */
void __sched yield(void)
{
        set_current_state(TASK_RUNNING);
        sys_sched_yield();
}
EXPORT_SYMBOL(yield);





1 seqlock 如何替换为 simple seqcounter and a rawlock？ PATCH vtime: Split lock and seqcount


* [  3.10 -    4.4] vtime: Split lock and seqcount{C::performance::mutex:: vtime: Split lock and seqcount}
  + [[file:3.10/vtime-split-lock-and-seqcount.patch][3.10]]
  M [[file:3.12/vtime-split-lock-and-seqcount.patch][3.12]]
  M [[file:3.14/vtime-split-lock-and-seqcount.patch][3.14]]
    [[file:3.18/vtime-split-lock-and-seqcount.patch][3.18]]
    [[file:4.0/vtime-split-lock-and-seqcount.patch][4.0]]
    [[file:4.1/vtime-split-lock-and-seqcount.patch][4.1]]
  m [[file:4.4/vtime-split-lock-and-seqcount.patch][4.4]]
  - 4.6

这里把一个锁拆成了多个锁，应该是为了增多抢占的空间。  








Q10:
1 如何理解resched_softirq在rt v.s. nonrt下的区别？
+#ifndef CONFIG_PREEMPT_RT_FULL
 extern int __cond_resched_softirq(void);
 
 #define cond_resched_softirq() ({					\
 	___might_sleep(__FILE__, __LINE__, SOFTIRQ_DISABLE_OFFSET);	\
 	__cond_resched_softirq();					\
 })
+#else
+# define cond_resched_softirq()		cond_resched()
+#endif
 
 
cond_resched_softirq 的语义有所不同在NORT情况下面，其直接返回0，在RT下面则进行了调度相关工作。

 

* [   3.0 -   4.11] sched: Take RT softirq semantics into account in cond_resched(){C::feature::rtsupport::make fun rt-lize}
  + [[file:3.0/cond-resched-softirq-rt.patch][3.0]]
    [[file:3.2/cond-resched-softirq-rt.patch][3.2]]
    [[file:3.4/cond-resched-softirq-rt.patch][3.4]]
    [[file:3.6/cond-resched-softirq-rt.patch][3.6]]
    [[file:3.8/cond-resched-softirq-rt.patch][3.8]]
    [[file:3.10/cond-resched-softirq-rt.patch][3.10]]
  m [[file:3.12/cond-resched-softirq-rt.patch][3.12]]
    [[file:3.14/cond-resched-softirq-rt.patch][3.14]]
    [[file:3.18/cond-resched-softirq-rt.patch][3.18]]
  m [[file:4.0/cond-resched-softirq-rt.patch][4.0]]
    [[file:4.1/cond-resched-softirq-rt.patch][4.1]]
    [[file:4.4/cond-resched-softirq-rt.patch][4.4]]
    [[file:4.6/cond-resched-softirq-rt.patch][4.6]]
    [[file:4.8/cond-resched-softirq-rt.patch][4.8]]
    [[file:4.9/cond-resched-softirq-rt.patch][4.9]]
    [[file:4.11/cond-resched-softirq-rt.patch][4.11]]
	
NORT
==================	
NORT 情况下面： _cond_resched(void) 为 { return 0; } 

./kernel/printk/printk.c:2257:   * console registration path, and should invoke cond_resched()
./kernel/printk/printk.c:2341:                  cond_resched();
./kernel/printk/printk.c:2384:          cond_resched();

#define cond_resched() ({ ___might_sleep(__FILE__, __LINE__, 0); _cond_resched(); })	


./include/linux/sched.h:1609:static inline int _cond_resched(void) { return 0; }
./kernel/sched/core.c:5179:int __sched _cond_resched(void)
./kernel/sched/core.c:5187:EXPORT_SYMBOL(_cond_resched);


#ifndef CONFIG_PREEMPT
extern int _cond_resched(void);
#else
static inline int _cond_resched(void) { return 0; }
#endif

#ifndef CONFIG_PREEMPT
int __sched _cond_resched(void)
{
        if (should_resched(0)) {
                preempt_schedule_common();
                return 1;
        }
        return 0;
}
EXPORT_SYMBOL(_cond_resched);
#endif

RT情况下面：
===============

#ifndef CONFIG_PREEMPT_RT_FULL
int __sched __cond_resched_softirq(void)
{
        BUG_ON(!in_softirq());

        if (should_resched(SOFTIRQ_DISABLE_OFFSET)) {
                local_bh_enable();
                preempt_schedule_common();
                local_bh_disable();
                return 1;
        }
        return 0;
}
EXPORT_SYMBOL(__cond_resched_softirq);
#endif





Q11:
1 如何理解 preempt-rt 对 hotplug的支持？为何cpu_down的重新实现？
+#ifdef CONFIG_PREEMPT_RT_FULL
+# define hotplug_lock()		rt_spin_lock(&cpu_hotplug.lock)
+# define hotplug_trylock()	rt_spin_trylock(&cpu_hotplug.lock)
+# define hotplug_unlock()	rt_spin_unlock(&cpu_hotplug.lock)
+#else
+# define hotplug_lock()		mutex_lock(&cpu_hotplug.lock)
+# define hotplug_trylock()	mutex_trylock(&cpu_hotplug.lock)
+# define hotplug_unlock()	mutex_unlock(&cpu_hotplug.lock)
+#endif


4.6和4.11的patch不一样。


* [   3.6 -   4.11] cpu/rt: Rework cpu down for PREEMPT_RT{C::feature::sched::migrate the current task off the cpu is going down}
  + [[file:3.6/cpu-rt-rework-cpu-down.patch][3.6]]
  m [[file:3.8/cpu-rt-rework-cpu-down.patch][3.8]]
  M [[file:3.10/cpu-rt-rework-cpu-down.patch][3.10]]
  m [[file:3.12/cpu-rt-rework-cpu-down.patch][3.12]]
  m [[file:3.14/cpu-rt-rework-cpu-down.patch][3.14]]
  M [[file:3.18/cpu-rt-rework-cpu-down.patch][3.18]]
  M [[file:4.0/cpu-rt-rework-cpu-down.patch][4.0]]
    [[file:4.1/cpu-rt-rework-cpu-down.patch][4.1]]
  M [[file:4.4/cpu-rt-rework-cpu-down.patch][4.4]]
  M [[file:4.6/cpu-rt-rework-cpu-down.patch][4.6]]
  M [[file:4.8/cpu-rt-rework-cpu-down.patch][4.8]]
  M [[file:4.9/cpu-rt-rework-cpu-down.patch][4.9]]
  M [[file:4.11/cpu-rt-rework-cpu-down.patch][4.11]]
  

这里只是参数的类型变了。 目前看到的区别是多了sync_tsk成员，来解决CPU down是percpu的问题。
下面需要追踪一下。


  
-#ifdef CONFIG_PREEMPT_RT_FULL
-# define hotplug_lock()		rt_spin_lock__no_mg(&cpu_hotplug.lock)
-# define hotplug_unlock()	rt_spin_unlock__no_mg(&cpu_hotplug.lock)
-#else
-# define hotplug_lock()		mutex_lock(&cpu_hotplug.lock)
-# define hotplug_unlock()	mutex_unlock(&cpu_hotplug.lock)
-#endif
-

+#ifdef CONFIG_PREEMPT_RT_FULL
+# define hotplug_lock(hp) rt_spin_lock__no_mg(&(hp)->lock)
+# define hotplug_unlock(hp) rt_spin_unlock__no_mg(&(hp)->lock)
+#else
+# define hotplug_lock(hp) mutex_lock(&(hp)->mutex)
+# define hotplug_unlock(hp) mutex_unlock(&(hp)->mutex)
+#endif
+  


struct hotplug_pcp *hp;


+/**
+ * hotplug_pcp - per cpu hotplug descriptor
+ * @unplug:	set when pin_current_cpu() needs to sync tasks
+ * @sync_tsk:	the task that waits for tasks to finish pinned sections
+ * @refcount:	counter of tasks in pinned sections
+ * @grab_lock:	set when the tasks entering pinned sections should wait
+ * @synced:	notifier for @sync_tsk to tell cpu_down it's finished
+ * @mutex:	the mutex to make tasks wait (used when @grab_lock is true)
+ * @mutex_init:	zero if the mutex hasn't been initialized yet.
+ *
+ * Although @unplug and @sync_tsk may point to the same task, the @unplug
+ * is used as a flag and still exists after @sync_tsk has exited and
+ * @sync_tsk set to NULL.
+ */
 struct hotplug_pcp {
 	struct task_struct *unplug;
+	struct task_struct *sync_tsk;
 	int refcount;
+	int grab_lock;
 	struct completion synced;
+#ifdef CONFIG_PREEMPT_RT_FULL
+	spinlock_t lock;
+#else
+	struct mutex mutex;
+#endif
+	int mutex_init;
 };
 
 
  static struct {
 	struct task_struct *active_writer;
-#ifdef CONFIG_PREEMPT_RT_FULL
-	/* Makes the lock keep the task's state */
-	spinlock_t lock;
-#else
 	struct mutex lock; /* Synchronizes accesses to refcount, */
-#endif
 	/*
 	 * Also blocks the new readers during
 	 * an ongoing cpu hotplug operation.
@@ -64,28 +59,46 @@ static struct {
 	int refcount;
 } cpu_hotplug = {
 	.active_writer = NULL,
-#ifdef CONFIG_PREEMPT_RT_FULL
-	.lock = __SPIN_LOCK_UNLOCKED(cpu_hotplug.lock),
-#else
 	.lock = __MUTEX_INITIALIZER(cpu_hotplug.lock),
-#endif
 	.refcount = 0,
 };
