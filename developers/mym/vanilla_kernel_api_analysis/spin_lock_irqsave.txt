

spin_lock_irqsave
{
        local_irq_save(flags);
        preempt_disable();
 queued_spin_lock(&lock->raw_lock);
} 
先关闭中断，然后使用自旋锁（spin_lock会关闭抢占）。

在使用自旋锁的同时关闭中断，可以防止被中断处理程序打断，并且也会进入临界区

spin_unlock_irqrestore
{
        do_raw_spin_unlock(lock);
        local_irq_restore(flags);
        preempt_enable();
}
先将lock置为0，然后恢复中断原来的状态，开启抢占。



static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)
{
 do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); _raw_spin_unlock_irqrestore(&lock->rlock, flags); } while (0);
}

static __always_inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)
{
        raw_spin_unlock_irqrestore(&lock->rlock, flags);
}


#define raw_spin_unlock_irqrestore(lock, flags)         \
        do {                                                    \
                typecheck(unsigned long, flags);                \
                _raw_spin_unlock_irqrestore(lock, flags);       \
        } while (0)
			
./kernel/locking/spinlock.c:189:void __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)
./kernel/locking/spinlock.c:193:EXPORT_SYMBOL(_raw_spin_unlock_irqrestore);

#ifndef CONFIG_INLINE_SPIN_UNLOCK_IRQRESTORE
void __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)
{
        __raw_spin_unlock_irqrestore(lock, flags);
}
EXPORT_SYMBOL(_raw_spin_unlock_irqrestore);
#endif


./include/linux/spinlock_api_smp.h:155:static inline void __raw_spin_unlock_irqrestore(raw_spinlock_t *lock,


static inline void __raw_spin_unlock_irqrestore(raw_spinlock_t *lock,
                                            unsigned long flags)
{
        spin_release(&lock->dep_map, 1, _RET_IP_);
        do_raw_spin_unlock(lock);
        local_irq_restore(flags);
        preempt_enable();
}


static inline __attribute__((no_instrument_function)) void __raw_spin_unlock_irqrestore(raw_spinlock_t *lock,
         unsigned long flags)
{
 do { } while (0);
 do_raw_spin_unlock(lock);
 do { do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); arch_local_irq_restore(flags); } while (0); } while (0);
 __asm__ __volatile__("": : :"memory");
}



static inline __attribute__((no_instrument_function)) void do_raw_spin_unlock(raw_spinlock_t *lock)
{
 queued_spin_unlock(&lock->raw_lock);
 (void)0;
}


		

grep -w  -E "spin_lock_irqsave|spin_unlock_irqrestore" ./* -Rn


./mm/slab.c:784:                        spin_lock_irqsave(&alc->lock, flags);
./mm/slab.c:786:                        spin_unlock_irqrestore(&alc->lock, flags);
./mm/slab.c:1383:               spin_lock_irqsave(&n->list_lock, flags);
./mm/slab.c:1387:               spin_unlock_irqrestore(&n->list_lock, flags);

*/

static void drain_alien_cache(struct kmem_cache *cachep,
                                struct alien_cache **alien)
{
        int i = 0;
        struct alien_cache *alc;
        struct array_cache *ac;
        unsigned long flags;

        for_each_online_node(i) {
                alc = alien[i];
                if (alc) {
                        LIST_HEAD(list);

                        ac = &alc->ac;
                        spin_lock_irqsave(&alc->lock, flags);
                        __drain_alien_cache(cachep, ac, i, &list);
                        spin_unlock_irqrestore(&alc->lock, flags);
                        slabs_destroy(cachep, &list);
                }
        }
}


 do { do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); flags = _raw_spin_lock_irqsave(spinlock_check(&alc->lock)); } while (0); } while (0);
 


#define spin_lock_irqsave(lock,flags) do { raw_spin_lock_irqsave(spinlock_check(lock), flags); } while (0)


#define raw_spin_lock_irqsave(lock,flags) do { typecheck(unsigned long, flags); flags = _raw_spin_lock_irqsave(lock); } while (0)


./kernel/locking/spinlock.c:157:unsigned long __lockfunc _raw_spin_lock_irqsave(raw_spinlock_t *lock)
./kernel/locking/spinlock.c:161:EXPORT_SYMBOL(_raw_spin_lock_irqsave);



#ifndef CONFIG_INLINE_SPIN_LOCK_IRQSAVE
unsigned long __lockfunc _raw_spin_lock_irqsave(raw_spinlock_t *lock)
{
        return __raw_spin_lock_irqsave(lock);
}
EXPORT_SYMBOL(_raw_spin_lock_irqsave);
#endif


./include/linux/spinlock_api_smp.h:104:static inline unsigned long __raw_spin_lock_irqsave(raw_spinlock_t *lock)



/*
 * If lockdep is enabled then we use the non-preemption spin-ops
 * even on CONFIG_PREEMPT, because lockdep assumes that interrupts are
 * not re-enabled during lock-acquire (which the preempt-spin-ops do):
 */
#if !defined(CONFIG_GENERIC_LOCKBREAK) || defined(CONFIG_DEBUG_LOCK_ALLOC)

static inline unsigned long __raw_spin_lock_irqsave(raw_spinlock_t *lock)
{
        unsigned long flags;

        local_irq_save(flags);
        preempt_disable();
        spin_acquire(&lock->dep_map, 0, 0, _RET_IP_);
        /*
         * On lockdep we dont want the hand-coded irq-enable of
         * do_raw_spin_lock_flags() code, because lockdep assumes
         * that interrupts are not re-enabled during lock-acquire:
         */
#ifdef CONFIG_LOCKDEP
        LOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);
#else
        do_raw_spin_lock_flags(lock, &flags);
#endif
        return flags;
}



# 104 "/home/elwin/rt-test-mym/linux-stable-git/linux-stable/include/linux/spinlock_api_smp.h"
static inline __attribute__((no_instrument_function)) unsigned long __raw_spin_lock_irqsave(raw_spinlock_t *lock)
{
 unsigned long flags;

 do { do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); flags = arch_local_irq_save(); } while (0); } while (0);
 __asm__ __volatile__("": : :"memory");
 do { } while (0);
# 119 "/home/elwin/rt-test-mym/linux-stable-git/linux-stable/include/linux/spinlock_api_smp.h"
 do_raw_spin_lock_flags(lock, &flags);

 return flags;
} 


static inline __attribute__((no_instrument_function)) void
do_raw_spin_lock_flags(raw_spinlock_t *lock, unsigned long *flags)
{
 (void)0;
 queued_spin_lock(&lock->raw_lock);
}
