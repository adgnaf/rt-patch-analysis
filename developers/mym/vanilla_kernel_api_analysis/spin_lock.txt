
spin_lock
======================
在PREEMT_VOLUNTARY情况下，其实并没有内核抢占。所以preempt_disable()为空。
spin_lock关闭抢占的操作也为空
最后是通过queued_spin_lock 来实现自旋锁的。



spin_unlock
只是简单的将lock的值设置为了0
没有理解的地方： 那么对与排队等待的自旋锁，是如何顺序的获取到锁的？




./mm/slab.c:603:        spin_lock(&n->list_lock);
./mm/slab.c:605:        spin_unlock(&n->list_lock);
./mm/slab.c:731:                spin_lock(&n->list_lock);
./mm/slab.c:742:                spin_unlock(&n->list_lock);
./mm/slab.c:805:                spin_lock(&alien->lock);
./mm/slab.c:811:                spin_unlock(&alien->lock);
./mm/slab.c:815:                spin_lock(&n->list_lock);
./mm/slab.c:817:                spin_unlock(&n->list_lock);
./mm/slab.c:2251:       spin_lock(&n->list_lock);
./mm/slab.c:2253:       spin_unlock(&n->list_lock);
./mm/slab.c:2746:       spin_lock(&n->list_lock);
./mm/slab.c:2756:       spin_unlock(&n->list_lock);
./mm/slab.c:2950:       spin_lock(&n->list_lock);
./mm/slab.c:2953:               spin_unlock(&n->list_lock);
./mm/slab.c:2962:       spin_unlock(&n->list_lock);
./mm/slab.c:3021:       spin_lock(&n->list_lock);
./mm/slab.c:3045:       spin_unlock(&n->list_lock);
./mm/slab.c:3269:       spin_lock(&n->list_lock);
./mm/slab.c:3287:       spin_unlock(&n->list_lock);
./mm/slab.c:3292:       spin_unlock(&n->list_lock);
./mm/slab.c:3468:       spin_lock(&n->list_lock);
./mm/slab.c:3497:       spin_unlock(&n->list_lock);


static noinline void cache_free_pfmemalloc(struct kmem_cache *cachep,
                                        struct page *page, void *objp)
{
        struct kmem_cache_node *n;
        int page_node;
        LIST_HEAD(list);

        page_node = page_to_nid(page);
        n = get_node(cachep, page_node);

        spin_lock(&n->list_lock);
        free_block(cachep, &objp, 1, page_node, &list);
        spin_unlock(&n->list_lock);

        slabs_destroy(cachep, &list);
}

make O=../v4.11.5/ CFLAGS_KERNEL=-g3 




过程分析 
spin_lock是一个函数不是一个宏定义了
============================
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void spin_lock(spinlock_t *lock)
{
 _raw_spin_lock(&lock->rlock);
}


static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void spin_unlock(spinlock_t *lock)
{
 __raw_spin_unlock(&lock->rlock);
}

./include/linux/preempt.h:171:#define preempt_disable() \
./include/linux/preempt.h:244:#define preempt_disable()                 barrier()

#else /* !CONFIG_PREEMPT_COUNT */

/*
 * Even if we don't have any preemption, we need preempt disable/enable
 * to be barriers, so that we don't have things like get_user/put_user
 * that can cause faults and scheduling migrate into our preempt-protected
 * region.
 */
#define preempt_disable()                       barrier()
#define preempt_enable()                        barrier()
#define preemptible()                           0

#endif /* CONFIG_PREEMPT_COUNT *

发现这里的关闭抢占和开启抢占都是一个memory barryer？

We have PREEMT_VOLUNTARY defined, so that is the reason why the CONFIG_PREEMPT_COUNT is undefined. 


spin_unlock只是将lock的值设置为了0


./kernel/locking/spinlock.c:149:void __lockfunc _raw_spin_lock(raw_spinlock_t *lock)
./kernel/locking/spinlock.c:153:EXPORT_SYMBOL(_raw_spin_lock);


#ifndef CONFIG_INLINE_SPIN_LOCK
void __lockfunc _raw_spin_lock(raw_spinlock_t *lock)
{
        __raw_spin_lock(lock);
}
EXPORT_SYMBOL(_raw_spin_lock);
#endif



static inline __attribute__((no_instrument_function)) void __raw_spin_unlock(raw_spinlock_t *lock)
{
 do { } while (0);
 do_raw_spin_unlock(lock);
 __asm__ __volatile__("": : :"memory");
}

static inline void __raw_spin_unlock(raw_spinlock_t *lock)
{
        spin_release(&lock->dep_map, 1, _RET_IP_);
        do_raw_spin_unlock(lock);
        preempt_enable();
}



static inline __attribute__((no_instrument_function)) void do_raw_spin_unlock(raw_spinlock_t *lock)
{
 queued_spin_unlock(&lock->raw_lock);
 (void)0;
}

static inline void do_raw_spin_unlock(raw_spinlock_t *lock) __releases(lock)
{
        arch_spin_unlock(&lock->raw_lock);
        __release(lock);
}

#define arch_spin_unlock(l) queued_spin_unlock(l)

#define __acquires(x)
#define __releases(x)
#define __acquire(x) (void)0
#define __release(x) (void)0

宏定义的覆盖后面的宏定义会覆盖前面的宏定义。

#define queued_spin_unlock queued_spin_unlock


# 42 "/home/elwin/rt-test-mym/linux-stable-git/linux-stable/arch/x86/include/asm/qspinlock.h"
static inline __attribute__((no_instrument_function)) void queued_spin_unlock(struct qspinlock *lock)
{
 native_queued_spin_unlock(lock);
}


/**
 * queued_spin_unlock - release a queued spinlock
 * @lock : Pointer to queued spinlock structure
 *
 * A smp_store_release() on the least-significant byte.
 */
static inline void native_queued_spin_unlock(struct qspinlock *lock)
{
        smp_store_release((u8 *)lock, 0);
}

#define smp_store_release(p,v) __smp_store_release(p, v)

# 62 "/home/elwin/rt-test-mym/linux-stable-git/linux-stable/arch/x86/include/asm/barrier.h"
#define __smp_store_release(p,v) do { compiletime_assert_atomic_type(*p); barrier(); WRITE_ONCE(*p, v); } while (0)
	


#define WRITE_ONCE(x,val) ({ union { typeof(x) __val; char __c[1]; } __u = { .__val = (__force typeof(x)) (val) }; __write_once_size(&(x), __u.__c, sizeof(x)); __u.__val; })



static inline __attribute__((no_instrument_function)) void native_queued_spin_unlock(struct qspinlock *lock)
{
 do { do { bool __cond = !((sizeof(*(u8 *)lock) == sizeof(char) || sizeof(*(u8 *)lock) == sizeof(short) || sizeof(*(u8 *)lock) == sizeof(int) || sizeof(*(u8 *)lock) == sizeof(long))); extern void __compiletime_assert_17(void) __attribute__((error("Need native word sized stores/loads for atomicity."))); if (__cond) __compiletime_assert_17(); do { } while (0); } while (0); __asm__ __volatile__("": : :"memory"); ({ union { typeof(*(u8 *)lock) __val; char __c[1]; } __u = { .__val = ( typeof(*(u8 *)lock)) (0) }; __write_once_size(&(*(u8 *)lock), __u.__c, sizeof(*(u8 *)lock)); __u.__val; }); } while (0);
}




./include/linux/spinlock_api_smp.h:139:static inline void __raw_spin_lock(raw_spinlock_t *lock)



static inline void __raw_spin_lock(raw_spinlock_t *lock)
{
        preempt_disable();
        spin_acquire(&lock->dep_map, 0, 0, _RET_IP_);
        LOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);
}




LOCK_CONTENDED 会变成 do_raw_spin_lock


make O=../v4.11.5/ CFLAGS_KERNEL=-g3  ./kernel/locking/spinlock.i



static inline __attribute__((no_instrument_function)) void __raw_spin_lock(raw_spinlock_t *lock)
{
 __asm__ __volatile__("": : :"memory");
 do { } while (0);
 do_raw_spin_lock(lock);
}


static inline __attribute__((no_instrument_function)) void do_raw_spin_lock(raw_spinlock_t *lock)
{
 (void)0;
 queued_spin_lock(&lock->raw_lock);
}

# 63 "/home/elwin/rt-test-mym/linux-stable-git/linux-stable/include/asm-generic/qspinlock.h"

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void queued_spin_lock(struct qspinlock *lock)
{
 u32 val;

 val = atomic_cmpxchg(&lock->val, 0, (1U << 0));
 if (__builtin_expect(!!(val == 0), 1))
  return;
 queued_spin_lock_slowpath(lock, val);
}



/**
 * queued_spin_lock - acquire a queued spinlock
 * @lock: Pointer to queued spinlock structure
 */
static __always_inline void queued_spin_lock(struct qspinlock *lock)
{
        u32 val;

        val = atomic_cmpxchg_acquire(&lock->val, 0, _Q_LOCKED_VAL);
        if (likely(val == 0))
                return;
        queued_spin_lock_slowpath(lock, val);
}
上面的val==0应该表示
此时没有任何人在等待锁，直接获取到了spinlock，然后就可以执行临界区代码了。

如果没有获取到锁就需要自旋了。 需要动态分析一下执行过程。


queued_spin_lock_slowpath
应该表示自旋？并且不断的检测是否可以获取到锁？


./kernel/locking/qspinlock.c:410:void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
./kernel/locking/qspinlock.c:624:EXPORT_SYMBOL(queued_spin_lock_slowpath);



下面的函数里面 cpu_relax 操作即表示正在自旋？

/**
 * queued_spin_lock_slowpath - acquire the queued spinlock
 * @lock: Pointer to queued spinlock structure
 * @val: Current value of the queued spinlock 32-bit word
 *
 * (queue tail, pending bit, lock value)
 *
 *              fast     :    slow                                  :    unlock
 *                       :                                          :
 * uncontended  (0,0,0) -:--> (0,0,1) ------------------------------:--> (*,*,0)
 *                       :       | ^--------.------.             /  :
 *                       :       v           \      \            |  :
 * pending               :    (0,1,1) +--> (0,1,0)   \           |  :
 *                       :       | ^--'              |           |  :
 *                       :       v                   |           |  :
 * uncontended           :    (n,x,y) +--> (n,0,0) --'           |  :
 *   queue               :       | ^--'                          |  :
 *                       :       v                               |  :
 * contended             :    (*,x,y) +--> (*,0,0) ---> (*,0,1) -'  :
 *   queue               :         ^--'                             :
 */
void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
{
        struct mcs_spinlock *prev, *next, *node;
        u32 new, old, tail;
        int idx;

        BUILD_BUG_ON(CONFIG_NR_CPUS >= (1U << _Q_TAIL_CPU_BITS));

        if (pv_enabled())
                goto queue;

        if (virt_spin_lock(lock))
                return;

        /*
         * wait for in-progress pending->locked hand-overs
         *
         * 0,1,0 -> 0,0,1
                return;

        /*
         * wait for in-progress pending->locked hand-overs
         *
         * 0,1,0 -> 0,0,1
         */
        if (val == _Q_PENDING_VAL) {
                while ((val = atomic_read(&lock->val)) == _Q_PENDING_VAL)
                        cpu_relax();
        }

        /*
         * trylock || pending
         *
         * 0,0,0 -> 0,0,1 ; trylock
         * 0,0,1 -> 0,1,1 ; pending
         */
        for (;;) {
                /*
                 * If we observe any contention; queue.
                 */
                if (val & ~_Q_LOCKED_MASK)
                        goto queue;

                new = _Q_LOCKED_VAL;
                if (val == new)
                        new |= _Q_PENDING_VAL;

                /*
                 * Acquire semantic is required here as the function may
                 * return immediately if the lock was free.
                 */
                old = atomic_cmpxchg_acquire(&lock->val, val, new);
                if (old == val)
                        break;

                val = old;
        }

        /*
         * we won the trylock
         */
        if (new == _Q_LOCKED_VAL)
                                                                                                                                                     422,1-8       69%
                return;

        /*
         * wait for in-progress pending->locked hand-overs
         *
         * 0,1,0 -> 0,0,1
         */
        if (val == _Q_PENDING_VAL) {
                while ((val = atomic_read(&lock->val)) == _Q_PENDING_VAL)
                        cpu_relax();
        }

        /*
         * trylock || pending
         *
         * 0,0,0 -> 0,0,1 ; trylock
         * 0,0,1 -> 0,1,1 ; pending
         */
        for (;;) {
                /*
                 * If we observe any contention; queue.
                 */
                if (val & ~_Q_LOCKED_MASK)
                        goto queue;

                new = _Q_LOCKED_VAL;
                if (val == new)
                        new |= _Q_PENDING_VAL;

                /*
                 * Acquire semantic is required here as the function may
                 * return immediately if the lock was free.
                 */
                old = atomic_cmpxchg_acquire(&lock->val, val, new);
                if (old == val)
                        break;

                val = old;
        }

        /*
         * we won the trylock
         */
        if (new == _Q_LOCKED_VAL)
                return;

        /*
         * we're pending, wait for the owner to go away.
         *
         * *,1,1 -> *,1,0
         *
         * this wait loop must be a load-acquire such that we match the
         * store-release that clears the locked bit and create lock
         * sequentiality; this is because not all clear_pending_set_locked()
         * implementations imply full barriers.
         */
        smp_cond_load_acquire(&lock->val.counter, !(VAL & _Q_LOCKED_MASK));

        /*
         * take ownership and clear the pending bit.
         *
         * *,1,0 -> *,0,1
         */
        clear_pending_set_locked(lock);
        return;

        /*
         * End of pending bit optimistic spinning and beginning of MCS
         * queuing.
         */
queue:
        node = this_cpu_ptr(&mcs_nodes[0]);
        idx = node->count++;
        tail = encode_tail(smp_processor_id(), idx);

        node += idx;
        node->locked = 0;
        node->next = NULL;
        pv_init_node(node);

        /*
         * We touched a (possibly) cold cacheline in the per-cpu queue node;
         * attempt the trylock once more in the hope someone let go while we
         * weren't watching.
         */
        if (queued_spin_trylock(lock))
                goto release;

        /*
         * We have already touched the queueing cacheline; don't bother with
         * pending stuff.
         *
         * p,*,* -> n,*,*
         *
         * RELEASE, such that the stores to @node must be complete.
         */
        old = xchg_tail(lock, tail);
        next = NULL;

        /*
         * if there was a previous node; link it and wait until reaching the
         * head of the waitqueue.
         */
        if (old & _Q_TAIL_MASK) {
                prev = decode_tail(old);
                /*
                 * The above xchg_tail() is also a load of @lock which generates,
                 * through decode_tail(), a pointer.
                prev = decode_tail(old);
                /*
                 * The above xchg_tail() is also a load of @lock which generates,
                 * through decode_tail(), a pointer.
                 *
                 * The address dependency matches the RELEASE of xchg_tail()
                 * such that the access to @prev must happen after.
                 */
                smp_read_barrier_depends();

                WRITE_ONCE(prev->next, node);

                pv_wait_node(node, prev);
                arch_mcs_spin_lock_contended(&node->locked);

                /*
                 * While waiting for the MCS lock, the next pointer may have
                 * been set by another lock waiter. We optimistically load
                 * the next pointer & prefetch the cacheline for writing
                 * to reduce latency in the upcoming MCS unlock operation.
                 */
                next = READ_ONCE(node->next);
                if (next)
                        prefetchw(next);
        }

        /*
         * we're at the head of the waitqueue, wait for the owner & pending to
         * go away.
         *
         * *,x,y -> *,0,0
         *
         * this wait loop must use a load-acquire such that we match the
         * store-release that clears the locked bit and create lock
         * sequentiality; this is because the set_locked() function below
         * does not imply a full barrier.
         *
         * The PV pv_wait_head_or_lock function, if active, will acquire
         * the lock and return a non-zero value. So we have to skip the
         * smp_cond_load_acquire() call. As the next PV queue head hasn't been
         * designated yet, there is no way for the locked value to become
         * _Q_SLOW_VAL. So both the set_locked() and the
         * atomic_cmpxchg_relaxed() calls will be safe.
         *
         * If PV isn't active, 0 will be returned instead.
         *
         */
        if ((val = pv_wait_head_or_lock(lock, node)))
                goto locked;

        val = smp_cond_load_acquire(&lock->val.counter, !(VAL & _Q_LOCKED_PENDING_MASK));

locked:
        /*
         * claim the lock:
         *
         * n,0,0 -> 0,0,1 : lock, uncontended
         * *,0,0 -> *,0,1 : lock, contended
         *
         * If the queue head is the only one in the queue (lock value == tail),
         * clear the tail code and grab the lock. Otherwise, we only need
         * to grab the lock.
         */
        for (;;) {
                /* In the PV case we might already have _Q_LOCKED_VAL set */
                if ((val & _Q_TAIL_MASK) != tail) {
                        set_locked(lock);
                        break;
                }
                /*
                 * The smp_cond_load_acquire() call above has provided the
                 * necessary acquire semantics required for locking. At most
                 * two iterations of this loop may be ran.
                 */
                old = atomic_cmpxchg_relaxed(&lock->val, val, _Q_LOCKED_VAL);
                if (old == val)
                        goto release;   /* No contention */

                val = old;
        }

        /*
         * contended path; wait for next if not observed yet, release.
         */
        if (!next) {
                while (!(next = READ_ONCE(node->next)))
                        cpu_relax();
        }

        arch_mcs_spin_unlock_contended(&next->locked);
        pv_kick_node(lock, next);

release:
        /*
         * release the node
         */
        __this_cpu_dec(mcs_nodes[0].count);
}
EXPORT_SYMBOL(queued_spin_lock_slowpath);				 