
在CONFIG_PREEMPT_COUNT is not set 
CONFIG_PREEEMPT_VOLUNTORY=y
的情况下，rcu_read_lock和rcu_read_unlock
变成了do nothing.

也就是说RCU里面，读者不需要任何锁。对写者要求比较多。


===============================
# 831 "/home/elwin/rt-test-mym/linux-stable-git/linux-stable/include/linux/rcupdate.h"
static inline __attribute__((no_instrument_function)) void rcu_read_lock(void)
{
 __rcu_read_lock();
 (void)0;
 do { } while (0);
 do { } while (0)
                                                ;
}
# 885 "/home/elwin/rt-test-mym/linux-stable-git/linux-stable/include/linux/rcupdate.h"
static inline __attribute__((no_instrument_function)) void rcu_read_unlock(void)
{
 do { } while (0)
                                                  ;
 (void)0;
 __rcu_read_unlock();
 do { } while (0);
}





#else /* #ifdef CONFIG_PREEMPT_RCU */

static inline void __rcu_read_lock(void)
{
        if (IS_ENABLED(CONFIG_PREEMPT_COUNT))
                preempt_disable();
}

static inline void __rcu_read_unlock(void)
{
        if (IS_ENABLED(CONFIG_PREEMPT_COUNT))
                preempt_enable();
}



# 267 "/home/elwin/rt-test-mym/linux-stable-git/linux-stable/include/linux/rcupdate.h"
static inline __attribute__((no_instrument_function)) void __rcu_read_lock(void)
{
 if (0)
  __asm__ __volatile__("": : :"memory");
}

static inline __attribute__((no_instrument_function)) void __rcu_read_unlock(void)
{
 if (0)
  __asm__ __volatile__("": : :"memory");
}



使用情况
===================================
./kernel/sched/core.c:541:      rcu_read_lock();
./kernel/sched/core.c:557:      rcu_read_unlock();
./kernel/sched/core.c:1613:             rcu_read_lock();
./kernel/sched/core.c:1620:             rcu_read_unlock();
./kernel/sched/core.c:1811:     rcu_read_lock();
./kernel/sched/core.c:1827:     rcu_read_unlock();
./kernel/sched/core.c:4097:     rcu_read_lock();
./kernel/sched/core.c:4101:     rcu_read_unlock();
./kernel/sched/core.c:4430:     rcu_read_lock();
./kernel/sched/core.c:4435:     rcu_read_unlock();
./kernel/sched/core.c:4561:     rcu_read_lock();
./kernel/sched/core.c:4566:     rcu_read_unlock();
./kernel/sched/core.c:4587:     rcu_read_lock();
./kernel/sched/core.c:4595:     rcu_read_unlock();
./kernel/sched/core.c:4616:     rcu_read_lock();
./kernel/sched/core.c:4628:     rcu_read_unlock();
./kernel/sched/core.c:4638:     rcu_read_unlock();
./kernel/sched/core.c:4698:     rcu_read_lock();
./kernel/sched/core.c:4718:     rcu_read_unlock();
./kernel/sched/core.c:4724:     rcu_read_unlock();
./kernel/sched/core.c:4734:     rcu_read_lock();



#ifdef CONFIG_SMP
#ifdef CONFIG_NO_HZ_COMMON
/*
 * In the semi idle case, use the nearest busy CPU for migrating timers
 * from an idle CPU.  This is good for power-savings.
 *
 * We don't do similar optimization for completely idle system, as
 * selecting an idle CPU will add more delays to the timers than intended
 * (as that CPU's timer base may not be uptodate wrt jiffies etc).
 */
int get_nohz_timer_target(void)
{
        int i, cpu = smp_processor_id();
        struct sched_domain *sd;

        if (!idle_cpu(cpu) && is_housekeeping_cpu(cpu))
                return cpu;

        rcu_read_lock();
        for_each_domain(cpu, sd) {
                for_each_cpu(i, sched_domain_span(sd)) {
                        if (cpu == i)
                                continue;

                        if (!idle_cpu(i) && is_housekeeping_cpu(i)) {
                                cpu = i;
                                goto unlock;
                        }
                }
        }

        if (!is_housekeeping_cpu(cpu))
                cpu = housekeeping_any_cpu();
unlock:
        rcu_read_unlock();
        return cpu;
}



static int __rt_schedulable(struct task_group *tg, u64 period, u64 runtime)
{
        int ret;

        struct rt_schedulable_data data = {
                .tg = tg,
                .rt_period = period,
                .rt_runtime = runtime,
        };

        rcu_read_lock();
        ret = walk_tg_tree(tg_rt_schedulable, tg_nop, &data);
        rcu_read_unlock();

        return ret;
}


