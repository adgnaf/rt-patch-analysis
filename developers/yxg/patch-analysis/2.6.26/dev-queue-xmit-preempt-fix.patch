From mingo@elte.hu Fri Jan 11 14:56:57 2008
Date: Thu, 3 Jan 2008 09:22:03 +0100
From: Ingo Molnar <mingo@elte.hu>
To: Steven Rostedt <rostedt@goodmis.org>
Subject: [mbeauch@cox.net: FW: [PATCH -rt] Preemption problem in kernel RT
    Patch]


----- Forwarded message from mbeauch <mbeauch@cox.net> -----

Date: Wed, 02 Jan 2008 20:27:09 -0500
From: mbeauch <mbeauch@cox.net>
To: mingo@elte.hu
Subject: FW: [PATCH -rt] Preemption problem in kernel RT Patch

Here's the updated patch:

Changed the real-time patch code to detect recursive calls 
to dev_queue_xmit and drop the packet when detected. 


Signed-off-by: Mark Beauchemin <mark.beauchemin@sycamorenet.com> 


---
 include/linux/netdevice.h |   20 ++++++++++----------
 net/core/dev.c            |   14 +++-----------
 net/sched/sch_generic.c   |    4 ++--
 3 files changed, 15 insertions(+), 23 deletions(-)

Index: linux-2.6.26/include/linux/netdevice.h
===================================================================
--- linux-2.6.26.orig/include/linux/netdevice.h
+++ linux-2.6.26/include/linux/netdevice.h
@@ -649,7 +649,7 @@ struct net_device
 	/* cpu id of processor entered to hard_start_xmit or -1,
 	   if nobody entered there.
 	 */
-	int			xmit_lock_owner;
+	void			*xmit_lock_owner;
 	void			*priv;	/* pointer to private data	*/
 	int			(*hard_start_xmit) (struct sk_buff *skb,
 						    struct net_device *dev);
@@ -1397,46 +1397,46 @@ static inline void netif_rx_complete(str
  *
  * Get network device transmit lock
  */
-static inline void __netif_tx_lock(struct net_device *dev, int cpu)
+static inline void __netif_tx_lock(struct net_device *dev)
 {
 	spin_lock(&dev->_xmit_lock);
-	dev->xmit_lock_owner = cpu;
+	dev->xmit_lock_owner = (void *)current;
 }
 
 static inline void netif_tx_lock(struct net_device *dev)
 {
-	__netif_tx_lock(dev, raw_smp_processor_id());
+	__netif_tx_lock(dev);
 }
 
 static inline void netif_tx_lock_bh(struct net_device *dev)
 {
 	spin_lock_bh(&dev->_xmit_lock);
-	dev->xmit_lock_owner = raw_smp_processor_id();
+	dev->xmit_lock_owner = (void *)current;
 }
 
 static inline int netif_tx_trylock(struct net_device *dev)
 {
 	int ok = spin_trylock(&dev->_xmit_lock);
 	if (likely(ok))
-		dev->xmit_lock_owner = raw_smp_processor_id();
+		dev->xmit_lock_owner = (void *)current;
 	return ok;
 }
 
 static inline void netif_tx_unlock(struct net_device *dev)
 {
-	dev->xmit_lock_owner = -1;
+	dev->xmit_lock_owner = (void *)-1;
 	spin_unlock(&dev->_xmit_lock);
 }
 
 static inline void netif_tx_unlock_bh(struct net_device *dev)
 {
-	dev->xmit_lock_owner = -1;
+	dev->xmit_lock_owner = (void *)-1;
 	spin_unlock_bh(&dev->_xmit_lock);
 }
 
-#define HARD_TX_LOCK(dev, cpu) {			\
+#define HARD_TX_LOCK(dev) {			\
 	if ((dev->features & NETIF_F_LLTX) == 0) {	\
-		__netif_tx_lock(dev, cpu);			\
+		__netif_tx_lock(dev);			\
 	}						\
 }
 
Index: linux-2.6.26/net/core/dev.c
===================================================================
--- linux-2.6.26.orig/net/core/dev.c
+++ linux-2.6.26/net/core/dev.c
@@ -1722,18 +1722,10 @@ gso:
 	   Either shot noqueue qdisc, it is even simpler 8)
 	 */
 	if (dev->flags & IFF_UP) {
-		int cpu = raw_smp_processor_id(); /* ok because BHs are off */
 
-		/*
-		 * No need to check for recursion with threaded interrupts:
-		 */
-#ifdef CONFIG_PREEMPT_RT
-		if (1) {
-#else
-		if (dev->xmit_lock_owner != cpu) {
-#endif
+		if (dev->xmit_lock_owner != (void *)current) {
 
-			HARD_TX_LOCK(dev, cpu);
+			HARD_TX_LOCK(dev);
 
 			if (!netif_queue_stopped(dev) &&
 			    !netif_subqueue_stopped(dev, skb)) {
@@ -3754,7 +3746,7 @@ int register_netdevice(struct net_device
 	spin_lock_init(&dev->queue_lock);
 	spin_lock_init(&dev->_xmit_lock);
 	netdev_set_lockdep_class(&dev->_xmit_lock, dev->type);
-	dev->xmit_lock_owner = -1;
+	dev->xmit_lock_owner = (void *)-1;
 	spin_lock_init(&dev->ingress_lock);
 
 	dev->iflink = -1;
Index: linux-2.6.26/net/sched/sch_generic.c
===================================================================
--- linux-2.6.26.orig/net/sched/sch_generic.c
+++ linux-2.6.26/net/sched/sch_generic.c
@@ -95,7 +95,7 @@ static inline int handle_dev_cpu_collisi
 {
 	int ret;
 
-	if (unlikely(dev->xmit_lock_owner == raw_smp_processor_id())) {
+	if (unlikely(dev->xmit_lock_owner == (void *)current)) {
 		/*
 		 * Same CPU holding the lock. It may be a transient
 		 * configuration error, when hard_start_xmit() recurses. We
@@ -152,7 +152,7 @@ static inline int qdisc_restart(struct n
 	/* And release queue */
 	spin_unlock(&dev->queue_lock);
 
-	HARD_TX_LOCK(dev, raw_smp_processor_id());
+	HARD_TX_LOCK(dev);
 	if (!netif_subqueue_stopped(dev, skb))
 		ret = dev_hard_start_xmit(skb, dev);
 	HARD_TX_UNLOCK(dev);
