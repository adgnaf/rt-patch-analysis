From: Steven Rostedt <srostedt@redhat.com>

With the added boost of SCHED_OTHER, the balancing load starts to stain
latencies. Bring it back down again.

NOTE: This is a workaround, we need to fix this because this work around
will once again hurt SCHED_OTHER performance.

Signed-off-by: Steven Rostedt <srostedt@redhat.com>
---
 kernel/sched.c |    4 ++++
 1 file changed, 4 insertions(+)

Index: linux-2.6.26/kernel/sched.c
===================================================================
--- linux-2.6.26.orig/kernel/sched.c
+++ linux-2.6.26/kernel/sched.c
@@ -833,7 +833,11 @@ late_initcall(sched_init_debug);
  * Number of tasks to iterate in a single balance run.
  * Limited because this is done with IRQs disabled.
  */
+#ifdef CONFIG_PREEMPT_RT
+const_debug unsigned int sysctl_sched_nr_migrate = 8;
+#else
 const_debug unsigned int sysctl_sched_nr_migrate = 32;
+#endif
 
 /*
  * period over which we measure -rt task cpu usage in us.
