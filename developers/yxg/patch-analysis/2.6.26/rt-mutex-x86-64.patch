---
 arch/x86/Kconfig              |    2 +-
 arch/x86/kernel/entry_64.S    |   18 +++++++++---------
 arch/x86/kernel/traps_64.c    |    8 ++++----
 arch/x86/kernel/tsc_sync.c    |    2 +-
 arch/x86/kernel/vsyscall_64.c |    2 +-
 5 files changed, 16 insertions(+), 16 deletions(-)

Index: linux-2.6.26/arch/x86/Kconfig
===================================================================
--- linux-2.6.26.orig/arch/x86/Kconfig
+++ linux-2.6.26/arch/x86/Kconfig
@@ -105,7 +105,7 @@ config ASM_SEMAPHORES
 
 config RWSEM_XCHGADD_ALGORITHM
 	bool
-	depends on X86_XADD && !RWSEM_GENERIC_SPINLOCK
+	depends on X86_XADD && !RWSEM_GENERIC_SPINLOCK && !PREEMPT_RT
 	default y
 
 config ARCH_HAS_ILOG2_U32
Index: linux-2.6.26/arch/x86/kernel/entry_64.S
===================================================================
--- linux-2.6.26.orig/arch/x86/kernel/entry_64.S
+++ linux-2.6.26/arch/x86/kernel/entry_64.S
@@ -391,8 +391,8 @@ sysret_check:		
 	/* Handle reschedules */
 	/* edx:	work, edi: workmask */	
 sysret_careful:
-	bt $TIF_NEED_RESCHED,%edx
-	jnc sysret_signal
+	testl $(_TIF_NEED_RESCHED),%edx
+	jz sysret_signal
 	TRACE_IRQS_ON
 	ENABLE_INTERRUPTS(CLBR_NONE)
 	pushq %rdi
@@ -415,7 +415,7 @@ sysret_signal:
 	leaq -ARGOFFSET(%rsp),%rdi # &pt_regs -> arg1
 	xorl %esi,%esi # oldset -> arg2
 	call ptregscall_common
-1:	movl $_TIF_NEED_RESCHED,%edi
+1:	movl $(_TIF_NEED_RESCHED),%edi
 	/* Use IRET because user could have changed frame. This
 	   works because ptregscall_common has called FIXUP_TOP_OF_STACK. */
 	DISABLE_INTERRUPTS(CLBR_NONE)
@@ -469,8 +469,8 @@ int_with_check:
 	/* First do a reschedule test. */
 	/* edx:	work, edi: workmask */
 int_careful:
-	bt $TIF_NEED_RESCHED,%edx
-	jnc  int_very_careful
+	testl $(_TIF_NEED_RESCHED),%edx
+	jz int_very_careful
 	TRACE_IRQS_ON
 	ENABLE_INTERRUPTS(CLBR_NONE)
 	pushq %rdi
@@ -505,7 +505,7 @@ int_signal:
 	movq %rsp,%rdi		# &ptregs -> arg1
 	xorl %esi,%esi		# oldset -> arg2
 	call do_notify_resume
-1:	movl $_TIF_NEED_RESCHED,%edi	
+1:	movl $(_TIF_NEED_RESCHED),%edi
 int_restore_rest:
 	RESTORE_REST
 	DISABLE_INTERRUPTS(CLBR_NONE)
@@ -732,8 +732,8 @@ bad_iret:
 	/* edi: workmask, edx: work */
 retint_careful:
 	CFI_RESTORE_STATE
-	bt    $TIF_NEED_RESCHED,%edx
-	jnc   retint_signal
+	testl $(_TIF_NEED_RESCHED),%edx
+	jz    retint_signal
 	TRACE_IRQS_ON
 	ENABLE_INTERRUPTS(CLBR_NONE)
 	pushq %rdi
@@ -759,7 +759,7 @@ retint_signal:
 	RESTORE_REST
 	DISABLE_INTERRUPTS(CLBR_NONE)
 	TRACE_IRQS_OFF
-	movl $_TIF_NEED_RESCHED,%edi
+	movl $(_TIF_NEED_RESCHED),%edi
 	GET_THREAD_INFO(%rcx)
 	jmp retint_check
 
Index: linux-2.6.26/arch/x86/kernel/traps_64.c
===================================================================
--- linux-2.6.26.orig/arch/x86/kernel/traps_64.c
+++ linux-2.6.26/arch/x86/kernel/traps_64.c
@@ -518,7 +518,7 @@ int is_valid_bugaddr(unsigned long ip)
 	return ud2 == 0x0b0f;
 }
 
-static raw_spinlock_t die_lock = __RAW_SPIN_LOCK_UNLOCKED;
+static raw_spinlock_t die_lock = RAW_SPIN_LOCK_UNLOCKED(die_lock);
 static int die_owner = -1;
 static unsigned int die_nest_count;
 
@@ -532,11 +532,11 @@ unsigned __kprobes long oops_begin(void)
 	/* racy, but better than risking deadlock. */
 	raw_local_irq_save(flags);
 	cpu = smp_processor_id();
-	if (!__raw_spin_trylock(&die_lock)) {
+	if (!spin_trylock(&die_lock)) {
 		if (cpu == die_owner) 
 			/* nested oops. should stop eventually */;
 		else
-			__raw_spin_lock(&die_lock);
+			spin_lock(&die_lock);
 	}
 	die_nest_count++;
 	die_owner = cpu;
@@ -552,7 +552,7 @@ void __kprobes oops_end(unsigned long fl
 	die_nest_count--;
 	if (!die_nest_count)
 		/* Nest count reaches zero, release the lock. */
-		__raw_spin_unlock(&die_lock);
+		spin_unlock(&die_lock);
 	raw_local_irq_restore(flags);
 	if (!regs) {
 		oops_exit();
Index: linux-2.6.26/arch/x86/kernel/tsc_sync.c
===================================================================
--- linux-2.6.26.orig/arch/x86/kernel/tsc_sync.c
+++ linux-2.6.26/arch/x86/kernel/tsc_sync.c
@@ -33,7 +33,7 @@ static __cpuinitdata atomic_t stop_count
  * we want to have the fastest, inlined, non-debug version
  * of a critical section, to be able to prove TSC time-warps:
  */
-static __cpuinitdata raw_spinlock_t sync_lock = __RAW_SPIN_LOCK_UNLOCKED;
+static __cpuinitdata __raw_spinlock_t sync_lock = __RAW_SPIN_LOCK_UNLOCKED;
 static __cpuinitdata cycles_t last_tsc;
 static __cpuinitdata cycles_t max_warp;
 static __cpuinitdata int nr_warps;
Index: linux-2.6.26/arch/x86/kernel/vsyscall_64.c
===================================================================
--- linux-2.6.26.orig/arch/x86/kernel/vsyscall_64.c
+++ linux-2.6.26/arch/x86/kernel/vsyscall_64.c
@@ -56,7 +56,7 @@ int __vgetcpu_mode __section_vgetcpu_mod
 
 struct vsyscall_gtod_data __vsyscall_gtod_data __section_vsyscall_gtod_data =
 {
-	.lock = SEQLOCK_UNLOCKED,
+	.lock = __RAW_SEQLOCK_UNLOCKED(__vsyscall_gtod_data.lock),
 	.sysctl_enabled = 1,
 };
 
