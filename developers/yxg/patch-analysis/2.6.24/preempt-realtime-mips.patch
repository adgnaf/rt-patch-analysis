 arch/mips/Kconfig                 |   13 ++
 arch/mips/kernel/asm-offsets.c    |    2 
 arch/mips/kernel/entry.S          |   22 +++-
 arch/mips/kernel/i8259.c          |    2 
 arch/mips/kernel/module.c         |    2 
 arch/mips/kernel/process.c        |    8 +
 arch/mips/kernel/scall32-o32.S    |    2 
 arch/mips/kernel/scall64-64.S     |    2 
 arch/mips/kernel/scall64-n32.S    |    2 
 arch/mips/kernel/scall64-o32.S    |    2 
 arch/mips/kernel/semaphore.c      |   22 +++-
 arch/mips/kernel/signal.c         |    4 
 arch/mips/kernel/signal32.c       |    4 
 arch/mips/kernel/smp.c            |   27 +++++
 arch/mips/kernel/traps.c          |    2 
 arch/mips/mm/init.c               |    2 
 arch/mips/sibyte/cfe/smp.c        |    4 
 arch/mips/sibyte/sb1250/irq.c     |    6 +
 arch/mips/sibyte/sb1250/smp.c     |    2 
 arch/mips/sibyte/swarm/setup.c    |    6 +
 include/asm-mips/asmmacro.h       |    8 -
 include/asm-mips/atomic.h         |    1 
 include/asm-mips/bitops.h         |    5 -
 include/asm-mips/hw_irq.h         |    1 
 include/asm-mips/i8259.h          |    2 
 include/asm-mips/io.h             |    1 
 include/asm-mips/linkage.h        |    5 +
 include/asm-mips/m48t35.h         |    2 
 include/asm-mips/rwsem.h          |  176 ++++++++++++++++++++++++++++++++++++++
 include/asm-mips/semaphore.h      |   31 +++---
 include/asm-mips/spinlock.h       |   18 +--
 include/asm-mips/spinlock_types.h |    4 
 include/asm-mips/thread_info.h    |    2 
 include/asm-mips/time.h           |    2 
 include/asm-mips/timeofday.h      |    5 +
 include/asm-mips/uaccess.h        |   12 --
 36 files changed, 331 insertions(+), 80 deletions(-)

Index: linux-2.6.24-rt1/arch/mips/Kconfig
===================================================================
--- linux-2.6.24-rt1.orig/arch/mips/Kconfig	2008-01-25 15:07:33.000000000 -0500
+++ linux-2.6.24-rt1/arch/mips/Kconfig	2008-01-25 15:07:43.000000000 -0500
@@ -702,18 +702,16 @@ source "arch/mips/vr41xx/Kconfig"
 
 endmenu
 
+
 config RWSEM_GENERIC_SPINLOCK
 	bool
-	depends on !PREEMPT_RT
 	default y
 
 config RWSEM_XCHGADD_ALGORITHM
 	bool
-	depends on !PREEMPT_RT
 
 config ASM_SEMAPHORES
 	bool
-#	depends on !PREEMPT_RT
 	default y
 
 config ARCH_HAS_ILOG2_U32
@@ -1898,6 +1896,15 @@ config SECCOMP
 
 	  If unsure, say Y. Only embedded should say N here.
 
+config GENERIC_TIME
+	bool
+	default y
+
+source "kernel/time/Kconfig"
+
+config CPU_SPEED
+	int "CPU speed used for clocksource/clockevent calculations"
+	default 600
 endmenu
 
 config LOCKDEP_SUPPORT
Index: linux-2.6.24-rt1/arch/mips/kernel/asm-offsets.c
===================================================================
--- linux-2.6.24-rt1.orig/arch/mips/kernel/asm-offsets.c	2008-01-25 15:06:35.000000000 -0500
+++ linux-2.6.24-rt1/arch/mips/kernel/asm-offsets.c	2008-01-25 15:07:43.000000000 -0500
@@ -10,9 +10,11 @@
  */
 #include <linux/compat.h>
 #include <linux/types.h>
+#include <linux/linkage.h>
 #include <linux/sched.h>
 #include <linux/mm.h>
 #include <linux/interrupt.h>
+#include <linux/irqflags.h>
 
 #include <asm/ptrace.h>
 #include <asm/processor.h>
Index: linux-2.6.24-rt1/arch/mips/kernel/entry.S
===================================================================
--- linux-2.6.24-rt1.orig/arch/mips/kernel/entry.S	2008-01-25 15:06:35.000000000 -0500
+++ linux-2.6.24-rt1/arch/mips/kernel/entry.S	2008-01-25 15:07:43.000000000 -0500
@@ -30,7 +30,7 @@
 	.align	5
 #ifndef CONFIG_PREEMPT
 FEXPORT(ret_from_exception)
-	local_irq_disable			# preempt stop
+	raw_local_irq_disable			# preempt stop
 	b	__ret_from_irq
 #endif
 FEXPORT(ret_from_irq)
@@ -41,7 +41,7 @@ FEXPORT(__ret_from_irq)
 	beqz	t0, resume_kernel
 
 resume_userspace:
-	local_irq_disable		# make sure we dont miss an
+	raw_local_irq_disable	# make sure we dont miss an
 					# interrupt setting need_resched
 					# between sampling and return
 	LONG_L	a2, TI_FLAGS($28)	# current->work
@@ -51,7 +51,9 @@ resume_userspace:
 
 #ifdef CONFIG_PREEMPT
 resume_kernel:
-	local_irq_disable
+	raw_local_irq_disable
+	lw	t0, kernel_preemption
+	beqz	t0, restore_all
 	lw	t0, TI_PRE_COUNT($28)
 	bnez	t0, restore_all
 need_resched:
@@ -61,7 +63,9 @@ need_resched:
 	LONG_L	t0, PT_STATUS(sp)		# Interrupts off?
 	andi	t0, 1
 	beqz	t0, restore_all
+	raw_local_irq_disable
 	jal	preempt_schedule_irq
+	sw      zero, TI_PRE_COUNT($28)
 	b	need_resched
 #endif
 
@@ -69,7 +73,7 @@ FEXPORT(ret_from_fork)
 	jal	schedule_tail		# a0 = struct task_struct *prev
 
 FEXPORT(syscall_exit)
-	local_irq_disable		# make sure need_resched and
+	raw_local_irq_disable	# make sure need_resched and
 					# signals dont change between
 					# sampling and return
 	LONG_L	a2, TI_FLAGS($28)	# current->work
@@ -142,19 +146,21 @@ FEXPORT(restore_partial)		# restore part
 	.set	at
 
 work_pending:
-	andi	t0, a2, _TIF_NEED_RESCHED # a2 is preloaded with TI_FLAGS
+					# a2 is preloaded with TI_FLAGS
+	andi	t0, a2, (_TIF_NEED_RESCHED|_TIF_NEED_RESCHED_DELAYED)
 	beqz	t0, work_notifysig
 work_resched:
+	raw_local_irq_enable  t0
 	jal	schedule
 
-	local_irq_disable		# make sure need_resched and
+	raw_local_irq_disable	# make sure need_resched and
 					# signals dont change between
 					# sampling and return
 	LONG_L	a2, TI_FLAGS($28)
 	andi	t0, a2, _TIF_WORK_MASK	# is there any work to be done
 					# other than syscall tracing?
 	beqz	t0, restore_all
-	andi	t0, a2, _TIF_NEED_RESCHED
+	andi	t0, a2, (_TIF_NEED_RESCHED|_TIF_NEED_RESCHED_DELAYED)
 	bnez	t0, work_resched
 
 work_notifysig:				# deal with pending signals and
@@ -170,7 +176,7 @@ syscall_exit_work:
 	li	t0, _TIF_SYSCALL_TRACE | _TIF_SYSCALL_AUDIT
 	and	t0, a2			# a2 is preloaded with TI_FLAGS
 	beqz	t0, work_pending	# trace bit set?
-	local_irq_enable		# could let do_syscall_trace()
+	raw_local_irq_enable	# could let do_syscall_trace()
 					# call schedule() instead
 	move	a0, sp
 	li	a1, 1
Index: linux-2.6.24-rt1/arch/mips/kernel/i8259.c
===================================================================
--- linux-2.6.24-rt1.orig/arch/mips/kernel/i8259.c	2008-01-25 15:06:35.000000000 -0500
+++ linux-2.6.24-rt1/arch/mips/kernel/i8259.c	2008-01-25 15:07:43.000000000 -0500
@@ -29,7 +29,7 @@
  */
 
 static int i8259A_auto_eoi = -1;
-DEFINE_SPINLOCK(i8259A_lock);
+DEFINE_RAW_SPINLOCK(i8259A_lock);
 static void disable_8259A_irq(unsigned int irq);
 static void enable_8259A_irq(unsigned int irq);
 static void mask_and_ack_8259A(unsigned int irq);
Index: linux-2.6.24-rt1/arch/mips/kernel/module.c
===================================================================
--- linux-2.6.24-rt1.orig/arch/mips/kernel/module.c	2008-01-25 15:06:35.000000000 -0500
+++ linux-2.6.24-rt1/arch/mips/kernel/module.c	2008-01-25 15:07:43.000000000 -0500
@@ -40,7 +40,7 @@ struct mips_hi16 {
 static struct mips_hi16 *mips_hi16_list;
 
 static LIST_HEAD(dbe_list);
-static DEFINE_SPINLOCK(dbe_lock);
+static DEFINE_RAW_SPINLOCK(dbe_lock);
 
 void *module_alloc(unsigned long size)
 {
Index: linux-2.6.24-rt1/arch/mips/kernel/process.c
===================================================================
--- linux-2.6.24-rt1.orig/arch/mips/kernel/process.c	2008-01-25 15:06:35.000000000 -0500
+++ linux-2.6.24-rt1/arch/mips/kernel/process.c	2008-01-25 15:07:43.000000000 -0500
@@ -54,7 +54,7 @@ void __noreturn cpu_idle(void)
 	/* endless idle loop with no priority at all */
 	while (1) {
 		tick_nohz_stop_sched_tick();
-		while (!need_resched()) {
+		while (!need_resched() && !need_resched_delayed()) {
 #ifdef CONFIG_SMTC_IDLE_HOOK_DEBUG
 			extern void smtc_idle_loop_hook(void);
 
@@ -64,9 +64,11 @@ void __noreturn cpu_idle(void)
 				(*cpu_wait)();
 		}
 		tick_nohz_restart_sched_tick();
-		preempt_enable_no_resched();
-		schedule();
+		local_irq_disable();
+		__preempt_enable_no_resched();
+		__schedule();
 		preempt_disable();
+		local_irq_enable();
 	}
 }
 
Index: linux-2.6.24-rt1/arch/mips/kernel/scall32-o32.S
===================================================================
--- linux-2.6.24-rt1.orig/arch/mips/kernel/scall32-o32.S	2008-01-25 15:06:35.000000000 -0500
+++ linux-2.6.24-rt1/arch/mips/kernel/scall32-o32.S	2008-01-25 15:07:43.000000000 -0500
@@ -73,7 +73,7 @@ stack_done:
 1:	sw	v0, PT_R2(sp)		# result
 
 o32_syscall_exit:
-	local_irq_disable		# make sure need_resched and
+	raw_local_irq_disable	# make sure need_resched and
 					# signals dont change between
 					# sampling and return
 	lw	a2, TI_FLAGS($28)	# current->work
Index: linux-2.6.24-rt1/arch/mips/kernel/scall64-64.S
===================================================================
--- linux-2.6.24-rt1.orig/arch/mips/kernel/scall64-64.S	2008-01-25 15:06:35.000000000 -0500
+++ linux-2.6.24-rt1/arch/mips/kernel/scall64-64.S	2008-01-25 15:07:43.000000000 -0500
@@ -72,7 +72,7 @@ NESTED(handle_sys64, PT_SIZE, sp)
 1:	sd	v0, PT_R2(sp)		# result
 
 n64_syscall_exit:
-	local_irq_disable		# make sure need_resched and
+	raw_local_irq_disable		# make sure need_resched and
 					# signals dont change between
 					# sampling and return
 	LONG_L	a2, TI_FLAGS($28)	# current->work
Index: linux-2.6.24-rt1/arch/mips/kernel/scall64-n32.S
===================================================================
--- linux-2.6.24-rt1.orig/arch/mips/kernel/scall64-n32.S	2008-01-25 15:06:35.000000000 -0500
+++ linux-2.6.24-rt1/arch/mips/kernel/scall64-n32.S	2008-01-25 15:07:43.000000000 -0500
@@ -69,7 +69,7 @@ NESTED(handle_sysn32, PT_SIZE, sp)
 	sd	v0, PT_R0(sp)		# set flag for syscall restarting
 1:	sd	v0, PT_R2(sp)		# result
 
-	local_irq_disable		# make sure need_resched and
+	raw_local_irq_disable		# make sure need_resched and
 					# signals dont change between
 					# sampling and return
 	LONG_L  a2, TI_FLAGS($28)	# current->work
Index: linux-2.6.24-rt1/arch/mips/kernel/scall64-o32.S
===================================================================
--- linux-2.6.24-rt1.orig/arch/mips/kernel/scall64-o32.S	2008-01-25 15:06:35.000000000 -0500
+++ linux-2.6.24-rt1/arch/mips/kernel/scall64-o32.S	2008-01-25 15:07:43.000000000 -0500
@@ -98,7 +98,7 @@ NESTED(handle_sys, PT_SIZE, sp)
 1:	sd	v0, PT_R2(sp)		# result
 
 o32_syscall_exit:
-	local_irq_disable		# make need_resched and
+	raw_local_irq_disable		# make need_resched and
 					# signals dont change between
 					# sampling and return
 	LONG_L	a2, TI_FLAGS($28)
Index: linux-2.6.24-rt1/arch/mips/kernel/semaphore.c
===================================================================
--- linux-2.6.24-rt1.orig/arch/mips/kernel/semaphore.c	2008-01-25 15:06:35.000000000 -0500
+++ linux-2.6.24-rt1/arch/mips/kernel/semaphore.c	2008-01-25 15:07:43.000000000 -0500
@@ -36,7 +36,7 @@
  * sem->count and sem->waking atomic.  Scalability isn't an issue because
  * this lock is used on UP only so it's just an empty variable.
  */
-static inline int __sem_update_count(struct semaphore *sem, int incr)
+static inline int __sem_update_count(struct compat_semaphore *sem, int incr)
 {
 	int old_count, tmp;
 
@@ -67,7 +67,7 @@ static inline int __sem_update_count(str
 		: "=&r" (old_count), "=&r" (tmp), "=m" (sem->count)
 		: "r" (incr), "m" (sem->count));
 	} else {
-		static DEFINE_SPINLOCK(semaphore_lock);
+		static DEFINE_RAW_SPINLOCK(semaphore_lock);
 		unsigned long flags;
 
 		spin_lock_irqsave(&semaphore_lock, flags);
@@ -80,7 +80,7 @@ static inline int __sem_update_count(str
 	return old_count;
 }
 
-void __up(struct semaphore *sem)
+void __compat_up(struct compat_semaphore *sem)
 {
 	/*
 	 * Note that we incremented count in up() before we came here,
@@ -94,7 +94,7 @@ void __up(struct semaphore *sem)
 	wake_up(&sem->wait);
 }
 
-EXPORT_SYMBOL(__up);
+EXPORT_SYMBOL(__compat_up);
 
 /*
  * Note that when we come in to __down or __down_interruptible,
@@ -104,7 +104,7 @@ EXPORT_SYMBOL(__up);
  * Thus it is only when we decrement count from some value > 0
  * that we have actually got the semaphore.
  */
-void __sched __down(struct semaphore *sem)
+void __sched __compat_down(struct compat_semaphore *sem)
 {
 	struct task_struct *tsk = current;
 	DECLARE_WAITQUEUE(wait, tsk);
@@ -133,9 +133,9 @@ void __sched __down(struct semaphore *se
 	wake_up(&sem->wait);
 }
 
-EXPORT_SYMBOL(__down);
+EXPORT_SYMBOL(__compat_down);
 
-int __sched __down_interruptible(struct semaphore * sem)
+int __sched __compat_down_interruptible(struct compat_semaphore * sem)
 {
 	int retval = 0;
 	struct task_struct *tsk = current;
@@ -165,4 +165,10 @@ int __sched __down_interruptible(struct 
 	return retval;
 }
 
-EXPORT_SYMBOL(__down_interruptible);
+EXPORT_SYMBOL(__compat_down_interruptible);
+
+int fastcall compat_sem_is_locked(struct compat_semaphore *sem)
+{
+	return (int) atomic_read(&sem->count) < 0;
+}
+EXPORT_SYMBOL(compat_sem_is_locked);
Index: linux-2.6.24-rt1/arch/mips/kernel/signal.c
===================================================================
--- linux-2.6.24-rt1.orig/arch/mips/kernel/signal.c	2008-01-25 15:06:35.000000000 -0500
+++ linux-2.6.24-rt1/arch/mips/kernel/signal.c	2008-01-25 15:07:43.000000000 -0500
@@ -629,6 +629,10 @@ static void do_signal(struct pt_regs *re
 	siginfo_t info;
 	int signr;
 
+#ifdef CONFIG_PREEMPT_RT
+	local_irq_enable();
+	preempt_check_resched();
+#endif
 	/*
 	 * We want the common case to go fast, which is why we may in certain
 	 * cases get here from kernel mode. Just return without doing anything
Index: linux-2.6.24-rt1/arch/mips/kernel/signal32.c
===================================================================
--- linux-2.6.24-rt1.orig/arch/mips/kernel/signal32.c	2008-01-25 15:06:35.000000000 -0500
+++ linux-2.6.24-rt1/arch/mips/kernel/signal32.c	2008-01-25 15:07:43.000000000 -0500
@@ -655,6 +655,10 @@ static int setup_rt_frame_32(struct k_si
 	if (err)
 		goto give_sigsegv;
 
+#ifdef CONFIG_PREEMPT_RT
+	local_irq_enable();
+	preempt_check_resched();
+#endif
 	/*
 	 * Arguments to signal handler:
 	 *
Index: linux-2.6.24-rt1/arch/mips/kernel/smp.c
===================================================================
--- linux-2.6.24-rt1.orig/arch/mips/kernel/smp.c	2008-01-25 15:06:35.000000000 -0500
+++ linux-2.6.24-rt1/arch/mips/kernel/smp.c	2008-01-25 15:07:43.000000000 -0500
@@ -91,7 +91,22 @@ asmlinkage __cpuinit void start_secondar
 	cpu_idle();
 }
 
-DEFINE_SPINLOCK(smp_call_lock);
+DEFINE_RAW_SPINLOCK(smp_call_lock);
+
+/*
+ * this function sends a 'reschedule' IPI to all other CPUs.
+ * This is used when RT tasks are starving and other CPUs
+ * might be able to run them.
+ */
+void smp_send_reschedule_allbutself(void)
+{
+	int cpu = smp_processor_id();
+	int i;
+
+	for (i = 0; i < NR_CPUS; i++)
+		if (cpu_online(i) && i != cpu)
+			core_send_ipi(i, SMP_RESCHEDULE_YOURSELF);
+}
 
 struct call_data_struct *call_data;
 
@@ -314,6 +329,8 @@ int setup_profiling_timer(unsigned int m
 	return 0;
 }
 
+static DEFINE_RAW_SPINLOCK(tlbstate_lock);
+
 static void flush_tlb_all_ipi(void *info)
 {
 	local_flush_tlb_all();
@@ -371,6 +388,7 @@ static inline void smp_on_each_tlb(void 
 void flush_tlb_mm(struct mm_struct *mm)
 {
 	preempt_disable();
+	spin_lock(&tlbstate_lock);
 
 	if ((atomic_read(&mm->mm_users) != 1) || (current->mm != mm)) {
 		smp_on_other_tlbs(flush_tlb_mm_ipi, mm);
@@ -383,6 +401,7 @@ void flush_tlb_mm(struct mm_struct *mm)
 			if (cpu_context(cpu, mm))
 				cpu_context(cpu, mm) = 0;
 	}
+	spin_unlock(&tlbstate_lock);
 	local_flush_tlb_mm(mm);
 
 	preempt_enable();
@@ -406,6 +425,8 @@ void flush_tlb_range(struct vm_area_stru
 	struct mm_struct *mm = vma->vm_mm;
 
 	preempt_disable();
+	spin_lock(&tlbstate_lock);
+
 	if ((atomic_read(&mm->mm_users) != 1) || (current->mm != mm)) {
 		struct flush_tlb_data fd = {
 			.vma = vma,
@@ -423,6 +444,7 @@ void flush_tlb_range(struct vm_area_stru
 			if (cpu_context(cpu, mm))
 				cpu_context(cpu, mm) = 0;
 	}
+	spin_unlock(&tlbstate_lock);
 	local_flush_tlb_range(vma, start, end);
 	preempt_enable();
 }
@@ -454,6 +476,8 @@ static void flush_tlb_page_ipi(void *inf
 void flush_tlb_page(struct vm_area_struct *vma, unsigned long page)
 {
 	preempt_disable();
+	spin_lock(&tlbstate_lock);
+
 	if ((atomic_read(&vma->vm_mm->mm_users) != 1) || (current->mm != vma->vm_mm)) {
 		struct flush_tlb_data fd = {
 			.vma = vma,
@@ -470,6 +494,7 @@ void flush_tlb_page(struct vm_area_struc
 			if (cpu_context(cpu, vma->vm_mm))
 				cpu_context(cpu, vma->vm_mm) = 0;
 	}
+	spin_unlock(&tlbstate_lock);
 	local_flush_tlb_page(vma, page);
 	preempt_enable();
 }
Index: linux-2.6.24-rt1/arch/mips/kernel/traps.c
===================================================================
--- linux-2.6.24-rt1.orig/arch/mips/kernel/traps.c	2008-01-25 15:06:35.000000000 -0500
+++ linux-2.6.24-rt1/arch/mips/kernel/traps.c	2008-01-25 15:07:43.000000000 -0500
@@ -320,7 +320,7 @@ void show_registers(const struct pt_regs
 	printk("\n");
 }
 
-static DEFINE_SPINLOCK(die_lock);
+static DEFINE_RAW_SPINLOCK(die_lock);
 
 void __noreturn die(const char * str, const struct pt_regs * regs)
 {
Index: linux-2.6.24-rt1/arch/mips/mm/init.c
===================================================================
--- linux-2.6.24-rt1.orig/arch/mips/mm/init.c	2008-01-25 15:06:35.000000000 -0500
+++ linux-2.6.24-rt1/arch/mips/mm/init.c	2008-01-25 15:07:43.000000000 -0500
@@ -61,7 +61,7 @@
 
 #endif /* CONFIG_MIPS_MT_SMTC */
 
-DEFINE_PER_CPU(struct mmu_gather, mmu_gathers);
+DEFINE_PER_CPU_LOCKED(struct mmu_gather, mmu_gathers);
 
 /*
  * We have up to 8 empty zeroed pages so we can map one of the right colour
Index: linux-2.6.24-rt1/arch/mips/sibyte/cfe/smp.c
===================================================================
--- linux-2.6.24-rt1.orig/arch/mips/sibyte/cfe/smp.c	2008-01-25 15:06:35.000000000 -0500
+++ linux-2.6.24-rt1/arch/mips/sibyte/cfe/smp.c	2008-01-25 15:07:43.000000000 -0500
@@ -107,4 +107,8 @@ void __cpuinit prom_smp_finish(void)
  */
 void prom_cpus_done(void)
 {
+#ifdef CONFIG_HIGH_RES_TIMERS
+	extern void sync_c0_count_master(void);
+	sync_c0_count_master();
+#endif
 }
Index: linux-2.6.24-rt1/arch/mips/sibyte/sb1250/irq.c
===================================================================
--- linux-2.6.24-rt1.orig/arch/mips/sibyte/sb1250/irq.c	2008-01-25 15:06:35.000000000 -0500
+++ linux-2.6.24-rt1/arch/mips/sibyte/sb1250/irq.c	2008-01-25 15:07:43.000000000 -0500
@@ -82,7 +82,7 @@ static struct irq_chip sb1250_irq_type =
 /* Store the CPU id (not the logical number) */
 int sb1250_irq_owner[SB1250_NR_IRQS];
 
-DEFINE_SPINLOCK(sb1250_imr_lock);
+DEFINE_RAW_SPINLOCK(sb1250_imr_lock);
 
 void sb1250_mask_irq(int cpu, int irq)
 {
@@ -316,6 +316,10 @@ void __init arch_init_irq(void)
 #ifdef CONFIG_KGDB
 	imask |= STATUSF_IP6;
 #endif
+
+#ifdef CONFIG_HIGH_RES_TIMERS
+	imask |= STATUSF_IP7;
+#endif
 	/* Enable necessary IPs, disable the rest */
 	change_c0_status(ST0_IM, imask);
 
Index: linux-2.6.24-rt1/arch/mips/sibyte/sb1250/smp.c
===================================================================
--- linux-2.6.24-rt1.orig/arch/mips/sibyte/sb1250/smp.c	2008-01-25 15:06:35.000000000 -0500
+++ linux-2.6.24-rt1/arch/mips/sibyte/sb1250/smp.c	2008-01-25 15:07:43.000000000 -0500
@@ -60,7 +60,7 @@ void __cpuinit sb1250_smp_finish(void)
 	extern void sb1250_clockevent_init(void);
 
 	sb1250_clockevent_init();
-	local_irq_enable();
+	raw_local_irq_enable();
 }
 
 /*
Index: linux-2.6.24-rt1/arch/mips/sibyte/swarm/setup.c
===================================================================
--- linux-2.6.24-rt1.orig/arch/mips/sibyte/swarm/setup.c	2008-01-25 15:06:35.000000000 -0500
+++ linux-2.6.24-rt1/arch/mips/sibyte/swarm/setup.c	2008-01-25 15:07:43.000000000 -0500
@@ -136,6 +136,12 @@ void __init plat_mem_setup(void)
 	if (m41t81_probe())
 		swarm_rtc_type = RTC_M4LT81;
 
+#ifdef CONFIG_HIGH_RES_TIMERS
+	/*
+	 * set the mips_hpt_frequency here
+	 */
+	mips_hpt_frequency = CONFIG_CPU_SPEED * 1000000;
+#endif
 	printk("This kernel optimized for "
 #ifdef CONFIG_SIMULATION
 	       "simulation"
Index: linux-2.6.24-rt1/include/asm-mips/asmmacro.h
===================================================================
--- linux-2.6.24-rt1.orig/include/asm-mips/asmmacro.h	2008-01-25 15:06:35.000000000 -0500
+++ linux-2.6.24-rt1/include/asm-mips/asmmacro.h	2008-01-25 15:07:43.000000000 -0500
@@ -21,7 +21,7 @@
 #endif
 
 #ifdef CONFIG_MIPS_MT_SMTC
-	.macro	local_irq_enable reg=t0
+	.macro	raw_local_irq_enable reg=t0
 	mfc0	\reg, CP0_TCSTATUS
 	ori	\reg, \reg, TCSTATUS_IXMT
 	xori	\reg, \reg, TCSTATUS_IXMT
@@ -29,21 +29,21 @@
 	_ehb
 	.endm
 
-	.macro	local_irq_disable reg=t0
+	.macro	raw_local_irq_disable reg=t0
 	mfc0	\reg, CP0_TCSTATUS
 	ori	\reg, \reg, TCSTATUS_IXMT
 	mtc0	\reg, CP0_TCSTATUS
 	_ehb
 	.endm
 #else
-	.macro	local_irq_enable reg=t0
+	.macro	raw_local_irq_enable reg=t0
 	mfc0	\reg, CP0_STATUS
 	ori	\reg, \reg, 1
 	mtc0	\reg, CP0_STATUS
 	irq_enable_hazard
 	.endm
 
-	.macro	local_irq_disable reg=t0
+	.macro	raw_local_irq_disable reg=t0
 	mfc0	\reg, CP0_STATUS
 	ori	\reg, \reg, 1
 	xori	\reg, \reg, 1
Index: linux-2.6.24-rt1/include/asm-mips/atomic.h
===================================================================
--- linux-2.6.24-rt1.orig/include/asm-mips/atomic.h	2008-01-25 15:07:33.000000000 -0500
+++ linux-2.6.24-rt1/include/asm-mips/atomic.h	2008-01-25 15:07:43.000000000 -0500
@@ -573,7 +573,6 @@ static __inline__ long atomic64_add_retu
 		raw_local_irq_restore(flags);
 	}
 #endif
-#endif
 
 	smp_llsc_mb();
 
Index: linux-2.6.24-rt1/include/asm-mips/bitops.h
===================================================================
--- linux-2.6.24-rt1.orig/include/asm-mips/bitops.h	2008-01-25 15:06:35.000000000 -0500
+++ linux-2.6.24-rt1/include/asm-mips/bitops.h	2008-01-25 15:07:43.000000000 -0500
@@ -606,9 +606,6 @@ static inline unsigned long __ffs(unsign
 }
 
 /*
- * fls - find last bit set.
- * @word: The word to search
- *
  * This is defined the same way as ffs.
  * Note fls(0) = 0, fls(1) = 1, fls(0x80000000) = 32.
  */
@@ -626,6 +623,8 @@ static inline int fls64(__u64 word)
 
 	return 64 - word;
 }
+#define __bi_local_irq_save(x)		raw_local_irq_save(x)
+#define __bi_local_irq_restore(x)	raw_local_irq_restore(x)
 #else
 #include <asm-generic/bitops/fls64.h>
 #endif
Index: linux-2.6.24-rt1/include/asm-mips/hw_irq.h
===================================================================
--- linux-2.6.24-rt1.orig/include/asm-mips/hw_irq.h	2008-01-25 15:06:35.000000000 -0500
+++ linux-2.6.24-rt1/include/asm-mips/hw_irq.h	2008-01-25 15:07:43.000000000 -0500
@@ -9,6 +9,7 @@
 #define __ASM_HW_IRQ_H
 
 #include <asm/atomic.h>
+#include <linux/irqflags.h>
 
 extern atomic_t irq_err_count;
 
Index: linux-2.6.24-rt1/include/asm-mips/i8259.h
===================================================================
--- linux-2.6.24-rt1.orig/include/asm-mips/i8259.h	2008-01-25 15:06:35.000000000 -0500
+++ linux-2.6.24-rt1/include/asm-mips/i8259.h	2008-01-25 15:07:43.000000000 -0500
@@ -35,7 +35,7 @@
 #define SLAVE_ICW4_DEFAULT	0x01
 #define PIC_ICW4_AEOI		2
 
-extern spinlock_t i8259A_lock;
+extern raw_spinlock_t i8259A_lock;
 
 extern int i8259A_irq_pending(unsigned int irq);
 extern void make_8259A_irq(unsigned int irq);
Index: linux-2.6.24-rt1/include/asm-mips/io.h
===================================================================
--- linux-2.6.24-rt1.orig/include/asm-mips/io.h	2008-01-25 15:06:35.000000000 -0500
+++ linux-2.6.24-rt1/include/asm-mips/io.h	2008-01-25 15:07:43.000000000 -0500
@@ -15,6 +15,7 @@
 #include <linux/compiler.h>
 #include <linux/kernel.h>
 #include <linux/types.h>
+#include <linux/irqflags.h>
 
 #include <asm/addrspace.h>
 #include <asm/byteorder.h>
Index: linux-2.6.24-rt1/include/asm-mips/linkage.h
===================================================================
--- linux-2.6.24-rt1.orig/include/asm-mips/linkage.h	2008-01-25 15:06:35.000000000 -0500
+++ linux-2.6.24-rt1/include/asm-mips/linkage.h	2008-01-25 15:07:43.000000000 -0500
@@ -3,6 +3,11 @@
 
 #ifdef __ASSEMBLY__
 #include <asm/asm.h>
+
+/* FASTCALL stuff */
+#define FASTCALL(x)	x
+#define fastcall
+
 #endif
 
 #define __weak __attribute__((weak))
Index: linux-2.6.24-rt1/include/asm-mips/m48t35.h
===================================================================
--- linux-2.6.24-rt1.orig/include/asm-mips/m48t35.h	2008-01-25 15:06:35.000000000 -0500
+++ linux-2.6.24-rt1/include/asm-mips/m48t35.h	2008-01-25 15:07:43.000000000 -0500
@@ -6,7 +6,7 @@
 
 #include <linux/spinlock.h>
 
-extern spinlock_t rtc_lock;
+extern raw_spinlock_t rtc_lock;
 
 struct m48t35_rtc {
 	volatile u8	pad[0x7ff8];    /* starts at 0x7ff8 */
Index: linux-2.6.24-rt1/include/asm-mips/rwsem.h
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.24-rt1/include/asm-mips/rwsem.h	2008-01-25 15:07:43.000000000 -0500
@@ -0,0 +1,176 @@
+/*
+ * include/asm-mips/rwsem.h: R/W semaphores for MIPS using the stuff
+ * in lib/rwsem.c.  Adapted largely from include/asm-ppc/rwsem.h
+ * by john.cooper@timesys.com
+ */
+
+#ifndef _MIPS_RWSEM_H
+#define _MIPS_RWSEM_H
+
+#ifndef _LINUX_RWSEM_H
+#error "please don't include asm/rwsem.h directly, use linux/rwsem.h instead"
+#endif
+
+#ifdef __KERNEL__
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <asm/atomic.h>
+#include <asm/system.h>
+
+/*
+ * the semaphore definition
+ */
+struct compat_rw_semaphore {
+	/* XXX this should be able to be an atomic_t  -- paulus */
+	signed long		count;
+#define RWSEM_UNLOCKED_VALUE		0x00000000
+#define RWSEM_ACTIVE_BIAS		0x00000001
+#define RWSEM_ACTIVE_MASK		0x0000ffff
+#define RWSEM_WAITING_BIAS		(-0x00010000)
+#define RWSEM_ACTIVE_READ_BIAS		RWSEM_ACTIVE_BIAS
+#define RWSEM_ACTIVE_WRITE_BIAS		(RWSEM_WAITING_BIAS + RWSEM_ACTIVE_BIAS)
+	raw_spinlock_t		wait_lock;
+	struct list_head	wait_list;
+#if RWSEM_DEBUG
+	int			debug;
+#endif
+};
+
+/*
+ * initialisation
+ */
+#if RWSEM_DEBUG
+#define __RWSEM_DEBUG_INIT      , 0
+#else
+#define __RWSEM_DEBUG_INIT	/* */
+#endif
+
+#define __COMPAT_RWSEM_INITIALIZER(name) \
+	{ RWSEM_UNLOCKED_VALUE, SPIN_LOCK_UNLOCKED, \
+	  LIST_HEAD_INIT((name).wait_list) \
+	  __RWSEM_DEBUG_INIT }
+
+#define COMPAT_DECLARE_RWSEM(name)		\
+	struct compat_rw_semaphore name = __COMPAT_RWSEM_INITIALIZER(name)
+
+extern struct compat_rw_semaphore *rwsem_down_read_failed(struct compat_rw_semaphore *sem);
+extern struct compat_rw_semaphore *rwsem_down_write_failed(struct compat_rw_semaphore *sem);
+extern struct compat_rw_semaphore *rwsem_wake(struct compat_rw_semaphore *sem);
+extern struct compat_rw_semaphore *rwsem_downgrade_wake(struct compat_rw_semaphore *sem);
+
+static inline void compat_init_rwsem(struct compat_rw_semaphore *sem)
+{
+	sem->count = RWSEM_UNLOCKED_VALUE;
+	spin_lock_init(&sem->wait_lock);
+	INIT_LIST_HEAD(&sem->wait_list);
+#if RWSEM_DEBUG
+	sem->debug = 0;
+#endif
+}
+
+/*
+ * lock for reading
+ */
+static inline void __down_read(struct compat_rw_semaphore *sem)
+{
+	if (atomic_inc_return((atomic_t *)(&sem->count)) > 0)
+		smp_wmb();
+	else
+		rwsem_down_read_failed(sem);
+}
+
+static inline int __down_read_trylock(struct compat_rw_semaphore *sem)
+{
+	int tmp;
+
+	while ((tmp = sem->count) >= 0) {
+		if (tmp == cmpxchg(&sem->count, tmp,
+				   tmp + RWSEM_ACTIVE_READ_BIAS)) {
+			smp_wmb();
+			return 1;
+		}
+	}
+	return 0;
+}
+
+/*
+ * lock for writing
+ */
+static inline void __down_write(struct compat_rw_semaphore *sem)
+{
+	int tmp;
+
+	tmp = atomic_add_return(RWSEM_ACTIVE_WRITE_BIAS,
+				(atomic_t *)(&sem->count));
+	if (tmp == RWSEM_ACTIVE_WRITE_BIAS)
+		smp_wmb();
+	else
+		rwsem_down_write_failed(sem);
+}
+
+static inline int __down_write_trylock(struct compat_rw_semaphore *sem)
+{
+	int tmp;
+
+	tmp = cmpxchg(&sem->count, RWSEM_UNLOCKED_VALUE,
+		      RWSEM_ACTIVE_WRITE_BIAS);
+	smp_wmb();
+	return tmp == RWSEM_UNLOCKED_VALUE;
+}
+
+/*
+ * unlock after reading
+ */
+static inline void __up_read(struct compat_rw_semaphore *sem)
+{
+	int tmp;
+
+	smp_wmb();
+	tmp = atomic_dec_return((atomic_t *)(&sem->count));
+	if (tmp < -1 && (tmp & RWSEM_ACTIVE_MASK) == 0)
+		rwsem_wake(sem);
+}
+
+/*
+ * unlock after writing
+ */
+static inline void __up_write(struct compat_rw_semaphore *sem)
+{
+	smp_wmb();
+	if (atomic_sub_return(RWSEM_ACTIVE_WRITE_BIAS,
+			      (atomic_t *)(&sem->count)) < 0)
+		rwsem_wake(sem);
+}
+
+/*
+ * implement atomic add functionality
+ */
+static inline void rwsem_atomic_add(int delta, struct compat_rw_semaphore *sem)
+{
+	atomic_add(delta, (atomic_t *)(&sem->count));
+}
+
+/*
+ * downgrade write lock to read lock
+ */
+static inline void __downgrade_write(struct compat_rw_semaphore *sem)
+{
+	int tmp;
+
+	smp_wmb();
+	tmp = atomic_add_return(-RWSEM_WAITING_BIAS, (atomic_t *)(&sem->count));
+	if (tmp < 0)
+		rwsem_downgrade_wake(sem);
+}
+
+/*
+ * implement exchange and add functionality
+ */
+static inline int rwsem_atomic_update(int delta, struct compat_rw_semaphore *sem)
+{
+	smp_mb();
+	return atomic_add_return(delta, (atomic_t *)(&sem->count));
+}
+
+#endif /* __KERNEL__ */
+#endif /* _MIPS_RWSEM_H */
Index: linux-2.6.24-rt1/include/asm-mips/semaphore.h
===================================================================
--- linux-2.6.24-rt1.orig/include/asm-mips/semaphore.h	2008-01-25 15:07:33.000000000 -0500
+++ linux-2.6.24-rt1/include/asm-mips/semaphore.h	2008-01-25 15:07:43.000000000 -0500
@@ -47,38 +47,41 @@ struct compat_semaphore {
 	wait_queue_head_t wait;
 };
 
-#define __SEMAPHORE_INITIALIZER(name, n)				\
+#define __COMPAT_SEMAPHORE_INITIALIZER(name, n)				\
 {									\
 	.count		= ATOMIC_INIT(n),				\
 	.wait		= __WAIT_QUEUE_HEAD_INITIALIZER((name).wait)	\
 }
 
-#define __DECLARE_SEMAPHORE_GENERIC(name, count) \
-	struct semaphore name = __SEMAPHORE_INITIALIZER(name, count)
+#define __COMPAT_MUTEX_INITIALIZER(name) \
+	__COMPAT_SEMAPHORE_INITIALIZER(name, 1)
 
-#define DECLARE_MUTEX(name)		__DECLARE_SEMAPHORE_GENERIC(name, 1)
+#define __COMPAT_DECLARE_SEMAPHORE_GENERIC(name, count) \
+	struct compat_semaphore name = __COMPAT_SEMAPHORE_INITIALIZER(name,count)
 
-static inline void sema_init(struct semaphore *sem, int val)
+#define COMPAT_DECLARE_MUTEX(name)		__COMPAT_DECLARE_SEMAPHORE_GENERIC(name, 1)
+
+static inline void compat_sema_init (struct compat_semaphore *sem, int val)
 {
 	atomic_set(&sem->count, val);
 	init_waitqueue_head(&sem->wait);
 }
 
-static inline void init_MUTEX(struct semaphore *sem)
+static inline void compat_init_MUTEX (struct compat_semaphore *sem)
 {
-	sema_init(sem, 1);
+	compat_sema_init(sem, 1);
 }
 
-static inline void init_MUTEX_LOCKED(struct semaphore *sem)
+static inline void compat_init_MUTEX_LOCKED (struct compat_semaphore *sem)
 {
-	sema_init(sem, 0);
+	compat_sema_init(sem, 0);
 }
 
-extern void __down(struct semaphore * sem);
-extern int  __down_interruptible(struct semaphore * sem);
-extern void __up(struct semaphore * sem);
+extern void __compat_down(struct compat_semaphore * sem);
+extern int  __compat_down_interruptible(struct compat_semaphore * sem);
+extern void __compat_up(struct compat_semaphore * sem);
 
-static inline void down(struct semaphore * sem)
+static inline void compat_down(struct compat_semaphore * sem)
 {
 	might_sleep();
 
@@ -111,6 +114,8 @@ static inline void compat_up(struct comp
 		__compat_up(sem);
 }
 
+extern int compat_sem_is_locked(struct compat_semaphore *sem);
+
 #define compat_sema_count(sem) atomic_read(&(sem)->count)
 
 #include <linux/semaphore.h>
Index: linux-2.6.24-rt1/include/asm-mips/spinlock.h
===================================================================
--- linux-2.6.24-rt1.orig/include/asm-mips/spinlock.h	2008-01-25 15:06:35.000000000 -0500
+++ linux-2.6.24-rt1/include/asm-mips/spinlock.h	2008-01-25 15:07:43.000000000 -0500
@@ -28,7 +28,7 @@
  * We make no fairness assumptions.  They have a cost.
  */
 
-static inline void __raw_spin_lock(raw_spinlock_t *lock)
+static inline void __raw_spin_lock(__raw_spinlock_t *lock)
 {
 	unsigned int tmp;
 
@@ -70,7 +70,7 @@ static inline void __raw_spin_lock(raw_s
 	smp_llsc_mb();
 }
 
-static inline void __raw_spin_unlock(raw_spinlock_t *lock)
+static inline void __raw_spin_unlock(__raw_spinlock_t *lock)
 {
 	smp_mb();
 
@@ -83,7 +83,7 @@ static inline void __raw_spin_unlock(raw
 	: "memory");
 }
 
-static inline unsigned int __raw_spin_trylock(raw_spinlock_t *lock)
+static inline unsigned int __raw_spin_trylock(__raw_spinlock_t *lock)
 {
 	unsigned int temp, res;
 
@@ -144,7 +144,7 @@ static inline unsigned int __raw_spin_tr
  */
 #define __raw_write_can_lock(rw)	(!(rw)->lock)
 
-static inline void __raw_read_lock(raw_rwlock_t *rw)
+static inline void __raw_read_lock(__raw_rwlock_t *rw)
 {
 	unsigned int tmp;
 
@@ -189,7 +189,7 @@ static inline void __raw_read_lock(raw_r
 /* Note the use of sub, not subu which will make the kernel die with an
    overflow exception if we ever try to unlock an rwlock that is already
    unlocked or is being held by a writer.  */
-static inline void __raw_read_unlock(raw_rwlock_t *rw)
+static inline void __raw_read_unlock(__raw_rwlock_t *rw)
 {
 	unsigned int tmp;
 
@@ -223,7 +223,7 @@ static inline void __raw_read_unlock(raw
 	}
 }
 
-static inline void __raw_write_lock(raw_rwlock_t *rw)
+static inline void __raw_write_lock(__raw_rwlock_t *rw)
 {
 	unsigned int tmp;
 
@@ -265,7 +265,7 @@ static inline void __raw_write_lock(raw_
 	smp_llsc_mb();
 }
 
-static inline void __raw_write_unlock(raw_rwlock_t *rw)
+static inline void __raw_write_unlock(__raw_rwlock_t *rw)
 {
 	smp_mb();
 
@@ -277,7 +277,7 @@ static inline void __raw_write_unlock(ra
 	: "memory");
 }
 
-static inline int __raw_read_trylock(raw_rwlock_t *rw)
+static inline int __raw_read_trylock(__raw_rwlock_t *rw)
 {
 	unsigned int tmp;
 	int ret;
@@ -321,7 +321,7 @@ static inline int __raw_read_trylock(raw
 	return ret;
 }
 
-static inline int __raw_write_trylock(raw_rwlock_t *rw)
+static inline int __raw_write_trylock(__raw_rwlock_t *rw)
 {
 	unsigned int tmp;
 	int ret;
Index: linux-2.6.24-rt1/include/asm-mips/spinlock_types.h
===================================================================
--- linux-2.6.24-rt1.orig/include/asm-mips/spinlock_types.h	2008-01-25 15:06:35.000000000 -0500
+++ linux-2.6.24-rt1/include/asm-mips/spinlock_types.h	2008-01-25 15:07:43.000000000 -0500
@@ -7,13 +7,13 @@
 
 typedef struct {
 	volatile unsigned int lock;
-} raw_spinlock_t;
+} __raw_spinlock_t;
 
 #define __RAW_SPIN_LOCK_UNLOCKED	{ 0 }
 
 typedef struct {
 	volatile unsigned int lock;
-} raw_rwlock_t;
+} __raw_rwlock_t;
 
 #define __RAW_RW_LOCK_UNLOCKED		{ 0 }
 
Index: linux-2.6.24-rt1/include/asm-mips/thread_info.h
===================================================================
--- linux-2.6.24-rt1.orig/include/asm-mips/thread_info.h	2008-01-25 15:06:35.000000000 -0500
+++ linux-2.6.24-rt1/include/asm-mips/thread_info.h	2008-01-25 15:07:43.000000000 -0500
@@ -112,6 +112,7 @@ register struct thread_info *__current_t
 #define TIF_NEED_RESCHED	2	/* rescheduling necessary */
 #define TIF_SYSCALL_AUDIT	3	/* syscall auditing active */
 #define TIF_SECCOMP		4	/* secure computing */
+#define TIF_NEED_RESCHED_DELAYED 6	/* reschedule on return to userspace */
 #define TIF_RESTORE_SIGMASK	9	/* restore signal mask in do_signal() */
 #define TIF_USEDFPU		16	/* FPU was used by this task this quantum (SMP) */
 #define TIF_POLLING_NRFLAG	17	/* true if poll_idle() is polling TIF_NEED_RESCHED */
@@ -129,6 +130,7 @@ register struct thread_info *__current_t
 #define _TIF_NEED_RESCHED	(1<<TIF_NEED_RESCHED)
 #define _TIF_SYSCALL_AUDIT	(1<<TIF_SYSCALL_AUDIT)
 #define _TIF_SECCOMP		(1<<TIF_SECCOMP)
+#define _TIF_NEED_RESCHED_DELAYED (1<<TIF_NEED_RESCHED_DELAYED)
 #define _TIF_RESTORE_SIGMASK	(1<<TIF_RESTORE_SIGMASK)
 #define _TIF_USEDFPU		(1<<TIF_USEDFPU)
 #define _TIF_POLLING_NRFLAG	(1<<TIF_POLLING_NRFLAG)
Index: linux-2.6.24-rt1/include/asm-mips/time.h
===================================================================
--- linux-2.6.24-rt1.orig/include/asm-mips/time.h	2008-01-25 15:06:35.000000000 -0500
+++ linux-2.6.24-rt1/include/asm-mips/time.h	2008-01-25 15:07:43.000000000 -0500
@@ -19,7 +19,7 @@
 #include <linux/clockchips.h>
 #include <linux/clocksource.h>
 
-extern spinlock_t rtc_lock;
+extern raw_spinlock_t rtc_lock;
 
 /*
  * RTC ops.  By default, they point to weak no-op RTC functions.
Index: linux-2.6.24-rt1/include/asm-mips/timeofday.h
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.24-rt1/include/asm-mips/timeofday.h	2008-01-25 15:07:43.000000000 -0500
@@ -0,0 +1,5 @@
+#ifndef _ASM_MIPS_TIMEOFDAY_H
+#define _ASM_MIPS_TIMEOFDAY_H
+#include <asm-generic/timeofday.h>
+#endif
+
Index: linux-2.6.24-rt1/include/asm-mips/uaccess.h
===================================================================
--- linux-2.6.24-rt1.orig/include/asm-mips/uaccess.h	2008-01-25 15:06:35.000000000 -0500
+++ linux-2.6.24-rt1/include/asm-mips/uaccess.h	2008-01-25 15:07:43.000000000 -0500
@@ -427,7 +427,6 @@ extern size_t __copy_user(void *__to, co
 	const void *__cu_from;						\
 	long __cu_len;							\
 									\
-	might_sleep();							\
 	__cu_to = (to);							\
 	__cu_from = (from);						\
 	__cu_len = (n);							\
@@ -483,7 +482,6 @@ extern size_t __copy_user_inatomic(void 
 	const void *__cu_from;						\
 	long __cu_len;							\
 									\
-	might_sleep();							\
 	__cu_to = (to);							\
 	__cu_from = (from);						\
 	__cu_len = (n);							\
@@ -562,7 +560,6 @@ extern size_t __copy_user_inatomic(void 
 	const void __user *__cu_from;					\
 	long __cu_len;							\
 									\
-	might_sleep();							\
 	__cu_to = (to);							\
 	__cu_from = (from);						\
 	__cu_len = (n);							\
@@ -593,7 +590,6 @@ extern size_t __copy_user_inatomic(void 
 	const void __user *__cu_from;					\
 	long __cu_len;							\
 									\
-	might_sleep();							\
 	__cu_to = (to);							\
 	__cu_from = (from);						\
 	__cu_len = (n);							\
@@ -611,7 +607,6 @@ extern size_t __copy_user_inatomic(void 
 	const void __user *__cu_from;					\
 	long __cu_len;							\
 									\
-	might_sleep();							\
 	__cu_to = (to);							\
 	__cu_from = (from);						\
 	__cu_len = (n);							\
@@ -638,7 +633,6 @@ __clear_user(void __user *addr, __kernel
 {
 	__kernel_size_t res;
 
-	might_sleep();
 	__asm__ __volatile__(
 		"move\t$4, %1\n\t"
 		"move\t$5, $0\n\t"
@@ -687,7 +681,6 @@ __strncpy_from_user(char *__to, const ch
 {
 	long res;
 
-	might_sleep();
 	__asm__ __volatile__(
 		"move\t$4, %1\n\t"
 		"move\t$5, %2\n\t"
@@ -724,7 +717,6 @@ strncpy_from_user(char *__to, const char
 {
 	long res;
 
-	might_sleep();
 	__asm__ __volatile__(
 		"move\t$4, %1\n\t"
 		"move\t$5, %2\n\t"
@@ -743,7 +735,6 @@ static inline long __strlen_user(const c
 {
 	long res;
 
-	might_sleep();
 	__asm__ __volatile__(
 		"move\t$4, %1\n\t"
 		__MODULE_JAL(__strlen_user_nocheck_asm)
@@ -773,7 +764,6 @@ static inline long strlen_user(const cha
 {
 	long res;
 
-	might_sleep();
 	__asm__ __volatile__(
 		"move\t$4, %1\n\t"
 		__MODULE_JAL(__strlen_user_asm)
@@ -790,7 +780,6 @@ static inline long __strnlen_user(const 
 {
 	long res;
 
-	might_sleep();
 	__asm__ __volatile__(
 		"move\t$4, %1\n\t"
 		"move\t$5, %2\n\t"
@@ -821,7 +810,6 @@ static inline long strnlen_user(const ch
 {
 	long res;
 
-	might_sleep();
 	__asm__ __volatile__(
 		"move\t$4, %1\n\t"
 		"move\t$5, %2\n\t"
