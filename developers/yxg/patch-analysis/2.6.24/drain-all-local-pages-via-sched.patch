---
 mm/page_alloc.c |   33 +++++++++++++++++++++++++++++++++
 1 file changed, 33 insertions(+)

Index: linux-2.6.24-rt1/mm/page_alloc.c
===================================================================
--- linux-2.6.24-rt1.orig/mm/page_alloc.c	2008-01-25 15:07:30.000000000 -0500
+++ linux-2.6.24-rt1/mm/page_alloc.c	2008-01-25 15:08:30.000000000 -0500
@@ -1020,6 +1020,38 @@ void smp_drain_local_pages(void *arg)
  */
 void drain_all_local_pages(void)
 {
+#ifdef CONFIG_PREEMPT_RT
+	/*
+	 * HACK!!!!!
+	 *  For RT we can't use IPIs to run drain_local_pages, since
+	 *  that code will call spin_locks that will now sleep.
+	 *  But, schedule_on_each_cpu will call kzalloc, which will
+	 *  call page_alloc which was what calls this.
+	 *
+	 *  Luckily, there's a condition to get here, and that is if
+	 *  the order passed in to alloc_pages is greater than 0
+	 *  (alloced more than a page size).  The slabs only allocate
+	 *  what is needed, and the allocation made by schedule_on_each_cpu
+	 *  does an alloc of "sizeof(void *)*nr_cpu_ids".
+	 *
+	 *  So we can safely call schedule_on_each_cpu if that number
+	 *  is less than a page. Otherwise don't bother. At least warn of
+	 *  this issue.
+	 *
+	 * And yes, this is one big hack.  Please fix ;-)
+	 */
+	if (sizeof(void *)*nr_cpu_ids < PAGE_SIZE)
+		schedule_on_each_cpu(smp_drain_local_pages, NULL, 0, 1);
+	else {
+		static int once;
+		if (!once) {
+			printk(KERN_ERR "Can't drain all CPUS due to possible recursion\n");
+			once = 1;
+		}
+		drain_local_pages();
+	}
+
+#else
 	unsigned long flags;
 
 	local_irq_save(flags);
@@ -1027,6 +1059,7 @@ void drain_all_local_pages(void)
 	local_irq_restore(flags);
 
 	smp_call_function(smp_drain_local_pages, NULL, 0, 1);
+#endif
 }
 
 /*
