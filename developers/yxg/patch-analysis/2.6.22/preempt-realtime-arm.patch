---
 arch/arm/kernel/dma.c         |    2 +-
 arch/arm/kernel/irq.c         |    2 +-
 arch/arm/kernel/signal.c      |    8 ++++++++
 arch/arm/kernel/smp.c         |    2 +-
 arch/arm/kernel/traps.c       |    4 ++--
 arch/arm/mm/consistent.c      |    2 +-
 arch/arm/mm/copypage-v4mc.c   |    2 +-
 arch/arm/mm/copypage-v6.c     |    2 +-
 arch/arm/mm/copypage-xscale.c |    2 +-
 arch/arm/mm/mmu.c             |    2 +-
 include/asm-arm/dma.h         |    2 +-
 include/asm-arm/tlb.h         |    9 ++++++---
 12 files changed, 25 insertions(+), 14 deletions(-)

Index: linux-rt.q/arch/arm/kernel/dma.c
===================================================================
--- linux-rt.q.orig/arch/arm/kernel/dma.c
+++ linux-rt.q/arch/arm/kernel/dma.c
@@ -20,7 +20,7 @@
 
 #include <asm/mach/dma.h>
 
-DEFINE_SPINLOCK(dma_spin_lock);
+DEFINE_RAW_SPINLOCK(dma_spin_lock);
 EXPORT_SYMBOL(dma_spin_lock);
 
 static dma_t dma_chan[MAX_DMA_CHANNELS];
Index: linux-rt.q/arch/arm/kernel/irq.c
===================================================================
--- linux-rt.q.orig/arch/arm/kernel/irq.c
+++ linux-rt.q/arch/arm/kernel/irq.c
@@ -100,7 +100,7 @@ unlock:
 /* Handle bad interrupts */
 static struct irq_desc bad_irq_desc = {
 	.handle_irq = handle_bad_irq,
-	.lock = SPIN_LOCK_UNLOCKED
+	.lock = RAW_SPIN_LOCK_UNLOCKED(bad_irq_desc.lock)
 };
 
 /*
Index: linux-rt.q/arch/arm/kernel/signal.c
===================================================================
--- linux-rt.q.orig/arch/arm/kernel/signal.c
+++ linux-rt.q/arch/arm/kernel/signal.c
@@ -623,6 +623,14 @@ static int do_signal(sigset_t *oldset, s
 	siginfo_t info;
 	int signr;
 
+#ifdef CONFIG_PREEMPT_RT
+	/*
+	 * Fully-preemptible kernel does not need interrupts disabled:
+	 */
+	local_irq_enable();
+	preempt_check_resched();
+#endif
+
 	/*
 	 * We want the common case to go fast, which
 	 * is why we may in certain cases get here from
Index: linux-rt.q/arch/arm/kernel/smp.c
===================================================================
--- linux-rt.q.orig/arch/arm/kernel/smp.c
+++ linux-rt.q/arch/arm/kernel/smp.c
@@ -521,7 +521,7 @@ static void ipi_call_function(unsigned i
 		cpu_clear(cpu, data->unfinished);
 }
 
-static DEFINE_SPINLOCK(stop_lock);
+static DEFINE_RAW_SPINLOCK(stop_lock);
 
 /*
  * ipi_cpu_stop - handle IPI from smp_send_stop()
Index: linux-rt.q/arch/arm/kernel/traps.c
===================================================================
--- linux-rt.q.orig/arch/arm/kernel/traps.c
+++ linux-rt.q/arch/arm/kernel/traps.c
@@ -234,7 +234,7 @@ static void __die(const char *str, int e
 	}
 }
 
-DEFINE_SPINLOCK(die_lock);
+DEFINE_RAW_SPINLOCK(die_lock);
 
 /*
  * This function is protected against re-entrancy.
@@ -276,7 +276,7 @@ void arm_notify_die(const char *str, str
 }
 
 static LIST_HEAD(undef_hook);
-static DEFINE_SPINLOCK(undef_lock);
+static DEFINE_RAW_SPINLOCK(undef_lock);
 
 void register_undef_hook(struct undef_hook *hook)
 {
Index: linux-rt.q/arch/arm/mm/consistent.c
===================================================================
--- linux-rt.q.orig/arch/arm/mm/consistent.c
+++ linux-rt.q/arch/arm/mm/consistent.c
@@ -40,7 +40,7 @@
  * These are the page tables (2MB each) covering uncached, DMA consistent allocations
  */
 static pte_t *consistent_pte[NUM_CONSISTENT_PTES];
-static DEFINE_SPINLOCK(consistent_lock);
+static DEFINE_RAW_SPINLOCK(consistent_lock);
 
 /*
  * VM region handling support.
Index: linux-rt.q/arch/arm/mm/copypage-v4mc.c
===================================================================
--- linux-rt.q.orig/arch/arm/mm/copypage-v4mc.c
+++ linux-rt.q/arch/arm/mm/copypage-v4mc.c
@@ -30,7 +30,7 @@
 #define minicache_pgprot __pgprot(L_PTE_PRESENT | L_PTE_YOUNG | \
 				  L_PTE_CACHEABLE)
 
-static DEFINE_SPINLOCK(minicache_lock);
+static DEFINE_RAW_SPINLOCK(minicache_lock);
 
 /*
  * ARMv4 mini-dcache optimised copy_user_page
Index: linux-rt.q/arch/arm/mm/copypage-v6.c
===================================================================
--- linux-rt.q.orig/arch/arm/mm/copypage-v6.c
+++ linux-rt.q/arch/arm/mm/copypage-v6.c
@@ -26,7 +26,7 @@
 #define from_address	(0xffff8000)
 #define to_address	(0xffffc000)
 
-static DEFINE_SPINLOCK(v6_lock);
+static DEFINE_RAW_SPINLOCK(v6_lock);
 
 /*
  * Copy the user page.  No aliasing to deal with so we can just
Index: linux-rt.q/arch/arm/mm/copypage-xscale.c
===================================================================
--- linux-rt.q.orig/arch/arm/mm/copypage-xscale.c
+++ linux-rt.q/arch/arm/mm/copypage-xscale.c
@@ -32,7 +32,7 @@
 #define minicache_pgprot __pgprot(L_PTE_PRESENT | L_PTE_YOUNG | \
 				  L_PTE_CACHEABLE)
 
-static DEFINE_SPINLOCK(minicache_lock);
+static DEFINE_RAW_SPINLOCK(minicache_lock);
 
 /*
  * XScale mini-dcache optimised copy_user_page
Index: linux-rt.q/arch/arm/mm/mmu.c
===================================================================
--- linux-rt.q.orig/arch/arm/mm/mmu.c
+++ linux-rt.q/arch/arm/mm/mmu.c
@@ -25,7 +25,7 @@
 
 #include "mm.h"
 
-DEFINE_PER_CPU(struct mmu_gather, mmu_gathers);
+DEFINE_PER_CPU_LOCKED(struct mmu_gather, mmu_gathers);
 
 extern void _stext, _etext, __data_start, _end;
 extern pgd_t swapper_pg_dir[PTRS_PER_PGD];
Index: linux-rt.q/include/asm-arm/dma.h
===================================================================
--- linux-rt.q.orig/include/asm-arm/dma.h
+++ linux-rt.q/include/asm-arm/dma.h
@@ -27,7 +27,7 @@ typedef unsigned int dmamode_t;
 #define DMA_MODE_CASCADE 2
 #define DMA_AUTOINIT	 4
 
-extern spinlock_t  dma_spin_lock;
+extern raw_spinlock_t  dma_spin_lock;
 
 static inline unsigned long claim_dma_lock(void)
 {
Index: linux-rt.q/include/asm-arm/tlb.h
===================================================================
--- linux-rt.q.orig/include/asm-arm/tlb.h
+++ linux-rt.q/include/asm-arm/tlb.h
@@ -36,15 +36,18 @@
 struct mmu_gather {
 	struct mm_struct	*mm;
 	unsigned int		fullmm;
+	int			cpu;
 };
 
-DECLARE_PER_CPU(struct mmu_gather, mmu_gathers);
+DECLARE_PER_CPU_LOCKED(struct mmu_gather, mmu_gathers);
 
 static inline struct mmu_gather *
 tlb_gather_mmu(struct mm_struct *mm, unsigned int full_mm_flush)
 {
-	struct mmu_gather *tlb = &get_cpu_var(mmu_gathers);
+	int cpu;
+	struct mmu_gather *tlb = &get_cpu_var_locked(mmu_gathers, &cpu);
 
+	tlb->cpu = cpu;
 	tlb->mm = mm;
 	tlb->fullmm = full_mm_flush;
 
@@ -60,7 +63,7 @@ tlb_finish_mmu(struct mmu_gather *tlb, u
 	/* keep the page table cache within bounds */
 	check_pgt_cache();
 
-	put_cpu_var(mmu_gathers);
+	put_cpu_var_locked(mmu_gathers, tlb->cpu);
 }
 
 #define tlb_remove_tlb_entry(tlb,ptep,address)	do { } while (0)
