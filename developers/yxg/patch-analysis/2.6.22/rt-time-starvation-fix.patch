Hey Ingo,
	Noticed -rt has been updated a few times and this is still missing so I
figured I'd resend it just in case you missed it:

	We've worked around this before, but its cropped up again. Since
update_wall_time is now called from a softirq, it can be preempted by a
high priority process. If its preempted for long enough, the clocksource
can wrap, causing time to stop incrementing, which if the preempting
process is checking the time, can cause a hard lockup.

This patch forces the clocksource to be read each tick, and accumulate
only the cycle count. This allows the update_wall_time to be deferred
w/o fear of hardware overflow.

thanks
-john

 arch/x86_64/kernel/vsyscall.c |    4 +++-
 include/linux/clocksource.h   |   40 ++++++++++++++++++++++++++++++++++++++--
 include/linux/time.h          |    1 +
 kernel/time/timekeeping.c     |   34 ++++++++++++++++++----------------
 kernel/timer.c                |    1 +
 5 files changed, 61 insertions(+), 19 deletions(-)

linux-2.6.21-rc5_cycles-accumulated_C7.patch
============================================
Index: linux-rt.q/arch/x86_64/kernel/vsyscall.c
===================================================================
--- linux-rt.q.orig/arch/x86_64/kernel/vsyscall.c
+++ linux-rt.q/arch/x86_64/kernel/vsyscall.c
@@ -128,7 +128,7 @@ static __always_inline long time_syscall
 
 static __always_inline void do_vgettimeofday(struct timeval * tv)
 {
-	cycle_t now, base, mask, cycle_delta;
+	cycle_t now, base, accumulated, mask, cycle_delta;
 	unsigned seq;
 	unsigned long mult, shift, nsec;
 	cycle_t (*vread)(void);
@@ -161,6 +161,7 @@ static __always_inline void do_vgettimeo
 		}
 		now = vread();
 		base = __vsyscall_gtod_data.clock.cycle_last;
+		accumulated  = __vsyscall_gtod_data.clock.cycle_accumulated;
 		mask = __vsyscall_gtod_data.clock.mask;
 		mult = __vsyscall_gtod_data.clock.mult;
 		shift = __vsyscall_gtod_data.clock.shift;
@@ -171,6 +172,7 @@ static __always_inline void do_vgettimeo
 
 	/* calculate interval: */
 	cycle_delta = (now - base) & mask;
+	cycle_delta += accumulated;
 	/* convert to nsecs: */
 	nsec += (cycle_delta * mult) >> shift;
 
Index: linux-rt.q/include/linux/clocksource.h
===================================================================
--- linux-rt.q.orig/include/linux/clocksource.h
+++ linux-rt.q/include/linux/clocksource.h
@@ -54,8 +54,12 @@ extern unsigned long preempt_mark_thresh
  * @flags:		flags describing special properties
  * @vread:		vsyscall based read
  * @resume:		resume function for the clocksource, if necessary
+ * @cycle_last:		Used internally by timekeeping core, please ignore.
+ * @cycle_accumulated:	Used internally by timekeeping core, please ignore.
  * @cycle_interval:	Used internally by timekeeping core, please ignore.
  * @xtime_interval:	Used internally by timekeeping core, please ignore.
+ * @xtime_nsec:		Used internally by timekeeping core, please ignore.
+ * @error:		Used internally by timekeeping core, please ignore.
  */
 struct clocksource {
 	/*
@@ -73,7 +77,7 @@ struct clocksource {
 	void (*resume)(void);
 
 	/* timekeeping specific data, ignore */
-	cycle_t cycle_interval;
+	cycle_t cycle_accumulated, cycle_interval;
 	u64	xtime_interval;
 	/*
 	 * Second part is written at each timer interrupt
@@ -166,11 +170,43 @@ static inline cycle_t clocksource_read(s
 }
 
 /**
+ * clocksource_get_cycles: - Access the clocksource's accumulated cycle value
+ * @cs:		pointer to clocksource being read
+ * @now:	current cycle value
+ *
+ * Uses the clocksource to return the current cycle_t value.
+ * NOTE!!!: This is different from clocksource_read, because it
+ * returns the accumulated cycle value! Must hold xtime lock!
+ */
+static inline cycle_t clocksource_get_cycles(struct clocksource *cs, cycle_t now)
+{
+	cycle_t offset = (now - cs->cycle_last) & cs->mask;
+	offset += cs->cycle_accumulated;
+	return offset;
+}
+
+/**
+ * clocksource_accumulate: - Accumulates clocksource cycles
+ * @cs:		pointer to clocksource being read
+ * @now:	current cycle value
+ *
+ * Used to avoids clocksource hardware overflow by periodically
+ * accumulating the current cycle delta. Must hold xtime write lock!
+ */
+static inline void clocksource_accumulate(struct clocksource *cs, cycle_t now)
+{
+	cycle_t offset = (now - cs->cycle_last) & cs->mask;
+	cs->cycle_last = now;
+	cs->cycle_accumulated += offset;
+}
+
+/**
  * cyc2ns - converts clocksource cycles to nanoseconds
  * @cs:		Pointer to clocksource
  * @cycles:	Cycles
  *
  * Uses the clocksource and ntp ajdustment to convert cycle_ts to nanoseconds.
+ * Must hold xtime lock!
  *
  * XXX - This could use some mult_lxl_ll() asm optimization
  */
@@ -200,7 +236,7 @@ static inline cycle_t ns2cyc(struct cloc
  * @length_nsec: Desired interval length in nanoseconds.
  *
  * Calculates a fixed cycle/nsec interval for a given clocksource/adjustment
- * pair and interval request.
+ * pair and interval request. Must hold xtime_lock!
  *
  * Unless you're the timekeeping code, you should not be using this!
  */
Index: linux-rt.q/include/linux/time.h
===================================================================
--- linux-rt.q.orig/include/linux/time.h
+++ linux-rt.q/include/linux/time.h
@@ -97,6 +97,7 @@ extern unsigned long read_persistent_clo
 extern int update_persistent_clock(struct timespec now);
 extern int no_sync_cmos_clock __read_mostly;
 void timekeeping_init(void);
+extern void timekeeping_accumulate(void);
 
 static inline unsigned long get_seconds(void)
 {
Index: linux-rt.q/kernel/time/timekeeping.c
===================================================================
--- linux-rt.q.orig/kernel/time/timekeeping.c
+++ linux-rt.q/kernel/time/timekeeping.c
@@ -56,16 +56,10 @@ static struct clocksource *clock; /* poi
  */
 s64 __get_nsec_offset(void)
 {
-	cycle_t cycle_now, cycle_delta;
+	cycle_t cycle_delta;
 	s64 ns_offset;
 
-	/* read clocksource: */
-	cycle_now = clocksource_read(clock);
-
-	/* calculate the delta since the last update_wall_time: */
-	cycle_delta = (cycle_now - clock->cycle_last) & clock->mask;
-
-	/* convert to nanoseconds: */
+	cycle_delta = clocksource_get_cycles(clock, clocksource_read(clock));
 	ns_offset = cyc2ns(clock, cycle_delta);
 
 	return ns_offset;
@@ -247,7 +241,7 @@ static void change_clocksource(void)
 
 	clock = new;
 	clock->cycle_last = now;
-
+	clock->cycle_accumulated = 0;
 	clock->error = 0;
 	clock->xtime_nsec = 0;
 	clocksource_calculate_interval(clock, NTP_INTERVAL_LENGTH);
@@ -259,8 +253,14 @@ static void change_clocksource(void)
 	       clock->name);
 #endif
 }
+
+void timekeeping_accumulate(void)
+{
+	clocksource_accumulate(clock, clocksource_read(clock));
+}
 #else
 static inline void change_clocksource(void) { }
+void timekeeping_accumulate(void) { }
 #endif
 
 /**
@@ -349,6 +349,7 @@ static int timekeeping_resume(struct sys
 	}
 	/* re-base the last cycle value */
 	clock->cycle_last = clocksource_read(clock);
+	clock->cycle_accumulated = 0;
 	clock->error = 0;
 	timekeeping_suspended = 0;
 	warp_check_clock_was_changed();
@@ -497,27 +498,28 @@ static void clocksource_adjust(s64 offse
  */
 void update_wall_time(void)
 {
-	cycle_t offset;
+	cycle_t cycle_now;
 
 	/* Make sure we're fully resumed: */
 	if (unlikely(timekeeping_suspended))
 		return;
 
 #ifdef CONFIG_GENERIC_TIME
-	offset = (clocksource_read(clock) - clock->cycle_last) & clock->mask;
+	cycle_now = (clocksource_read(clock) - clock->cycle_last) & clock->mask;
 #else
-	offset = clock->cycle_interval;
+	cycle_now = clock->cycle_interval;
 #endif
+	clocksource_accumulate(clock, cycle_now);
+
 	clock->xtime_nsec += (s64)xtime.tv_nsec << clock->shift;
 
 	/* normally this loop will run just once, however in the
 	 * case of lost or late ticks, it will accumulate correctly.
 	 */
-	while (offset >= clock->cycle_interval) {
+	while (clock->cycle_accumulated >= clock->cycle_interval) {
 		/* accumulate one interval */
 		clock->xtime_nsec += clock->xtime_interval;
-		clock->cycle_last += clock->cycle_interval;
-		offset -= clock->cycle_interval;
+		clock->cycle_accumulated -= clock->cycle_interval;
 
 		if (clock->xtime_nsec >= (u64)NSEC_PER_SEC << clock->shift) {
 			clock->xtime_nsec -= (u64)NSEC_PER_SEC << clock->shift;
@@ -535,7 +537,7 @@ void update_wall_time(void)
 	}
 
 	/* correct the clock when NTP error is too big */
-	clocksource_adjust(offset);
+	clocksource_adjust(clock->cycle_accumulated);
 
 	/* store full nanoseconds into xtime */
 	xtime.tv_nsec = (s64)clock->xtime_nsec >> clock->shift;
Index: linux-rt.q/kernel/timer.c
===================================================================
--- linux-rt.q.orig/kernel/timer.c
+++ linux-rt.q/kernel/timer.c
@@ -1017,6 +1017,7 @@ static void run_timer_softirq(struct sof
 void do_timer(unsigned long ticks)
 {
 	jiffies_64 += ticks;
+	timekeeping_accumulate();
 }
 
 #ifdef __ARCH_WANT_SYS_ALARM
