Subject: Preemption problem in kernel RT    Patch]
From: Ingo Molnar <mingo@elte.hu>
Date: Thu, 3 Jan 2008 09:22:03 +0100

----- Forwarded message from mbeauch <mbeauch@cox.net> -----

Date: Wed, 02 Jan 2008 20:27:09 -0500
From: mbeauch <mbeauch@cox.net>
To: mingo@elte.hu

Here's the updated patch:

Changed the real-time patch code to detect recursive calls
to dev_queue_xmit and drop the packet when detected.

Signed-off-by: Mark Beauchemin <mark.beauchemin@sycamorenet.com>
[ ported to latest upstream ]
Signed-off-by: Ingo Molnar <mingo@elte.hu>
---
 drivers/net/bnx2.c        |    2 +-
 drivers/net/mv643xx_eth.c |    6 +++---
 drivers/net/niu.c         |    2 +-
 include/linux/netdevice.h |   30 +++++++++++++++---------------
 net/core/dev.c            |   10 +++-------
 net/core/netpoll.c        |    2 +-
 net/sched/sch_generic.c   |    4 ++--
 7 files changed, 26 insertions(+), 30 deletions(-)

Index: linux-2.6-tip/drivers/net/bnx2.c
===================================================================
--- linux-2.6-tip.orig/drivers/net/bnx2.c
+++ linux-2.6-tip/drivers/net/bnx2.c
@@ -2661,7 +2661,7 @@ bnx2_tx_int(struct bnx2 *bp, struct bnx2
 
 	if (unlikely(netif_tx_queue_stopped(txq)) &&
 		     (bnx2_tx_avail(bp, txr) > bp->tx_wake_thresh)) {
-		__netif_tx_lock(txq, smp_processor_id());
+		__netif_tx_lock(txq, (void *)current);
 		if ((netif_tx_queue_stopped(txq)) &&
 		    (bnx2_tx_avail(bp, txr) > bp->tx_wake_thresh))
 			netif_tx_wake_queue(txq);
Index: linux-2.6-tip/drivers/net/mv643xx_eth.c
===================================================================
--- linux-2.6-tip.orig/drivers/net/mv643xx_eth.c
+++ linux-2.6-tip/drivers/net/mv643xx_eth.c
@@ -484,7 +484,7 @@ static void txq_maybe_wake(struct tx_que
 	struct netdev_queue *nq = netdev_get_tx_queue(mp->dev, txq->index);
 
 	if (netif_tx_queue_stopped(nq)) {
-		__netif_tx_lock(nq, smp_processor_id());
+		__netif_tx_lock(nq, (void *)current);
 		if (txq->tx_ring_size - txq->tx_desc_count >= MAX_SKB_FRAGS + 1)
 			netif_tx_wake_queue(nq);
 		__netif_tx_unlock(nq);
@@ -838,7 +838,7 @@ static void txq_kick(struct tx_queue *tx
 	u32 hw_desc_ptr;
 	u32 expected_ptr;
 
-	__netif_tx_lock(nq, smp_processor_id());
+	__netif_tx_lock(nq, (void *)current);
 
 	if (rdlp(mp, TXQ_COMMAND) & (1 << txq->index))
 		goto out;
@@ -862,7 +862,7 @@ static int txq_reclaim(struct tx_queue *
 	struct netdev_queue *nq = netdev_get_tx_queue(mp->dev, txq->index);
 	int reclaimed;
 
-	__netif_tx_lock(nq, smp_processor_id());
+	__netif_tx_lock(nq, (void *)current);
 
 	reclaimed = 0;
 	while (reclaimed < budget && txq->tx_desc_count > 0) {
Index: linux-2.6-tip/drivers/net/niu.c
===================================================================
--- linux-2.6-tip.orig/drivers/net/niu.c
+++ linux-2.6-tip/drivers/net/niu.c
@@ -3519,7 +3519,7 @@ static void niu_tx_work(struct niu *np, 
 out:
 	if (unlikely(netif_tx_queue_stopped(txq) &&
 		     (niu_tx_avail(rp) > NIU_TX_WAKEUP_THRESH(rp)))) {
-		__netif_tx_lock(txq, smp_processor_id());
+		__netif_tx_lock(txq, (void *)current);
 		if (netif_tx_queue_stopped(txq) &&
 		    (niu_tx_avail(rp) > NIU_TX_WAKEUP_THRESH(rp)))
 			netif_tx_wake_queue(txq);
Index: linux-2.6-tip/include/linux/netdevice.h
===================================================================
--- linux-2.6-tip.orig/include/linux/netdevice.h
+++ linux-2.6-tip/include/linux/netdevice.h
@@ -439,7 +439,7 @@ struct netdev_queue {
 	struct Qdisc		*qdisc;
 	unsigned long		state;
 	spinlock_t		_xmit_lock;
-	int			xmit_lock_owner;
+	void			*xmit_lock_owner;
 	struct Qdisc		*qdisc_sleeping;
 } ____cacheline_aligned_in_smp;
 
@@ -1625,35 +1625,35 @@ static inline void netif_rx_complete(str
 	napi_complete(napi);
 }
 
-static inline void __netif_tx_lock(struct netdev_queue *txq, int cpu)
+static inline void __netif_tx_lock(struct netdev_queue *txq, void *curr)
 {
 	spin_lock(&txq->_xmit_lock);
-	txq->xmit_lock_owner = cpu;
+	txq->xmit_lock_owner = curr;
 }
 
 static inline void __netif_tx_lock_bh(struct netdev_queue *txq)
 {
 	spin_lock_bh(&txq->_xmit_lock);
-	txq->xmit_lock_owner = raw_smp_processor_id();
+	txq->xmit_lock_owner = (void *)current;
 }
 
 static inline int __netif_tx_trylock(struct netdev_queue *txq)
 {
 	int ok = spin_trylock(&txq->_xmit_lock);
 	if (likely(ok))
-		txq->xmit_lock_owner = raw_smp_processor_id();
+		txq->xmit_lock_owner = (void *)current;
 	return ok;
 }
 
 static inline void __netif_tx_unlock(struct netdev_queue *txq)
 {
-	txq->xmit_lock_owner = -1;
+	txq->xmit_lock_owner = (void *)-1;
 	spin_unlock(&txq->_xmit_lock);
 }
 
 static inline void __netif_tx_unlock_bh(struct netdev_queue *txq)
 {
-	txq->xmit_lock_owner = -1;
+	txq->xmit_lock_owner = (void *)-1;
 	spin_unlock_bh(&txq->_xmit_lock);
 }
 
@@ -1666,10 +1666,10 @@ static inline void __netif_tx_unlock_bh(
 static inline void netif_tx_lock(struct net_device *dev)
 {
 	unsigned int i;
-	int cpu;
+	void *curr;
 
 	spin_lock(&dev->tx_global_lock);
-	cpu = raw_smp_processor_id();
+	curr = (void *)current;
 	for (i = 0; i < dev->num_tx_queues; i++) {
 		struct netdev_queue *txq = netdev_get_tx_queue(dev, i);
 
@@ -1679,7 +1679,7 @@ static inline void netif_tx_lock(struct 
 		 * the ->hard_start_xmit() handler and already
 		 * checked the frozen bit.
 		 */
-		__netif_tx_lock(txq, cpu);
+		__netif_tx_lock(txq, curr);
 		set_bit(__QUEUE_STATE_FROZEN, &txq->state);
 		__netif_tx_unlock(txq);
 	}
@@ -1715,9 +1715,9 @@ static inline void netif_tx_unlock_bh(st
 	local_bh_enable();
 }
 
-#define HARD_TX_LOCK(dev, txq, cpu) {			\
+#define HARD_TX_LOCK(dev, txq, curr) {			\
 	if ((dev->features & NETIF_F_LLTX) == 0) {	\
-		__netif_tx_lock(txq, cpu);		\
+		__netif_tx_lock(txq, curr);		\
 	}						\
 }
 
@@ -1730,14 +1730,14 @@ static inline void netif_tx_unlock_bh(st
 static inline void netif_tx_disable(struct net_device *dev)
 {
 	unsigned int i;
-	int cpu;
+	void *curr;
 
 	local_bh_disable();
-	cpu = raw_smp_processor_id();
+	curr = (void *)current;
 	for (i = 0; i < dev->num_tx_queues; i++) {
 		struct netdev_queue *txq = netdev_get_tx_queue(dev, i);
 
-		__netif_tx_lock(txq, cpu);
+		__netif_tx_lock(txq, curr);
 		netif_tx_stop_queue(txq);
 		__netif_tx_unlock(txq);
 	}
Index: linux-2.6-tip/net/core/dev.c
===================================================================
--- linux-2.6-tip.orig/net/core/dev.c
+++ linux-2.6-tip/net/core/dev.c
@@ -1884,13 +1884,9 @@ gso:
 		/*
 		 * No need to check for recursion with threaded interrupts:
 		 */
-#ifdef CONFIG_PREEMPT_RT
-		if (1) {
-#else
-		if (txq->xmit_lock_owner != cpu) {
-#endif
+		if (txq->xmit_lock_owner != (void *)current) {
 
-			HARD_TX_LOCK(dev, txq, cpu);
+			HARD_TX_LOCK(dev, txq, (void *)current);
 
 			if (!netif_tx_queue_stopped(txq)) {
 				rc = 0;
@@ -4262,7 +4258,7 @@ static void __netdev_init_queue_locks_on
 {
 	spin_lock_init(&dev_queue->_xmit_lock);
 	netdev_set_xmit_lockdep_class(&dev_queue->_xmit_lock, dev->type);
-	dev_queue->xmit_lock_owner = -1;
+	dev_queue->xmit_lock_owner = (void *)-1;
 }
 
 static void netdev_init_queue_locks(struct net_device *dev)
Index: linux-2.6-tip/net/core/netpoll.c
===================================================================
--- linux-2.6-tip.orig/net/core/netpoll.c
+++ linux-2.6-tip/net/core/netpoll.c
@@ -69,7 +69,7 @@ static void queue_process(struct work_st
 		txq = netdev_get_tx_queue(dev, skb_get_queue_mapping(skb));
 
 		local_irq_save_nort(flags);
-		__netif_tx_lock(txq, smp_processor_id());
+		__netif_tx_lock(txq, (void *)current);
 		if (netif_tx_queue_stopped(txq) ||
 		    netif_tx_queue_frozen(txq) ||
 		    ops->ndo_start_xmit(skb, dev) != NETDEV_TX_OK) {
Index: linux-2.6-tip/net/sched/sch_generic.c
===================================================================
--- linux-2.6-tip.orig/net/sched/sch_generic.c
+++ linux-2.6-tip/net/sched/sch_generic.c
@@ -80,7 +80,7 @@ static inline int handle_dev_cpu_collisi
 {
 	int ret;
 
-	if (unlikely(dev_queue->xmit_lock_owner == raw_smp_processor_id())) {
+	if (unlikely(dev_queue->xmit_lock_owner == (void *)current)) {
 		/*
 		 * Same CPU holding the lock. It may be a transient
 		 * configuration error, when hard_start_xmit() recurses. We
@@ -143,7 +143,7 @@ static inline int qdisc_restart(struct Q
 	dev = qdisc_dev(q);
 	txq = netdev_get_tx_queue(dev, skb_get_queue_mapping(skb));
 
-	HARD_TX_LOCK(dev, txq, raw_smp_processor_id());
+	HARD_TX_LOCK(dev, txq, (void *)current);
 	if (!netif_tx_queue_stopped(txq) &&
 	    !netif_tx_queue_frozen(txq))
 		ret = dev_hard_start_xmit(skb, dev, txq);
