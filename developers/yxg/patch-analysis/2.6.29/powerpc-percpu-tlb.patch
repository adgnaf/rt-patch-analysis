Subject: powerpc-percpu-tlb.patch
From: Thomas Gleixner <tglx@linutronix.de>
Date: Fri, 20 Mar 2009 17:48:44 +0100

Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
---
 arch/powerpc/include/asm/pgtable-ppc64.h |    9 +++++++-
 arch/powerpc/include/asm/tlb.h           |    6 +++--
 arch/powerpc/include/asm/tlbflush.h      |   31 +++++++++++++++++++++++-----
 arch/powerpc/kernel/process.c            |   22 ++++++++++++++++++++
 arch/powerpc/mm/init_32.c                |    2 -
 arch/powerpc/mm/pgtable.c                |   34 ++++++++++++++++++++-----------
 arch/powerpc/mm/tlb_hash64.c             |   20 ++++++++++++++++--
 arch/powerpc/platforms/pseries/iommu.c   |    9 +++++---
 8 files changed, 107 insertions(+), 26 deletions(-)

Index: linux-2.6-tip/arch/powerpc/include/asm/pgtable-ppc64.h
===================================================================
--- linux-2.6-tip.orig/arch/powerpc/include/asm/pgtable-ppc64.h
+++ linux-2.6-tip/arch/powerpc/include/asm/pgtable-ppc64.h
@@ -285,8 +285,15 @@ static inline unsigned long pte_update(s
 	: "r" (ptep), "r" (clr), "m" (*ptep), "i" (_PAGE_BUSY)
 	: "cc" );
 
-	if (old & _PAGE_HASHPTE)
+	if (old & _PAGE_HASHPTE) {
+#ifdef CONFIG_PREEMPT_RT
+		preempt_disable();
+#endif
 		hpte_need_flush(mm, addr, ptep, old, huge);
+#ifdef CONFIG_PREEMPT_RT
+		preempt_enable();
+#endif
+	}
 	return old;
 }
 
Index: linux-2.6-tip/arch/powerpc/include/asm/tlb.h
===================================================================
--- linux-2.6-tip.orig/arch/powerpc/include/asm/tlb.h
+++ linux-2.6-tip/arch/powerpc/include/asm/tlb.h
@@ -40,15 +40,17 @@ extern void pte_free_finish(void);
 
 static inline void tlb_flush(struct mmu_gather *tlb)
 {
-	struct ppc64_tlb_batch *tlbbatch = &__get_cpu_var(ppc64_tlb_batch);
+	struct ppc64_tlb_batch *tlbbatch = &get_cpu_var(ppc64_tlb_batch);
 
 	/* If there's a TLB batch pending, then we must flush it because the
 	 * pages are going to be freed and we really don't want to have a CPU
 	 * access a freed page because it has a stale TLB
 	 */
-	if (tlbbatch->index)
+	if (tlbbatch->index) {
 		__flush_tlb_pending(tlbbatch);
+	}
 
+	put_cpu_var(ppc64_tlb_batch);
 	pte_free_finish();
 }
 
Index: linux-2.6-tip/arch/powerpc/include/asm/tlbflush.h
===================================================================
--- linux-2.6-tip.orig/arch/powerpc/include/asm/tlbflush.h
+++ linux-2.6-tip/arch/powerpc/include/asm/tlbflush.h
@@ -101,18 +101,39 @@ extern void hpte_need_flush(struct mm_st
 
 static inline void arch_enter_lazy_mmu_mode(void)
 {
-	struct ppc64_tlb_batch *batch = &__get_cpu_var(ppc64_tlb_batch);
+	struct ppc64_tlb_batch *batch;
+#ifdef CONFIG_PREEMPT_RT
+	preempt_disable();
+#endif
+	batch = &get_cpu_var(ppc64_tlb_batch);
+
+#ifdef CONFIG_PREEMPT_RT
+	preempt_enable();
+#endif
 
 	batch->active = 1;
+	put_cpu_var(ppc64_tlb_batch);
 }
 
 static inline void arch_leave_lazy_mmu_mode(void)
 {
-	struct ppc64_tlb_batch *batch = &__get_cpu_var(ppc64_tlb_batch);
+	struct ppc64_tlb_batch *batch;
 
-	if (batch->index)
-		__flush_tlb_pending(batch);
-	batch->active = 0;
+#ifdef CONFIG_PREEMPT_RT
+	preempt_disable();
+#endif
+	batch = &get_cpu_var(ppc64_tlb_batch);
+
+	if (batch->active) {
+		if (batch->index) {
+			__flush_tlb_pending(batch);
+		}
+		batch->active = 0;
+	}
+#ifdef CONFIG_PREEMPT_RT
+	preempt_enable();
+#endif
+	put_cpu_var(ppc64_tlb_batch);
 }
 
 #define arch_flush_lazy_mmu_mode()      do {} while (0)
Index: linux-2.6-tip/arch/powerpc/kernel/process.c
===================================================================
--- linux-2.6-tip.orig/arch/powerpc/kernel/process.c
+++ linux-2.6-tip/arch/powerpc/kernel/process.c
@@ -302,6 +302,10 @@ struct task_struct *__switch_to(struct t
 	struct thread_struct *new_thread, *old_thread;
 	unsigned long flags;
 	struct task_struct *last;
+#if defined(CONFIG_PPC64) && defined (CONFIG_PREEMPT_RT)
+	struct ppc64_tlb_batch *batch;
+	int hadbatch;
+#endif
 
 #ifdef CONFIG_SMP
 	/* avoid complexity of lazy save/restore of fpu
@@ -393,6 +397,17 @@ struct task_struct *__switch_to(struct t
 		old_thread->accum_tb += (current_tb - start_tb);
 		new_thread->start_tb = current_tb;
 	}
+
+#ifdef CONFIG_PREEMPT_RT
+	batch = &__get_cpu_var(ppc64_tlb_batch);
+	if (batch->active) {
+		hadbatch = 1;
+		if (batch->index) {
+			__flush_tlb_pending(batch);
+		}
+		batch->active = 0;
+	}
+#endif /* #ifdef CONFIG_PREEMPT_RT */
 #endif
 
 	local_irq_save(flags);
@@ -411,6 +426,13 @@ struct task_struct *__switch_to(struct t
 
 	local_irq_restore(flags);
 
+#if defined(CONFIG_PPC64) && defined(CONFIG_PREEMPT_RT)
+	if (hadbatch) {
+		batch = &__get_cpu_var(ppc64_tlb_batch);
+		batch->active = 1;
+	}
+#endif
+
 	return last;
 }
 
Index: linux-2.6-tip/arch/powerpc/mm/init_32.c
===================================================================
--- linux-2.6-tip.orig/arch/powerpc/mm/init_32.c
+++ linux-2.6-tip/arch/powerpc/mm/init_32.c
@@ -54,7 +54,7 @@
 #endif
 #define MAX_LOW_MEM	CONFIG_LOWMEM_SIZE
 
-DEFINE_PER_CPU(struct mmu_gather, mmu_gathers);
+DEFINE_PER_CPU_LOCKED(struct mmu_gather, mmu_gathers);
 
 phys_addr_t total_memory;
 phys_addr_t total_lowmem;
Index: linux-2.6-tip/arch/powerpc/mm/pgtable.c
===================================================================
--- linux-2.6-tip.orig/arch/powerpc/mm/pgtable.c
+++ linux-2.6-tip/arch/powerpc/mm/pgtable.c
@@ -29,7 +29,7 @@
 #include <asm/tlbflush.h>
 #include <asm/tlb.h>
 
-static DEFINE_PER_CPU(struct pte_freelist_batch *, pte_freelist_cur);
+static DEFINE_PER_CPU_LOCKED(struct pte_freelist_batch *, pte_freelist_cur);
 static unsigned long pte_freelist_forced_free;
 
 struct pte_freelist_batch
@@ -80,21 +80,24 @@ static void pte_free_submit(struct pte_f
 
 void pgtable_free_tlb(struct mmu_gather *tlb, pgtable_free_t pgf)
 {
-	/* This is safe since tlb_gather_mmu has disabled preemption */
-        cpumask_t local_cpumask = cpumask_of_cpu(smp_processor_id());
-	struct pte_freelist_batch **batchp = &__get_cpu_var(pte_freelist_cur);
+	/* tlb->cpu is set by tlb_gather_mmu */
+	cpumask_t local_cpumask = cpumask_of_cpu(tlb->cpu);
+	struct pte_freelist_batch **batchp;
+	int cpu;
+
+	batchp = &get_cpu_var_locked(pte_freelist_cur, &cpu);
 
 	if (atomic_read(&tlb->mm->mm_users) < 2 ||
 	    cpus_equal(tlb->mm->cpu_vm_mask, local_cpumask)) {
 		pgtable_free(pgf);
-		return;
+		goto cleanup;
 	}
 
 	if (*batchp == NULL) {
 		*batchp = (struct pte_freelist_batch *)__get_free_page(GFP_ATOMIC);
 		if (*batchp == NULL) {
 			pgtable_free_now(pgf);
-			return;
+			goto cleanup;
 		}
 		(*batchp)->index = 0;
 	}
@@ -103,15 +106,22 @@ void pgtable_free_tlb(struct mmu_gather 
 		pte_free_submit(*batchp);
 		*batchp = NULL;
 	}
+
+cleanup:
+	put_cpu_var_locked(pte_freelist_cur, cpu);
 }
 
 void pte_free_finish(void)
 {
-	/* This is safe since tlb_gather_mmu has disabled preemption */
-	struct pte_freelist_batch **batchp = &__get_cpu_var(pte_freelist_cur);
+	struct pte_freelist_batch **batchp;
+	int cpu;
+
+	batchp = &get_cpu_var_locked(pte_freelist_cur, &cpu);
+
+	if (*batchp) {
+		pte_free_submit(*batchp);
+		*batchp = NULL;
+	}
 
-	if (*batchp == NULL)
-		return;
-	pte_free_submit(*batchp);
-	*batchp = NULL;
+	put_cpu_var_locked(pte_freelist_cur, cpu);
 }
Index: linux-2.6-tip/arch/powerpc/mm/tlb_hash64.c
===================================================================
--- linux-2.6-tip.orig/arch/powerpc/mm/tlb_hash64.c
+++ linux-2.6-tip/arch/powerpc/mm/tlb_hash64.c
@@ -30,13 +30,14 @@
 #include <asm/tlbflush.h>
 #include <asm/tlb.h>
 #include <asm/bug.h>
+#include <asm/machdep.h>
 
 DEFINE_PER_CPU(struct ppc64_tlb_batch, ppc64_tlb_batch);
 
 /* This is declared as we are using the more or less generic
  * arch/powerpc/include/asm/tlb.h file -- tgall
  */
-DEFINE_PER_CPU(struct mmu_gather, mmu_gathers);
+DEFINE_PER_CPU_LOCKED(struct mmu_gather, mmu_gathers);
 
 /*
  * A linux PTE was changed and the corresponding hash table entry
@@ -49,7 +50,7 @@ DEFINE_PER_CPU(struct mmu_gather, mmu_ga
 void hpte_need_flush(struct mm_struct *mm, unsigned long addr,
 		     pte_t *ptep, unsigned long pte, int huge)
 {
-	struct ppc64_tlb_batch *batch = &__get_cpu_var(ppc64_tlb_batch);
+	struct ppc64_tlb_batch *batch = &get_cpu_var(ppc64_tlb_batch);
 	unsigned long vsid, vaddr;
 	unsigned int psize;
 	int ssize;
@@ -100,6 +101,7 @@ void hpte_need_flush(struct mm_struct *m
 	 */
 	if (!batch->active) {
 		flush_hash_page(vaddr, rpte, psize, ssize, 0);
+		put_cpu_var(ppc64_tlb_batch);
 		return;
 	}
 
@@ -126,8 +128,22 @@ void hpte_need_flush(struct mm_struct *m
 	batch->pte[i] = rpte;
 	batch->vaddr[i] = vaddr;
 	batch->index = ++i;
+
+#ifdef CONFIG_PREEMPT_RT
+	/*
+	 * Since flushing tlb needs expensive hypervisor call(s) on celleb,
+	 * always flush it on RT to reduce scheduling latency.
+	 */
+	if (machine_is(celleb)) {
+		__flush_tlb_pending(batch);
+		put_cpu_var(ppc64_tlb_batch);
+		return;
+	}
+#endif /* CONFIG_PREEMPT_RT */
+
 	if (i >= PPC64_TLB_BATCH_NR)
 		__flush_tlb_pending(batch);
+	put_cpu_var(ppc64_tlb_batch);
 }
 
 /*
Index: linux-2.6-tip/arch/powerpc/platforms/pseries/iommu.c
===================================================================
--- linux-2.6-tip.orig/arch/powerpc/platforms/pseries/iommu.c
+++ linux-2.6-tip/arch/powerpc/platforms/pseries/iommu.c
@@ -140,7 +140,7 @@ static int tce_build_pSeriesLP(struct io
 	return ret;
 }
 
-static DEFINE_PER_CPU(u64 *, tce_page) = NULL;
+static DEFINE_PER_CPU_LOCKED(u64 *, tce_page) = NULL;
 
 static int tce_buildmulti_pSeriesLP(struct iommu_table *tbl, long tcenum,
 				     long npages, unsigned long uaddr,
@@ -154,13 +154,14 @@ static int tce_buildmulti_pSeriesLP(stru
 	long l, limit;
 	long tcenum_start = tcenum, npages_start = npages;
 	int ret = 0;
+	int cpu;
 
 	if (npages == 1) {
 		return tce_build_pSeriesLP(tbl, tcenum, npages, uaddr,
 		                           direction, attrs);
 	}
 
-	tcep = __get_cpu_var(tce_page);
+	tcep = get_cpu_var_locked(tce_page, &cpu);
 
 	/* This is safe to do since interrupts are off when we're called
 	 * from iommu_alloc{,_sg}()
@@ -169,10 +170,11 @@ static int tce_buildmulti_pSeriesLP(stru
 		tcep = (u64 *)__get_free_page(GFP_ATOMIC);
 		/* If allocation fails, fall back to the loop implementation */
 		if (!tcep) {
+			put_cpu_var_locked(tce_page, cpu);
 			return tce_build_pSeriesLP(tbl, tcenum, npages, uaddr,
 					    direction, attrs);
 		}
-		__get_cpu_var(tce_page) = tcep;
+		per_cpu_var_locked(tce_page, cpu) = tcep;
 	}
 
 	rpn = (virt_to_abs(uaddr)) >> TCE_SHIFT;
@@ -216,6 +218,7 @@ static int tce_buildmulti_pSeriesLP(stru
 		printk("\ttce[0] val = 0x%llx\n", tcep[0]);
 		show_stack(current, (unsigned long *)__get_SP());
 	}
+	put_cpu_var_locked(tce_page, cpu);
 	return ret;
 }
 
