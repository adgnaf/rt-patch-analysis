Add event tracer.

This patch adds a event trace that hooks into various events
in the kernel. Although it can be used separately, it is mainly
to help other traces (wakeup and preempt off) with seeing various
events in the traces without having to enable the heavy mcount
hooks.

Signed-off-by: Steven Rostedt <srostedt@redhat.com>
---
 kernel/trace/Kconfig              |   10 +
 kernel/trace/Makefile             |    1 
 kernel/trace/trace.c              |  194 ++++++++++++++++++++++++++++
 kernel/trace/trace.h              |  104 +++++++++++++++
 kernel/trace/trace_events.c       |  258 ++++++++++++++++++++++++++++++++++++++
 kernel/trace/trace_irqsoff.c      |    7 +
 kernel/trace/trace_sched_switch.c |    7 +
 kernel/trace/trace_selftest.c     |    6 
 8 files changed, 587 insertions(+)

Index: linux-2.6.25.4-rt1/kernel/trace/trace_events.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.25.4-rt1/kernel/trace/trace_events.c	2008-05-17 08:26:55.000000000 -0400
@@ -0,0 +1,258 @@
+/*
+ * trace task events
+ *
+ * Copyright (C) 2007 Steven Rostedt <srostedt@redhat.com>
+ *
+ * Based on code from the latency_tracer, that is:
+ *
+ *  Copyright (C) 2004-2006 Ingo Molnar
+ *  Copyright (C) 2004 William Lee Irwin III
+ */
+#include <linux/module.h>
+#include <linux/fs.h>
+#include <linux/debugfs.h>
+#include <linux/kallsyms.h>
+#include <linux/uaccess.h>
+#include <linux/ftrace.h>
+
+#include "trace.h"
+
+int	ftrace_events_enabled __read_mostly;
+
+static struct trace_array	*events_trace __read_mostly;
+
+static void event_reset(struct trace_array *tr)
+{
+	struct trace_array_cpu *data;
+	int cpu;
+
+	for_each_possible_cpu(cpu) {
+		data = tr->data[cpu];
+		tracing_reset(data);
+	}
+
+	tr->time_start = ftrace_now(raw_smp_processor_id());
+}
+
+void trace_event_sched_switch(struct task_struct *prev,
+			      struct task_struct *next)
+{
+	struct trace_array *tr = events_trace;
+	struct trace_array_cpu *data;
+	unsigned long flags;
+	long disabled;
+	int cpu;
+
+	if (!ftrace_events_enabled || !tr)
+		return;
+
+	local_irq_save(flags);
+	cpu = raw_smp_processor_id();
+	data = tr->data[cpu];
+
+	disabled = atomic_inc_return(&data->disabled);
+	if (unlikely(disabled != 1))
+		goto out;
+
+	tracing_sched_switch_trace(tr, data, prev, next, flags);
+
+ out:
+	atomic_dec(&data->disabled);
+	local_irq_restore(flags);
+}
+
+/* Taken from sched.c */
+#define __PRIO(prio) \
+	((prio) <= 99 ? 199 - (prio) : (prio) - 120)
+
+#define PRIO(p) __PRIO((p)->prio)
+
+void trace_event_wakeup(struct task_struct *wakee,
+			struct task_struct *curr)
+{
+	struct trace_array *tr = events_trace;
+	struct trace_array_cpu *data;
+	unsigned long flags, ip;
+	long disabled;
+	int cpu;
+
+	if (!ftrace_events_enabled || !tr)
+		return;
+
+	ip = CALLER_ADDR0;
+
+	local_irq_save(flags);
+	cpu = raw_smp_processor_id();
+	data = tr->data[cpu];
+
+	disabled = atomic_inc_return(&data->disabled);
+	if (unlikely(disabled != 1))
+		goto out;
+
+	/* record process's command line */
+	tracing_record_cmdline(wakee);
+	tracing_record_cmdline(curr);
+	tracing_event_wakeup(tr, data, flags, ip, wakee->pid, PRIO(wakee), PRIO(curr));
+
+ out:
+	atomic_dec(&data->disabled);
+	local_irq_restore(flags);
+}
+
+#define getarg(arg, ap) arg = va_arg(ap, typeof(arg));
+
+void ftrace_record_event(enum ftrace_event_enum event, ...)
+{
+	struct trace_array *tr = events_trace;
+	struct trace_array_cpu *data;
+	unsigned long flags;
+	unsigned long ip;
+	long disabled;
+	int cpu;
+	va_list ap;
+	int irq, usermode, prio;
+	pid_t pid;
+	ktime_t *time;
+	void *p1, *p2;
+	unsigned long ret_ip, error_code, address, running;
+
+
+	if (!ftrace_events_enabled || !events_trace)
+		return;
+
+	local_irq_save(flags);
+	cpu = raw_smp_processor_id();
+	data = tr->data[cpu];
+
+	disabled = atomic_inc_return(&data->disabled);
+	if (unlikely(disabled != 1))
+		goto out;
+
+	ip = CALLER_ADDR0;
+
+	va_start(ap, event);
+	switch (event) {
+	case FTRACE_EVENTS_IRQ:
+		getarg(irq, ap);
+		getarg(usermode, ap);
+		getarg(ret_ip, ap);
+		tracing_event_irq(tr, data, flags, ip, irq, usermode, ret_ip);
+		break;
+	case FTRACE_EVENTS_FAULT:
+		getarg(ret_ip, ap);
+		getarg(error_code, ap);
+		getarg(address, ap);
+		tracing_event_fault(tr, data, flags, ip, ret_ip, error_code, address);
+		break;
+	case FTRACE_EVENTS_TIMER:
+		getarg(p1, ap);
+		getarg(p2, ap);
+		tracing_event_timer(tr, data, flags, ip, p1, p2);
+		break;
+	case FTRACE_EVENTS_TIMESTAMP:
+		getarg(time, ap);
+		tracing_event_timestamp(tr, data, flags, ip, time);
+		break;
+	case FTRACE_EVENTS_TASK:
+		getarg(pid, ap);
+		getarg(prio, ap);
+		getarg(running, ap);
+		tracing_event_task(tr, data, flags, ip, pid, prio, running);
+		break;
+	}
+	va_end(ap);
+
+ out:
+	atomic_dec(&data->disabled);
+	local_irq_restore(flags);
+}
+
+static void start_event_trace(struct trace_array *tr)
+{
+	event_reset(tr);
+	ftrace_events_enabled = 1;
+	tracing_start_function_trace();
+}
+
+static void stop_event_trace(struct trace_array *tr)
+{
+	tracing_stop_function_trace();
+	ftrace_events_enabled = 0;
+}
+
+static void event_trace_init(struct trace_array *tr)
+{
+	events_trace = tr;
+
+	if (tr->ctrl)
+		start_event_trace(tr);
+}
+
+static void event_trace_reset(struct trace_array *tr)
+{
+	if (tr->ctrl)
+		stop_event_trace(tr);
+}
+
+static void event_trace_ctrl_update(struct trace_array *tr)
+{
+	if (tr->ctrl)
+		start_event_trace(tr);
+	else
+		stop_event_trace(tr);
+}
+
+static void event_trace_open(struct trace_iterator *iter)
+{
+	/* stop the trace while dumping */
+	if (iter->tr->ctrl)
+		stop_event_trace(iter->tr);
+}
+
+static void event_trace_close(struct trace_iterator *iter)
+{
+	if (iter->tr->ctrl)
+		start_event_trace(iter->tr);
+}
+
+static struct tracer event_trace __read_mostly =
+{
+	.name = "events",
+	.init = event_trace_init,
+	.reset = event_trace_reset,
+	.open = event_trace_open,
+	.close = event_trace_close,
+	.ctrl_update = event_trace_ctrl_update,
+};
+
+void trace_event_register(struct trace_array *tr)
+{
+	events_trace = tr;
+}
+
+void trace_event_unregister(struct trace_array *tr)
+{
+}
+
+void trace_start_events(void)
+{
+	ftrace_events_enabled = 1;
+}
+
+void trace_stop_events(void)
+{
+	ftrace_events_enabled = 0;
+}
+
+__init static int init_event_trace(void)
+{
+	int ret;
+
+	ret = register_tracer(&event_trace);
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+device_initcall(init_event_trace);
Index: linux-2.6.25.4-rt1/kernel/trace/Kconfig
===================================================================
--- linux-2.6.25.4-rt1.orig/kernel/trace/Kconfig	2008-05-17 08:26:52.000000000 -0400
+++ linux-2.6.25.4-rt1/kernel/trace/Kconfig	2008-05-17 08:26:55.000000000 -0400
@@ -99,6 +99,16 @@ config CONTEXT_SWITCH_TRACER
 	  This tracer gets called from the context switch and records
 	  all switching of tasks.
 
+config EVENT_TRACER
+	bool "trace kernel events"
+	depends on DEBUG_KERNEL
+	select TRACING
+	select CONTEXT_SWITCH_TRACER
+	help
+	  This option activates the event tracer of the latency_tracer.
+	  It activates markers through out the kernel for tracing.
+	  This option has a fairly low overhead when enabled.
+
 config DYNAMIC_FTRACE
 	bool "enable/disable ftrace tracepoints dynamically"
 	depends on FTRACE
Index: linux-2.6.25.4-rt1/kernel/trace/Makefile
===================================================================
--- linux-2.6.25.4-rt1.orig/kernel/trace/Makefile	2008-05-17 08:26:52.000000000 -0400
+++ linux-2.6.25.4-rt1/kernel/trace/Makefile	2008-05-17 08:26:55.000000000 -0400
@@ -20,5 +20,6 @@ obj-$(CONFIG_IRQSOFF_TRACER) += trace_ir
 obj-$(CONFIG_PREEMPT_TRACER) += trace_irqsoff.o
 obj-$(CONFIG_SCHED_TRACER) += trace_sched_wakeup.o
 obj-$(CONFIG_MMIOTRACE) += trace_mmiotrace.o
+obj-$(CONFIG_EVENT_TRACER) += trace_events.o
 
 libftrace-y := ftrace.o
Index: linux-2.6.25.4-rt1/kernel/trace/trace.c
===================================================================
--- linux-2.6.25.4-rt1.orig/kernel/trace/trace.c	2008-05-17 08:26:54.000000000 -0400
+++ linux-2.6.25.4-rt1/kernel/trace/trace.c	2008-05-17 08:26:55.000000000 -0400
@@ -1010,6 +1010,110 @@ void tracing_stop_function_trace(void)
 }
 #endif
 
+void tracing_event_irq(struct trace_array *tr,
+		       struct trace_array_cpu *data,
+		       unsigned long flags,
+		       unsigned long ip,
+		       int irq, int usermode,
+		       unsigned long retip)
+{
+	struct trace_entry *entry;
+
+	entry = tracing_get_trace_entry(tr, data);
+	tracing_generic_entry_update(entry, flags);
+	entry->type		= TRACE_IRQ;
+	entry->irq.ip		= ip;
+	entry->irq.irq		= irq;
+	entry->irq.ret_ip	= retip;
+	entry->irq.usermode	= usermode;
+}
+
+void tracing_event_fault(struct trace_array *tr,
+			 struct trace_array_cpu *data,
+			 unsigned long flags,
+			 unsigned long ip,
+			 unsigned long retip,
+			 unsigned long error_code,
+			 unsigned long address)
+{
+	struct trace_entry *entry;
+
+	entry = tracing_get_trace_entry(tr, data);
+	tracing_generic_entry_update(entry, flags);
+	entry->type		= TRACE_FAULT;
+	entry->fault.ip		= ip;
+	entry->fault.ret_ip	= retip;
+	entry->fault.errorcode	= error_code;
+	entry->fault.address	= address;
+}
+
+void tracing_event_timer(struct trace_array *tr,
+			 struct trace_array_cpu *data,
+			 unsigned long flags,
+			 unsigned long ip,
+			 void *p1, void *p2)
+{
+	struct trace_entry *entry;
+
+	entry = tracing_get_trace_entry(tr, data);
+	tracing_generic_entry_update(entry, flags);
+	entry->type		= TRACE_TIMER;
+	entry->timer.ip		= ip;
+	entry->timer.p1		= p1;
+	entry->timer.p2		= p2;
+}
+
+void tracing_event_timestamp(struct trace_array *tr,
+			     struct trace_array_cpu *data,
+			     unsigned long flags,
+			     unsigned long ip,
+			     ktime_t *now)
+{
+	struct trace_entry *entry;
+
+	entry = tracing_get_trace_entry(tr, data);
+	tracing_generic_entry_update(entry, flags);
+	entry->type		= TRACE_TIMESTAMP;
+	entry->timestamp.ip		= ip;
+	entry->timestamp.now		= *now;
+}
+
+void tracing_event_task(struct trace_array *tr,
+			struct trace_array_cpu *data,
+			unsigned long flags,
+			unsigned long ip,
+			pid_t pid, int prio,
+			unsigned long running)
+{
+	struct trace_entry *entry;
+
+	entry = tracing_get_trace_entry(tr, data);
+	tracing_generic_entry_update(entry, flags);
+	entry->type		= TRACE_TASK;
+	entry->task.ip		= ip;
+	entry->task.pid		= pid;
+	entry->task.prio	= prio;
+	entry->task.running	= running;
+}
+
+void tracing_event_wakeup(struct trace_array *tr,
+			  struct trace_array_cpu *data,
+			  unsigned long flags,
+			  unsigned long ip,
+			  pid_t pid, int prio,
+			  int curr_prio)
+{
+	struct trace_entry *entry;
+
+	entry = tracing_get_trace_entry(tr, data);
+	tracing_generic_entry_update(entry, flags);
+	entry->type		= TRACE_TASK;
+	entry->wakeup.ip		= ip;
+	entry->wakeup.pid		= pid;
+	entry->wakeup.prio	= prio;
+	entry->wakeup.curr_prio	= curr_prio;
+}
+
 enum trace_file_type {
 	TRACE_FILE_LAT_FMT	= 1,
 };
@@ -1474,6 +1578,50 @@ print_lat_fmt(struct trace_iterator *ite
 		}
 		trace_seq_puts(s, "\n");
 		break;
+	case TRACE_IRQ:
+		seq_print_ip_sym(s, entry->irq.ip, sym_flags);
+		if (entry->irq.irq >= 0)
+			trace_seq_printf(s, " %d ", entry->irq.irq);
+		if (entry->irq.usermode)
+			trace_seq_puts(s, " (usermode)\n ");
+		else {
+			trace_seq_puts(s, " (");
+			seq_print_ip_sym(s, entry->irq.ret_ip, sym_flags);
+			trace_seq_puts(s, ")\n");
+		}
+		break;
+	case TRACE_FAULT:
+		seq_print_ip_sym(s, entry->fault.ip, sym_flags);
+		trace_seq_printf(s, " %lx ", entry->fault.errorcode);
+		trace_seq_puts(s, " (");
+		seq_print_ip_sym(s, entry->fault.ret_ip, sym_flags);
+		trace_seq_puts(s, ")");
+		trace_seq_printf(s, " [%lx]\n", entry->fault.address);
+		break;
+	case TRACE_TIMER:
+		seq_print_ip_sym(s, entry->timer.ip, sym_flags);
+		trace_seq_printf(s, " (%p) (%p)\n",
+			   entry->timer.p1, entry->timer.p2);
+		break;
+	case TRACE_TIMESTAMP:
+		seq_print_ip_sym(s, entry->timestamp.ip, sym_flags);
+		trace_seq_printf(s, " (%Ld)\n",
+			   entry->timestamp.now.tv64);
+		break;
+	case TRACE_TASK:
+		seq_print_ip_sym(s, entry->task.ip, sym_flags);
+		comm = trace_find_cmdline(entry->task.pid);
+		trace_seq_printf(s, " %s %d %d %ld\n",
+			   comm, entry->task.pid,
+			   entry->task.prio, entry->task.running);
+		break;
+	case TRACE_WAKEUP:
+		seq_print_ip_sym(s, entry->task.ip, sym_flags);
+		comm = trace_find_cmdline(entry->task.pid);
+		trace_seq_printf(s, " %s %d %d %d\n",
+			   comm, entry->wakeup.pid,
+			   entry->wakeup.prio, entry->wakeup.curr_prio);
+		break;
 	default:
 		trace_seq_printf(s, "Unknown type %d\n", entry->type);
 	}
@@ -1571,6 +1719,52 @@ static int print_trace_fmt(struct trace_
 		if (!ret)
 			return 0;
 		break;
+	case TRACE_IRQ:
+		seq_print_ip_sym(s, entry->irq.ip, sym_flags);
+		if (entry->irq.irq >= 0)
+			trace_seq_printf(s, " %d ", entry->irq.irq);
+		if (entry->irq.usermode)
+			trace_seq_puts(s, " (usermode)\n ");
+		else {
+			trace_seq_puts(s, " (");
+			seq_print_ip_sym(s, entry->irq.ret_ip, sym_flags);
+			trace_seq_puts(s, ")\n");
+		}
+		break;
+	case TRACE_FAULT:
+		seq_print_ip_sym(s, entry->fault.ip, sym_flags);
+		trace_seq_printf(s, " %lx ", entry->fault.errorcode);
+		trace_seq_puts(s, " (");
+		seq_print_ip_sym(s, entry->fault.ret_ip, sym_flags);
+		trace_seq_puts(s, ")");
+		trace_seq_printf(s, " [%lx]\n", entry->fault.address);
+		break;
+	case TRACE_TIMER:
+		seq_print_ip_sym(s, entry->timer.ip, sym_flags);
+		trace_seq_printf(s, " (%p) (%p)\n",
+			   entry->timer.p1, entry->timer.p2);
+		break;
+	case TRACE_TIMESTAMP:
+		seq_print_ip_sym(s, entry->timestamp.ip, sym_flags);
+		trace_seq_printf(s, " (%Ld)\n",
+			   entry->timestamp.now.tv64);
+		break;
+	case TRACE_TASK:
+		seq_print_ip_sym(s, entry->task.ip, sym_flags);
+		comm = trace_find_cmdline(entry->task.pid);
+		trace_seq_printf(s, " %s %d %d %ld\n",
+			   comm, entry->task.pid,
+			   entry->task.prio, entry->task.running);
+		break;
+	case TRACE_WAKEUP:
+		seq_print_ip_sym(s, entry->task.ip, sym_flags);
+		comm = trace_find_cmdline(entry->task.pid);
+		trace_seq_printf(s, " %s %d %d %d\n",
+			   comm, entry->wakeup.pid,
+			   entry->wakeup.prio, entry->wakeup.curr_prio);
+		break;
+	default:
+		trace_seq_printf(s, "Unknown type %d\n", entry->type);
 	}
 	return 1;
 }
Index: linux-2.6.25.4-rt1/kernel/trace/trace.h
===================================================================
--- linux-2.6.25.4-rt1.orig/kernel/trace/trace.h	2008-05-17 08:26:52.000000000 -0400
+++ linux-2.6.25.4-rt1/kernel/trace/trace.h	2008-05-17 08:26:55.000000000 -0400
@@ -17,6 +17,12 @@ enum trace_type {
 	TRACE_SPECIAL,
 	TRACE_MMIO_RW,
 	TRACE_MMIO_MAP,
+	TRACE_IRQ,
+	TRACE_FAULT,
+	TRACE_TIMER,
+	TRACE_TIMESTAMP,
+	TRACE_TASK,
+	TRACE_WAKEUP,
 
 	__TRACE_LAST_TYPE
 };
@@ -60,6 +66,45 @@ struct stack_entry {
 	unsigned long		caller[FTRACE_STACK_ENTRIES];
 };
 
+struct irq_entry {
+	unsigned long		ip;
+	unsigned long		ret_ip;
+	unsigned		irq;
+	unsigned		usermode;
+};
+
+struct fault_entry {
+	unsigned long		ip;
+	unsigned long		ret_ip;
+	unsigned long		errorcode;
+	unsigned long		address;
+};
+
+struct timer_entry {
+	unsigned long		ip;
+	void			*p1;
+	void			*p2;
+};
+
+struct timestamp_entry {
+	unsigned long		ip;
+	ktime_t			now;
+};
+
+struct task_entry {
+	unsigned long		ip;
+	pid_t			pid;
+	unsigned		prio;
+	unsigned long		running;
+};
+
+struct wakeup_entry {
+	unsigned long		ip;
+	pid_t			pid;
+	unsigned		prio;
+	unsigned		curr_prio;
+};
+
 /*
  * The trace entry - the most basic unit of tracing. This is what
  * is printed in the end as a single line in the trace output, such as:
@@ -80,6 +125,12 @@ struct trace_entry {
 		struct stack_entry		stack;
 		struct mmiotrace_rw		mmiorw;
 		struct mmiotrace_map		mmiomap;
+		struct irq_entry		irq;
+		struct fault_entry		fault;
+		struct timer_entry		timer;
+		struct timestamp_entry		timestamp;
+		struct task_entry		task;
+		struct wakeup_entry		wakeup;
 	};
 };
 
@@ -205,6 +256,41 @@ void tracing_sched_switch_trace(struct t
 				struct task_struct *next,
 				unsigned long flags);
 void tracing_record_cmdline(struct task_struct *tsk);
+void tracing_event_irq(struct trace_array *tr,
+		       struct trace_array_cpu *data,
+		       unsigned long flags,
+		       unsigned long ip,
+		       int irq, int usermode,
+		       unsigned long retip);
+void tracing_event_fault(struct trace_array *tr,
+			 struct trace_array_cpu *data,
+			 unsigned long flags,
+			 unsigned long ip,
+			 unsigned long retip,
+			 unsigned long error_code,
+			 unsigned long address);
+void tracing_event_timer(struct trace_array *tr,
+			 struct trace_array_cpu *data,
+			 unsigned long flags,
+			 unsigned long ip,
+			 void *p1, void *p2);
+void tracing_event_timestamp(struct trace_array *tr,
+			     struct trace_array_cpu *data,
+			     unsigned long flags,
+			     unsigned long ip,
+			     ktime_t *now);
+void tracing_event_task(struct trace_array *tr,
+			struct trace_array_cpu *data,
+			unsigned long flags,
+			unsigned long ip,
+			pid_t pid, int prio,
+			unsigned long running);
+void tracing_event_wakeup(struct trace_array *tr,
+			  struct trace_array_cpu *data,
+			  unsigned long flags,
+			  unsigned long ip,
+			  pid_t pid, int prio,
+			  int curr_prio);
 
 void tracing_sched_wakeup_trace(struct trace_array *tr,
 				struct trace_array_cpu *data,
@@ -345,4 +431,22 @@ enum trace_iterator_flags {
 	TRACE_ITER_SCHED_TREE		= 0x200,
 };
 
+#ifdef CONFIG_EVENT_TRACER
+extern void trace_event_sched_switch(struct task_struct *prev,
+				     struct task_struct *next);
+extern void trace_event_wakeup(struct task_struct *wakee,
+			       struct task_struct *curr);
+extern void trace_start_events(void);
+extern void trace_stop_events(void);
+extern void trace_event_register(struct trace_array *tr);
+extern void trace_event_unregister(struct trace_array *tr);
+#else
+# define trace_event_sched_switch(prev, next)	do { } while (0)
+# define trace_event_wakeup(wakee, curr)	do { } while (0)
+# define trace_start_events()			do { } while (0)
+# define trace_stop_events()			do { } while (0)
+# define trace_event_register(tr)		do { } while (0)
+# define trace_event_unregister(tr)		do { } while (0)
+#endif
+
 #endif /* _LINUX_KERNEL_TRACE_H */
Index: linux-2.6.25.4-rt1/kernel/trace/trace_irqsoff.c
===================================================================
--- linux-2.6.25.4-rt1.orig/kernel/trace/trace_irqsoff.c	2008-05-17 08:26:53.000000000 -0400
+++ linux-2.6.25.4-rt1/kernel/trace/trace_irqsoff.c	2008-05-17 08:26:55.000000000 -0400
@@ -209,6 +209,8 @@ start_critical_timing(unsigned long ip, 
 
 	trace_function(tr, data, ip, parent_ip, flags);
 
+	trace_start_events();
+
 	per_cpu(tracing_cpu, cpu) = 1;
 
 	atomic_dec(&data->disabled);
@@ -241,6 +243,7 @@ stop_critical_timing(unsigned long ip, u
 	atomic_inc(&data->disabled);
 
 	local_save_flags(flags);
+	trace_stop_events();
 	trace_function(tr, data, ip, parent_ip, flags);
 	check_critical_timing(tr, data, parent_ip ? : ip, cpu);
 	data->critical_start = 0;
@@ -364,12 +367,16 @@ static void __irqsoff_tracer_init(struct
 	/* make sure that the tracer is visible */
 	smp_wmb();
 
+	trace_event_register(tr);
+
 	if (tr->ctrl)
 		start_irqsoff_tracer(tr);
 }
 
 static void irqsoff_tracer_reset(struct trace_array *tr)
 {
+	trace_event_unregister(tr);
+
 	if (tr->ctrl)
 		stop_irqsoff_tracer(tr);
 }
Index: linux-2.6.25.4-rt1/kernel/trace/trace_sched_switch.c
===================================================================
--- linux-2.6.25.4-rt1.orig/kernel/trace/trace_sched_switch.c	2008-05-17 08:26:52.000000000 -0400
+++ linux-2.6.25.4-rt1/kernel/trace/trace_sched_switch.c	2008-05-17 08:26:55.000000000 -0400
@@ -84,12 +84,19 @@ ftrace_ctx_switch(void *__rq, struct tas
 	 * Chain to the wakeup tracer (this is a NOP if disabled):
 	 */
 	wakeup_sched_switch(prev, next);
+
+	/*
+	 * Chain to event trace;
+	 */
+	trace_event_sched_switch(prev, next);
 }
 
 void
 ftrace_wake_up_task(void *__rq, struct task_struct *wakee,
 		    struct task_struct *curr)
 {
+	trace_event_wakeup(wakee, curr);
+
 	wakeup_func(__rq, wakee, curr);
 
 	/*
Index: linux-2.6.25.4-rt1/kernel/trace/trace_selftest.c
===================================================================
--- linux-2.6.25.4-rt1.orig/kernel/trace/trace_selftest.c	2008-05-17 08:26:52.000000000 -0400
+++ linux-2.6.25.4-rt1/kernel/trace/trace_selftest.c	2008-05-17 08:26:55.000000000 -0400
@@ -11,6 +11,12 @@ static inline int trace_valid_entry(stru
 	case TRACE_WAKE:
 	case TRACE_STACK:
 	case TRACE_SPECIAL:
+	case TRACE_IRQ:
+	case TRACE_FAULT:
+	case TRACE_TIMER:
+	case TRACE_TIMESTAMP:
+	case TRACE_TASK:
+	case TRACE_WAKEUP:
 		return 1;
 	}
 	return 0;
