From: Steven Rostedt <srostedt@redhat.com>
Subject: ftrace: avoid lockdep annotation problems

Ftrace uses raw spin locks in the tracing to avoid the overhead of lockdep.
In doing so, if a lock is taken that is checked by lockdep, the hardirqs
counters can be corrupted and lockdep will fail.

The problem arises with the timestamp of ftrace. Some timestamps will
call spinlocks that are used by lockdep. If this happens inside of
the raw irqs disabled of ftrace, lockdeps counters will be corrupted.

This patch moves the capturing of the timestamp outside of the raw spinlocks
of ftrace.

Signed-off-by: Steven Rostedt <srostedt@redhat.com>

---
 kernel/trace/trace.c |   41 ++++++++++++++++++++++++++++++-----------
 1 file changed, 30 insertions(+), 11 deletions(-)

Index: linux-2.6.25.2-rt1/kernel/trace/trace.c
===================================================================
--- linux-2.6.25.2-rt1.orig/kernel/trace/trace.c	2008-05-09 09:22:45.000000000 -0400
+++ linux-2.6.25.2-rt1/kernel/trace/trace.c	2008-05-09 09:22:45.000000000 -0400
@@ -769,7 +769,8 @@ tracing_get_trace_entry(struct trace_arr
 }
 
 static inline void
-tracing_generic_entry_update(struct trace_entry *entry, unsigned long flags)
+tracing_generic_entry_update(struct trace_entry *entry, unsigned long flags,
+			     unsigned long long *t)
 {
 	struct task_struct *tsk = current;
 	unsigned long pc;
@@ -778,7 +779,7 @@ tracing_generic_entry_update(struct trac
 
 	entry->preempt_count	= pc & 0xff;
 	entry->pid		= (tsk) ? tsk->pid : 0;
-	entry->t		= ftrace_now(raw_smp_processor_id());
+	entry->t		= *t;
 	entry->flags = (irqs_disabled_flags(flags) ? TRACE_FLAG_IRQS_OFF : 0) |
 		((pc & HARDIRQ_MASK) ? TRACE_FLAG_HARDIRQ : 0) |
 		((pc & SOFTIRQ_MASK) ? TRACE_FLAG_SOFTIRQ : 0) |
@@ -791,11 +792,13 @@ trace_function(struct trace_array *tr, s
 {
 	struct trace_entry *entry;
 	unsigned long irq_flags;
+	unsigned long long t;
 
+	t = ftrace_now(raw_smp_processor_id());
 	raw_local_irq_save(irq_flags);
 	__raw_spin_lock(&data->lock);
 	entry			= tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, &t);
 	entry->type		= TRACE_FN;
 	entry->fn.ip		= ip;
 	entry->fn.parent_ip	= parent_ip;
@@ -819,11 +822,14 @@ __trace_special(void *__tr, void *__data
 	struct trace_array *tr = __tr;
 	struct trace_entry *entry;
 	unsigned long irq_flags;
+	unsigned long long t;
+
+	t = ftrace_now(raw_smp_processor_id());
 
 	raw_local_irq_save(irq_flags);
 	__raw_spin_lock(&data->lock);
 	entry			= tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, 0);
+	tracing_generic_entry_update(entry, 0, &t);
 	entry->type		= TRACE_SPECIAL;
 	entry->special.arg1	= arg1;
 	entry->special.arg2	= arg2;
@@ -840,12 +846,15 @@ void __trace_mmiotrace_rw(struct trace_a
 {
 	struct trace_entry *entry;
 	unsigned long irq_flags;
+	unsigned long long t;
+
+	t = ftrace_now(raw_smp_processor_id());
 
 	raw_local_irq_save(irq_flags);
 	__raw_spin_lock(&data->lock);
 
 	entry			= tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, 0);
+	tracing_generic_entry_update(entry, 0, &t);
 	entry->type		= TRACE_MMIO_RW;
 	entry->mmiorw		= *rw;
 
@@ -860,12 +869,15 @@ void __trace_mmiotrace_map(struct trace_
 {
 	struct trace_entry *entry;
 	unsigned long irq_flags;
+	unsigned long long t;
+
+	t = ftrace_now(raw_smp_processor_id());
 
 	raw_local_irq_save(irq_flags);
 	__raw_spin_lock(&data->lock);
 
 	entry			= tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, 0);
+	tracing_generic_entry_update(entry, 0, &t);
 	entry->type		= TRACE_MMIO_MAP;
 	entry->mmiomap		= *map;
 
@@ -879,6 +891,7 @@ void __trace_mmiotrace_map(struct trace_
 void __trace_stack(struct trace_array *tr,
 		   struct trace_array_cpu *data,
 		   unsigned long flags,
+		   unsigned long long *t,
 		   int skip)
 {
 	struct trace_entry *entry;
@@ -888,7 +901,7 @@ void __trace_stack(struct trace_array *t
 		return;
 
 	entry			= tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, t);
 	entry->type		= TRACE_STACK;
 
 	memset(&entry->stack, 0, sizeof(entry->stack));
@@ -910,11 +923,14 @@ tracing_sched_switch_trace(struct trace_
 {
 	struct trace_entry *entry;
 	unsigned long irq_flags;
+	unsigned long long t;
+
+	t = ftrace_now(raw_smp_processor_id());
 
 	raw_local_irq_save(irq_flags);
 	__raw_spin_lock(&data->lock);
 	entry			= tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, &t);
 	entry->type		= TRACE_CTX;
 	entry->ctx.prev_pid	= prev->pid;
 	entry->ctx.prev_prio	= prev->prio;
@@ -922,7 +938,7 @@ tracing_sched_switch_trace(struct trace_
 	entry->ctx.next_pid	= next->pid;
 	entry->ctx.next_prio	= next->prio;
 	entry->ctx.next_state	= next->state;
-	__trace_stack(tr, data, flags, 4);
+	__trace_stack(tr, data, flags, &t, 4);
 	__raw_spin_unlock(&data->lock);
 	raw_local_irq_restore(irq_flags);
 }
@@ -936,11 +952,14 @@ tracing_sched_wakeup_trace(struct trace_
 {
 	struct trace_entry *entry;
 	unsigned long irq_flags;
+	unsigned long long t;
+
+	t = ftrace_now(raw_smp_processor_id());
 
 	raw_local_irq_save(irq_flags);
 	__raw_spin_lock(&data->lock);
 	entry			= tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, &t);
 	entry->type		= TRACE_WAKE;
 	entry->ctx.prev_pid	= curr->pid;
 	entry->ctx.prev_prio	= curr->prio;
@@ -948,7 +967,7 @@ tracing_sched_wakeup_trace(struct trace_
 	entry->ctx.next_pid	= wakee->pid;
 	entry->ctx.next_prio	= wakee->prio;
 	entry->ctx.next_state	= wakee->state;
-	__trace_stack(tr, data, flags, 5);
+	__trace_stack(tr, data, flags, &t, 5);
 	__raw_spin_unlock(&data->lock);
 	raw_local_irq_restore(irq_flags);
 
