Temporary disable sched_rt groups, because for some reason it hangs the
-rt kernel.

Signed-off-by: Steven Rostedt <srostedt@redhat.com>

---
 kernel/sched_rt.c |    6 +++++-
 1 file changed, 5 insertions(+), 1 deletion(-)

Index: linux-2.6.25.4-rt1/kernel/sched_rt.c
===================================================================
--- linux-2.6.25.4-rt1.orig/kernel/sched_rt.c	2008-05-17 08:28:08.000000000 -0400
+++ linux-2.6.25.4-rt1/kernel/sched_rt.c	2008-05-17 08:28:14.000000000 -0400
@@ -115,6 +115,7 @@ static void sched_rt_rq_dequeue(struct r
 
 static inline int rt_rq_throttled(struct rt_rq *rt_rq)
 {
+	return 0;
 	return rt_rq->rt_throttled && !rt_rq->rt_nr_boosted;
 }
 
@@ -174,6 +175,7 @@ static inline void sched_rt_rq_dequeue(s
 
 static inline int rt_rq_throttled(struct rt_rq *rt_rq)
 {
+	return 0;
 	return rt_rq->rt_throttled;
 }
 #endif
@@ -194,6 +196,7 @@ static int sched_rt_runtime_exceeded(str
 {
 	u64 runtime = sched_rt_runtime(rt_rq);
 
+	return 0;
 	if (runtime == RUNTIME_INF)
 		return 0;
 
@@ -220,6 +223,7 @@ static void update_sched_rt_period(struc
 	struct rt_rq *rt_rq;
 	u64 period;
 
+	return;
 	while (rq->clock > rq->rt_period_expire) {
 		period = (u64)sysctl_sched_rt_period * NSEC_PER_USEC;
 		rq->rt_period_expire += period;
@@ -262,7 +266,7 @@ static void update_curr_rt(struct rq *rq
 	curr->se.exec_start = rq->clock;
 	cpuacct_charge(curr, delta_exec);
 
-	rt_rq->rt_time += delta_exec;
+//	rt_rq->rt_time += delta_exec;
 	if (sched_rt_runtime_exceeded(rt_rq))
 		resched_task(curr);
 }
