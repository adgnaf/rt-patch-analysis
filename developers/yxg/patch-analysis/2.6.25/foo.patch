From: Peter Zijlstra

Update of sched_clock code.

---
 arch/x86/Kconfig     |    1 +
 init/Kconfig         |    6 ++++++
 kernel/sched_clock.c |   37 ++++++++++++++++++++++++++++++-------
 3 files changed, 37 insertions(+), 7 deletions(-)

Index: linux-2.6.25.4-rt1/arch/x86/Kconfig
===================================================================
--- linux-2.6.25.4-rt1.orig/arch/x86/Kconfig	2008-05-17 08:27:56.000000000 -0400
+++ linux-2.6.25.4-rt1/arch/x86/Kconfig	2008-05-17 08:28:13.000000000 -0400
@@ -18,6 +18,7 @@ config X86_64
 ### Arch settings
 config X86
 	def_bool y
+	select HAVE_UNSTABLE_SCHED_CLOCK
 	select HAVE_FTRACE
 	select HAVE_IDE
 	select HAVE_OPROFILE
Index: linux-2.6.25.4-rt1/kernel/sched_clock.c
===================================================================
--- linux-2.6.25.4-rt1.orig/kernel/sched_clock.c	2008-05-17 08:27:15.000000000 -0400
+++ linux-2.6.25.4-rt1/kernel/sched_clock.c	2008-05-17 08:28:13.000000000 -0400
@@ -28,6 +28,7 @@
 #include <linux/spinlock.h>
 #include <linux/ktime.h>
 #include <linux/module.h>
+#include <linux/hardirq.h>
 
 
 #ifdef CONFIG_HAVE_UNSTABLE_SCHED_CLOCK
@@ -59,24 +60,26 @@ static inline struct sched_clock_data *c
 	return &per_cpu(sched_clock_data, cpu);
 }
 
-static __read_mostly u64 ktime_offset;
+static __read_mostly int sched_clock_running;
 
 void sched_clock_init(void)
 {
 	int cpu;
-
-	ktime_offset = ktime_to_ns(ktime_get());
+	unsigned long now_jiffies = jiffies;
+	u64 ktime = ktime_to_ns(ktime_get());
 
 	for_each_possible_cpu(cpu) {
 		struct sched_clock_data *scd = cpu_sdc(cpu);
 
 		scd->lock = (__raw_spinlock_t)__RAW_SPIN_LOCK_UNLOCKED;
-		scd->prev_jiffies = jiffies;
+		scd->prev_jiffies = now_jiffies;
 		scd->prev_raw = 0;
 		scd->tick_raw = 0;
 		scd->tick_gtod = 0;
-		scd->clock = 0;
+		scd->clock = ktime;
 	}
+
+	sched_clock_running = 1;
 }
 
 /*
@@ -138,6 +141,16 @@ u64 sched_clock_cpu(int cpu)
 	struct sched_clock_data *scd = cpu_sdc(cpu);
 	u64 now, clock;
 
+	if (unlikely(!sched_clock_running))
+		return 0ULL;
+
+	/*
+	 * Normally this is not called in NMI context - but if it is,
+	 * trying to do any locking here is totally lethal.
+	if (unlikely(in_nmi()))
+		return scd->clock;
+	 */
+
 	WARN_ON_ONCE(!irqs_disabled());
 	now = sched_clock();
 
@@ -176,10 +189,13 @@ void sched_clock_tick(void)
 	struct sched_clock_data *scd = this_scd();
 	u64 now, now_gtod;
 
+	if (unlikely(!sched_clock_running))
+		return;
+
 	WARN_ON_ONCE(!irqs_disabled());
 
 	now = sched_clock();
-	now_gtod = ktime_to_ns(ktime_get()) - ktime_offset;
+	now_gtod = ktime_to_ns(ktime_get());
 
 	__raw_spin_lock(&scd->lock);
 	__update_sched_clock(scd, now);
@@ -239,6 +255,13 @@ unsigned long long __attribute__((weak))
 
 unsigned long long cpu_clock(int cpu)
 {
-	return sched_clock_cpu(cpu);
+	unsigned long flags;
+	unsigned long long clock;
+
+	raw_local_irq_save(flags);
+	clock = sched_clock_cpu(cpu);
+	raw_local_irq_restore(flags);
+
+	return clock;
 }
 EXPORT_SYMBOL_GPL(cpu_clock);
Index: linux-2.6.25.4-rt1/init/Kconfig
===================================================================
--- linux-2.6.25.4-rt1.orig/init/Kconfig	2008-05-17 08:27:55.000000000 -0400
+++ linux-2.6.25.4-rt1/init/Kconfig	2008-05-17 08:28:13.000000000 -0400
@@ -311,6 +311,12 @@ config CPUSETS
 
 	  Say N if unsure.
 
+#
+# Architectures with an unreliable sched_clock() should select this:
+#
+config HAVE_UNSTABLE_SCHED_CLOCK
+        bool
+
 config GROUP_SCHED
 	bool "Group CPU scheduler"
 	default y
