---
 kernel/sched.c       |   75 ---------------------------------------------------
 kernel/sched_clock.c |   22 ++++++++++----
 2 files changed, 15 insertions(+), 82 deletions(-)

Index: linux-2.6.25.4-rt1/kernel/sched.c
===================================================================
--- linux-2.6.25.4-rt1.orig/kernel/sched.c	2008-05-17 08:26:53.000000000 -0400
+++ linux-2.6.25.4-rt1/kernel/sched.c	2008-05-17 08:26:54.000000000 -0400
@@ -611,81 +611,6 @@ static inline u64 global_rt_runtime(void
 	return (u64)sysctl_sched_rt_runtime * NSEC_PER_USEC;
 }
 
-static const unsigned long long time_sync_thresh = 100000;
-
-static DEFINE_PER_CPU(unsigned long long, time_offset);
-static DEFINE_PER_CPU(unsigned long long, prev_cpu_time);
-
-/*
- * Global lock which we take every now and then to synchronize
- * the CPUs time. This method is not warp-safe, but it's good
- * enough to synchronize slowly diverging time sources and thus
- * it's good enough for tracing:
- */
-static DEFINE_SPINLOCK(time_sync_lock);
-static unsigned long long prev_global_time;
-
-static unsigned long long notrace __sync_cpu_clock(cycles_t time, int cpu)
-{
-	unsigned long flags;
-
-	/*
-	 * We want this inlined, to not get tracer function calls
-	 * in this critical section:
-	 */
-	local_irq_save(flags);
-	spin_acquire(&time_sync_lock.dep_map, 0, 0, _THIS_IP_);
-	__raw_spin_lock(&time_sync_lock.raw_lock);
-
-	if (time < prev_global_time) {
-		per_cpu(time_offset, cpu) += prev_global_time - time;
-		time = prev_global_time;
-	} else {
-		prev_global_time = time;
-	}
-
-	__raw_spin_unlock(&time_sync_lock.raw_lock);
-	spin_release(&time_sync_lock.dep_map, 1, _THIS_IP_);
-	local_irq_restore(flags);
-
-	return time;
-}
-
-static unsigned long long notrace __cpu_clock(int cpu)
-{
-	unsigned long long now;
-
-	/*
-	 * Only call sched_clock() if the scheduler has already been
-	 * initialized (some code might call cpu_clock() very early):
-	 */
-	if (unlikely(!scheduler_running))
-		return 0;
-
-	now = sched_clock_cpu(cpu);
-
-	return now;
-}
-
-/*
- * For kernel-internal use: high-speed (but slightly incorrect) per-cpu
- * clock constructed from sched_clock():
- */
-unsigned long long notrace cpu_clock(int cpu)
-{
-	unsigned long long prev_cpu_time, time, delta_time;
-
-	prev_cpu_time = per_cpu(prev_cpu_time, cpu);
-	time = __cpu_clock(cpu) + per_cpu(time_offset, cpu);
-	delta_time = time-prev_cpu_time;
-
-	if (unlikely(delta_time > time_sync_thresh))
-		time = __sync_cpu_clock(time, cpu);
-
-	return time;
-}
-EXPORT_SYMBOL_GPL(cpu_clock);
-
 #ifndef prepare_arch_switch
 # define prepare_arch_switch(next)	do { } while (0)
 #endif
Index: linux-2.6.25.4-rt1/kernel/sched_clock.c
===================================================================
--- linux-2.6.25.4-rt1.orig/kernel/sched_clock.c	2008-05-17 08:26:53.000000000 -0400
+++ linux-2.6.25.4-rt1/kernel/sched_clock.c	2008-05-17 08:26:54.000000000 -0400
@@ -59,21 +59,23 @@ static inline struct sched_clock_data *c
 	return &per_cpu(sched_clock_data, cpu);
 }
 
+static __read_mostly u64 ktime_offset;
+
 void sched_clock_init(void)
 {
-	u64 ktime_now = ktime_to_ns(ktime_get());
-	u64 now = 0;
 	int cpu;
 
+	ktime_offset = ktime_to_ns(ktime_get());
+
 	for_each_possible_cpu(cpu) {
 		struct sched_clock_data *scd = cpu_sdc(cpu);
 
 		scd->lock = (raw_spinlock_t)__RAW_SPIN_LOCK_UNLOCKED;
 		scd->prev_jiffies = jiffies;
-		scd->prev_raw = now;
-		scd->tick_raw = now;
-		scd->tick_gtod = ktime_now;
-		scd->clock = ktime_now;
+		scd->prev_raw = 0;
+		scd->tick_raw = 0;
+		scd->tick_gtod = 0;
+		scd->clock = 0;
 	}
 }
 
@@ -177,7 +179,7 @@ void sched_clock_tick(void)
 	WARN_ON_ONCE(!irqs_disabled());
 
 	now = sched_clock();
-	now_gtod = ktime_to_ns(ktime_get());
+	now_gtod = ktime_to_ns(ktime_get()) - ktime_offset;
 
 	__raw_spin_lock(&scd->lock);
 	__update_sched_clock(scd, now);
@@ -234,3 +236,9 @@ unsigned long long __attribute__((weak))
 {
 	return (unsigned long long)jiffies * (NSEC_PER_SEC / HZ);
 }
+
+unsigned long long cpu_clock(int cpu)
+{
+	return sched_clock_cpu(cpu);
+}
+EXPORT_SYMBOL_GPL(cpu_clock);
